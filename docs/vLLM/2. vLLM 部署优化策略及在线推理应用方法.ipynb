{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>企业级大模型部署推理管理工具</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Part 2. vLLM 部署优化策略及在线推理应用方法</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在上一节《Part 1. Vllm 框架基础入门与本地私有化部署》中，我们核心介绍了 `vLLM` 离线推理的部署及其使用方法。离线推理服务中的大模型生命周期是：<font color=red>仅当发生实际的调用请求时，大模型资源才会被加载到显存中，当调用请求结束后，大模型会被立即卸载</font>，这种工作模式会导致每产生一轮新的调用都需要重新加载大模型，产生非常大的响应延迟。 因此从工作模式上看，离线推理通常会用于非实时性任务，比如数据预处理、批量文本生成、模型评估等不要求实时返回响应结果，但需要高吞吐量的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于像实时问答、聊天机器人、AI 助手等对实时性要求较高的场景，企业级应用需处理数千 QPS（每秒查询数），如金融交易、搜索引擎等高并发服务，离线推理显然是无法满足需求的。因此，我们需要掌握`vLLM`的在线推理方法，这种工作模式才是我们在实际构建大模型应用及高效使用`vLLM`框架启动的模型服务进行项目开发时，最常用且最优的应用方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在`vLLM`官方文档中有对`vLLM`在线推理的基本介绍，其访问地址为：https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241345570.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单来说，所谓的`vLLM`在线推理，就是`vLLM`框架提供了一个可以设置为遵循`OpenAI API`协议的`http`服务器，实现了`OpenAI`的`Completions API` 、 `Chat API` 等接口规范，只有通过这些接口规范，才能够允许我们使用通用的接口协议接入其他的客户端进行集成使用。比如热门开源项目`Kotaemon`、`Open webui`等，其接入的规范都是遵循`OpenAI API`协议的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，接下来我们就重点介绍`vLLM`的在线推理方法，以及在本地启动模型过程中如何配置模型服务的优化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. vLLM 服务器配置参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对上节课介绍的离线推理来说，当想要访问某一个模型服务的时候，仅仅需要在对应的虚拟环境中按照如下代码调用方式，即可自动执行先加载模型、执行推理、卸载模型的完整流程。\n",
    "\n",
    "```python\n",
    "    from vllm import LLM\n",
    "\n",
    "    llm = LLM(model=\"/home/08_vllm/qwen/Qwen2___5-7B\",\n",
    "            trust_remote_code=True,\n",
    "            max_model_len=4096,\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\"你好，请你介绍一下你自己\")\n",
    "\n",
    "    print(outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是，对于在线推理服务，我们则需要先启动`vLLM`模型服务，然后才能通过`http`协议来访问模型服务。启动`Http`服务器`vLLM` 提供的是`vllm serve`命令，可以通过`vllm serve --help`命令查看详细、可以在启动时指定模型服务启动的参数。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241428919.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里能够看到在启动时可指定的参数非常多，每个参数都有对应的应用需求。用好`vLLM`的框架关键就是对这些参数的正确应用。因此我们接下来需要分场景，依次拆解核心参数的配置方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;首先，从底层架构上来看，`vLLM` 使用的是`FastAPI` 托管其`http`服务器。其官方的源码定义地址如下：https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241405131.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`FastAPI` 服务器在启动时，其可传递的参数定义如下：\n",
    "\n",
    "```python\n",
    "        parser = FlexibleArgumentParser()\n",
    "        parser.add_argument(\"--host\", type=str, default=None)\n",
    "        parser.add_argument(\"--port\", type=parser.check_port, default=8000)\n",
    "        parser.add_argument(\"--ssl-keyfile\", type=str, default=None)\n",
    "        parser.add_argument(\"--ssl-certfile\", type=str, default=None)\n",
    "        parser.add_argument(\"--ssl-ca-certs\",\n",
    "                            type=str,\n",
    "                            default=None,\n",
    "                            help=\"The CA certificates file\")\n",
    "        parser.add_argument(\n",
    "            \"--enable-ssl-refresh\",\n",
    "            action=\"store_true\",\n",
    "            default=False,\n",
    "            help=\"Refresh SSL Context when SSL certificate files change\")\n",
    "        parser.add_argument(\n",
    "            \"--ssl-cert-reqs\",\n",
    "            type=int,\n",
    "            default=int(ssl.CERT_NONE),\n",
    "            help=\"Whether client certificate is required (see stdlib ssl module's)\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--root-path\",\n",
    "            type=str,\n",
    "            default=None,\n",
    "            help=\"FastAPI root_path when app is behind a path based routing proxy\")\n",
    "        parser.add_argument(\"--log-level\", type=str, default=\"debug\")\n",
    "        parser = AsyncEngineArgs.add_cli_args(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中核心的参数说明如下表所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>vLLM 模型服务部署参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数                  | 类型      | 默认值       | 描述                                                         |\n",
    "|---------------------|---------|------------|------------------------------------------------------------|\n",
    "| `--host`            | `str`   | `None`     | 服务器主机地址。                                             |\n",
    "| `--port`            | `int`   | `8000`     | 服务器端口，使用 `parser.check_port` 进行验证。             |\n",
    "| `--ssl-keyfile`     | `str`   | `None`     | SSL 密钥文件路径。                                          |\n",
    "| `--ssl-certfile`    | `str`   | `None`     | SSL 证书文件路径。                                         |\n",
    "| `--ssl-ca-certs`    | `str`   | `None`     | CA 证书文件路径。                                          |\n",
    "| `--enable-ssl-refresh` | `store_true` | `False` | 当 SSL 证书文件更改时，刷新 SSL 上下文。                     |\n",
    "| `--ssl-cert-reqs`   | `int`   | `0`        | 是否需要客户端证书（参考标准库 ssl 模块）。                  |\n",
    "| `--root-path`       | `str`   | `None`     | 当应用程序在基于路径的路由代理后面时的 FastAPI root_path。 |\n",
    "| `--log-level`       | `str`   | `debug`    | 日志级别。                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以看到有大量参数都是用于`SSL`证书的配置，SSL（Secure Sockets Layer）证书是一种数字证书，主要用于在互联网上建立安全的加密连接，通过加密数据传输，确保在客户端和服务器之间传输的数据不被窃取或篡改，在数据加密、身份验证、API 安全等方面发挥重要作用。如要启用`SSL`认证，`vLLM` 仅需要配置相关的秘钥路径，因此大家对这几个参数要有了解，我们就不展开介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要关注的点是`--host` 和 `--port` 参数，`--host` 参数用于指定服务器的主机地址，`--port` 参数用于指定服务器的端口号，直接决定了模型服务启动后，我们如何通过`http`协议来访问模型服务。比如：\n",
    "\n",
    "```python\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://192.168.110.131:8000/v1\",\n",
    "        api_key=\"sk-xxx\", # 随便填写，只是为了通过接口参数校验\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"Qwen2.5-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己\"},\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中`base_url=\"http://192.168.110.131:8000/v1\"`，则表示我们通过`http`协议访问模型服务的地址为：`http://192.168.110.131:8000/v1`，`192.168.110.131`为服务器的主机地址，`8000`为服务器启动的端口号，具体要根据大家实际部署的模型服务地址来填写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，结合上一节课中的参数介绍，在启动`vLLM`提供的`http`服务器时，最基本核心参数如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>vLLM 在线推理服务启动参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数 | 描述 | 默认值 |\n",
    "|------------------------|----------------------------------------|--------------------------|\n",
    "| --host | 服务器主机地址。 | None |\n",
    "| --port | 服务器端口，使用 parser.check_port 进行验证。 | 8000 |\n",
    "| --api_key | 访问模型服务的API密钥。 | None |\n",
    "| --served-model-name | 模型服务启动后，在代码环境下通过`http`协议访问时需要指定的模型名称。 | 无默认值 |\n",
    "| --trust_remote_code | 信任来自 Hugging Face 的远程代码。 | 无默认值 |\n",
    "| --tensor_parallel_size | 张量并行组的数量。 | 1 |\n",
    "| --device | vLLM 执行使用的设备类型：auto, cuda, neuron, cpu, tpu, xpu, hpu。 | “auto” |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们就以`Qwen2.5-7B-Instruct`模型为例，按照如上参数启动`vLLM`的在线推理服务。在服务器终端执行如下命令：\n",
    "\n",
    "```bash\n",
    "    vllm serve Qwen2.5-7B-Instruct --served-model-name qwen2.5-7b --api_key muyu --host 192.168.110.131 --port 9000 --trust_remote_code --tensor_parallel_size 2 --device cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241508021.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里的核心参数定义规则是：`vllm serve <模型名称> --served-model-name <模型别名> --api_key <API密钥> --host <服务器地址> --port <端口号> --trust_remote_code --tensor_parallel_size <张量并行组数量> --device <设备类型>`。其中 `vllm serve` 后不使用任何参数，直接指定本地模型的存储路径，而`--served-model-name` 参数则用于指定模型服务启动后，在代码环境下通过`http`协议访问时需要指定的模型名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当能看到`Application startup complete`，则表示`vLLM`的在线推理服务启动成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241456874.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当`HTTP`服务器启动后，首先可以访问 http://192.168.110.131:9000/docs （具体地址根据大家实际部署的模型服务地址来填写） 来查看`Swagger UI`界面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241524001.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`/completions`主要用于处理基本的文本生成任务，模型会在给定的提示后生成一段文本。这种类型的任务通常用于生成文章、故事、邮件等。而`/chat/completions`则主要用于处理面向对话的任务，模型需要理解和生成对话。这种类型的任务通常用于构建聊天机器人或者对话系统。因此我们现在可以使用`OpenAI`的接口规范来访问`vLLM`的在线推理服务。代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我叫Qwen，是阿里云开发的一款超大规模语言模型。我的主要功能是在各种主题上与用户进行对话交流，并能够提供信息查询、知识解答、创意写作等多种服务。作为一个人工智能助手，我的目标是帮助用户获得他们需要的信息，提供有用的建议和支持。我会不断学习和进步，尽力为用户提供准确、及时的帮助。如果你有任何问题或需要帮助，都可以随时向我提问哦！\n"
     ]
    }
   ],
   "source": [
    "# ! pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://192.168.110.131:9000/v1\",    # 这里是 --host + --port 的组合\n",
    "    api_key=\"muyu\",   # 这里是 --api_key 的参数\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "model=\"qwen2.5-7b\",  # 这里是 --served-model-name 的参数\n",
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己\"},\n",
    "]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果能够正常返回模型推理结果，则说明当前环境下可以通过`OpenAI`的接口规范正常访问`vLLM`的在线推理服务，同时，在服务器终端也会实时打印模型推理的日志信息。如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241514669.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. vLLM模型推理参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;回看`FastAPI` 服务器启动的源码，最后一行`parser = AsyncEngineArgs.add_cli_args(parser)`则是说明，在服务器启动时，会实例化一个 `AsyncLLMEngine` 对象，这里需要明确的一个底层逻辑是：<font color=red>虽然离线推理和在线推理的模型服务启动方式不同，但本质上它们是共享相同的底层推理引擎，也就是这里的`AsyncLLMEngine`对象</font>。当`Http`服务器收到请求时，会触发`generate`函数。源码位置：https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py\n",
    "\n",
    "```python\n",
    "    @app.post(\"/generate\")\n",
    "    async def generate(request: Request) -> Response:\n",
    "        \"\"\"Generate completion for the request.\n",
    "\n",
    "        The request should be a JSON object with the following fields:\n",
    "        - prompt: the prompt to use for the generation.\n",
    "        - stream: whether to stream the results or not.\n",
    "        - other fields: the sampling parameters (See `SamplingParams` for details).\n",
    "        \"\"\"\n",
    "        request_dict = await request.json()\n",
    "        return await _generate(request_dict, raw_request=request)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`AsyncLLMEngine` 引擎在推理接口`generate`中，会调用`_generate`函数，提供流式和非流式两种生成方式。对于流式请求，服务器会在生成新的`Token`时立即返回，无需等待整个文本补全完成。相反，对于非流式请求，服务器会等到整个文本补全完成后才响应客户端。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504241540672.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;生成器的核心是调用`add_request`方法获得产生的迭代器，这里我们要关注的是：`sampling_params` 参数。\n",
    "\n",
    "```python\n",
    "            async for output in await self.add_request(\n",
    "                    request_id,\n",
    "                    prompt,\n",
    "                    sampling_params,\n",
    "                    lora_request=lora_request,\n",
    "                    trace_headers=trace_headers,\n",
    "                    prompt_adapter_request=prompt_adapter_request,\n",
    "                    priority=priority,\n",
    "            ):\n",
    "                yield LLMEngine.validate_output(output, RequestOutput)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`sampling_params` 可以定义调用模型服务时控制响应结果的生成参数，如`temperature`、`top_p`、`top_k`等，`vLLM` 扩展了一些`OpenAI` 协议不支持的参数，所有可用的参数如下表所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>控制模型输出采样参数列表</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数                          | 描述                                                                                     |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------|\n",
    "| `n`                         | 要返回的输出序列数量。                                                                  |\n",
    "| `best_of`                  | 从提示生成的输出序列数量。从这些 `best_of` 序列中返回前 `n` 个序列。`best_of` 必须大于或等于 `n`。默认情况下，`best_of` 设置为 `n`。警告：此功能仅在 V0 中支持。 |\n",
    "| `presence_penalty`         | 浮点数，根据新 token 是否出现在已生成的文本中对其进行惩罚。值 > 0 鼓励模型使用新 token，值 < 0 鼓励模型重复 token。 |\n",
    "| `frequency_penalty`        | 浮点数，根据新 token 在已生成文本中的频率对其进行惩罚。值 > 0 鼓励模型使用新 token，值 < 0 鼓励模型重复 token。 |\n",
    "| `repetition_penalty`       | 浮点数，根据新 token 是否出现在提示和已生成文本中对其进行惩罚。值 > 1 鼓励模型使用新 token，值 < 1 鼓励模型重复 token。 |\n",
    "| `temperature`              | 浮点数，控制采样的随机性。较低的值使模型更确定，较高的值使模型更随机。零表示贪婪采样。 |\n",
    "| `top_p`                    | 浮点数，控制考虑的 top token 的累积概率。必须在 (0, 1] 之间。设置为 1 以考虑所有 token。 |\n",
    "| `top_k`                    | 整数，控制考虑的 top token 数量。设置为 -1 以考虑所有 token。                       |\n",
    "| `min_p`                    | 浮点数，表示相对于最可能 token 的概率，考虑 token 的最小概率。必须在 [0, 1] 之间。设置为 0 以禁用此功能。 |\n",
    "| `seed`                     | 用于生成的随机种子。                                                                    |\n",
    "| `stop`                     | 停止生成的字符串列表。当生成这些字符串时，生成将停止。返回的输出将不包含停止字符串。 |\n",
    "| `stop_token_ids`          | 停止生成的 token 列表。当生成这些 token 时，生成将停止。返回的输出将包含停止 token，除非停止 token 是特殊 token。 |\n",
    "| `bad_words`                | 不允许生成的单词列表。更准确地说，只有当下一个生成的 token 可以完成序列时，才不允许对应 token 序列的最后一个 token。 |\n",
    "| `include_stop_str_in_output` | 是否在输出文本中包含停止字符串。默认值为 False。                                      |\n",
    "| `ignore_eos`               | 是否忽略 EOS token，并在生成 EOS token 后继续生成 token。                           |\n",
    "| `max_tokens`               | 每个输出序列生成的最大 token 数量。                                                    |\n",
    "| `min_tokens`               | 每个输出序列生成的最小 token 数量，直到可以生成 EOS 或 stop_token_ids。               |\n",
    "| `logprobs`                 | 每个输出 token 返回的 log 概率数量。当设置为 None 时，不返回概率。如果设置为非 None 值，结果将包括指定数量的最可能 token 的 log 概率，以及选择的 token。注意，实施遵循 OpenAI API：API 将始终返回采样 token 的 log 概率，因此响应中可能有多达 logprobs+1 个元素。 |\n",
    "| `prompt_logprobs`          | 每个提示 token 返回的 log 概率数量。                                                  |\n",
    "| `detokenize`               | 是否对输出进行反分词。默认值为 True。                                                  |\n",
    "| `skip_special_tokens`      | 是否在输出中跳过特殊 token。                                                            |\n",
    "| `spaces_between_special_tokens` | 是否在输出中的特殊 token 之间添加空格。默认值为 True。                               |\n",
    "| `logits_processors`        | 修改 logits 的函数列表，基于先前生成的 token，并可选地将提示 token 作为第一个参数。   |\n",
    "| `truncate_prompt_tokens`    | 如果设置为整数 k，将仅使用提示的最后 k 个 token（即左截断）。默认值为 None（即不截断）。 |\n",
    "| `guided_decoding`          | 如果提供，引擎将根据这些参数构建引导解码 logits 处理器。默认值为 None。               |\n",
    "| `logit_bias`               | 如果提供，引擎将构建一个应用这些 logit 偏置的 logits 处理器。默认值为 None。         |\n",
    "| `allowed_token_ids`        | 如果提供，引擎将构建一个 logits 处理器，仅保留给定 token ids 的分数。默认值为 None。 |\n",
    "| `extra_args`               | 任意额外参数，可供自定义采样实现使用。未被任何树内采样实现使用。                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，在不改变`vllm serve`启动参数的情况下，在调用时，我们可以通过`extra_body`参数来控制模型推理的生成参数，代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我叫Qwen，是由阿里云\n"
     ]
    }
   ],
   "source": [
    "# ! pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://192.168.110.131:9000/v1\",    # 这里是 --host + --port 的组合\n",
    "    api_key=\"muyu\",   # 这里是 --api_key 的参数\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen2.5-7b\",  # 这里是 --served-model-name 的参数\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己\"},\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 10,\n",
    "        \"max_tokens\": 10,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"stop\": [\"\\n\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如上结果所示，为了测试`extra_body`参数的效果，我们通过设置`max_tokens=10`来直观感受模型推理结果的变化。上表中的参数均可以作为`extra_body`的参数来使用，大家理解了`extra_body`参数的用法后，可以根据需要来控制模型推理的生成参数，这里不再过多演示，大家可以自行尝试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. vLLM模型启动参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vLLM` 框架作为目前在实际生产中使用最为广泛的模型推理框架，<font color=red>在实际使用中是一定需要根据不同的业务需求来调整模型推理的启动参数才能达到一个符合预期的推理效果，并不是仅仅靠默认的启动参数就能够达到最佳的性能，而是由所使用的硬件设备、模型大小、推理需求等共同决定的</font>。因此我们需要简单理解`vLLm`框架在提供推理服务时其基本的底层工作原理，才能根据实际需求来调整模型推理的启动参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里依然借助`vLLM`框架的源码来帮助大家理解。首先，当请求通过`POST`发送到`generate`接口发送后，会触发`AsyncLLMEngine`引擎中的`add_request`函数，源码位置：https://github.com/vllm-project/vllm/blob/main/vllm/engine/async_llm_engine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def add_request(self,\n",
    "                    request_id: str,\n",
    "                    *,\n",
    "                    verbose: bool = False,\n",
    "                    **engine_add_request_kwargs) -> AsyncStream:\n",
    "        \"\"\"Add a request to be sent to the engine on the next background\n",
    "        loop iteration.\"\"\"\n",
    "        if request_id in self._request_streams:\n",
    "            raise KeyError(f\"Request {request_id} already exists.\")\n",
    "\n",
    "        abort_request = partial(self.abort_request, verbose=verbose)\n",
    "        stream = AsyncStream(request_id, abort_request)\n",
    "        self._new_requests.put_nowait((stream, {\n",
    "            \"request_id\": request_id,\n",
    "            **engine_add_request_kwargs\n",
    "        }))\n",
    "\n",
    "        self.new_requests_event.set()\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(\"Added request %s.\", request_id)\n",
    "\n",
    "        return stream\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;处理新请求的过程与`AsyncLLMEngine`引擎的操作完全独立。它的唯一目的是将请求入队。`AsyncLLMEngine`引擎会在另一个线程（或协程）中并发运行，从队列中获取请求。 其中：\n",
    "\n",
    "- 使用 `partial` 创建一个可以中止请求的函数，并创建一个 `AsyncStream` 实例，表示这个请求的流。\n",
    "- 将请求流和请求参数放入 `_new_requests` 队列中，准备在下一个循环中处理。\n",
    "- 设置 `new_requests_event`，通知后台循环有新的请求需要处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，再进行`AsyncLLMEngine`引擎初始化：\n",
    "\n",
    "```python\n",
    "                async for output in await self.add_request(\n",
    "                    request_id,\n",
    "                    prompt,\n",
    "                    sampling_params,\n",
    "                    lora_request=lora_request,\n",
    "                    trace_headers=trace_headers,\n",
    "                    prompt_adapter_request=prompt_adapter_request,\n",
    "                    priority=priority,\n",
    "            ):\n",
    "                yield LLMEngine.validate_output(output, RequestOutput)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`AsyncLLMEngine`引擎的初始化工作会包括 `Create worker`、`Cache engine`、`Scheduler` 的三个初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vLLM` 引擎会为每个 `GPU` 分配一个工作线程，从而保证高效的并行性。比如如果我们使用`4`个`GPU`，它将创建`4`个对应的工作线程。`Worker` 负责 `GPU` 相关的任务，在`AsyncLLMEngine`引擎初始化时，会将大模型权重加载到 `GPU` 上。这个过程大家经常会出现如下报错："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231557616.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;错误信息指出：大模型的最大序列长度: 131072，所需的 KV 缓存内存: 7.00 GiB，可用的 KV 缓存内存: 5.26 GiB。很明显就是`GPU`的显存不足，无法加载模型权重。注意：这里为了复现这个错误，这里加载的是`Qwen2.5__7B`模型，其模型下载的`ModelScope`地址是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504242008327.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时对应的启动命令是：`vllm serve Qwen2___5-7B --served-model-name qwen2.5-7b --api_key muyu --host 192.168.110.131 --port 9000 --trust_remote_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504242013633.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该模型是在单卡3090（24G显存）上启动就会显示显存不足：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504242015301.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;要理解这个原因，我们需要知道`vLLM`框架在初始化第二步，即设置缓存阶段所执行的底层工作原理。其核心源码如下：\n",
    "\n",
    "```python\n",
    "        def _initialize_kv_caches(self) -> None:\n",
    "        \"\"\"Initialize the KV cache in the worker(s).\n",
    "\n",
    "        The workers will determine the number of blocks in both the GPU cache\n",
    "        and the swap CPU cache.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        num_gpu_blocks, num_cpu_blocks = (\n",
    "            self.model_executor.determine_num_available_blocks())\n",
    "\n",
    "        if self.cache_config.num_gpu_blocks_override is not None:\n",
    "            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n",
    "            logger.info(\n",
    "                \"Overriding num_gpu_blocks=%d with \"\n",
    "                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n",
    "                num_gpu_blocks_override)\n",
    "            num_gpu_blocks = num_gpu_blocks_override\n",
    "\n",
    "        self.cache_config.num_gpu_blocks = num_gpu_blocks\n",
    "        self.cache_config.num_cpu_blocks = num_cpu_blocks\n",
    "\n",
    "        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
    "        elapsed = time.time() - start\n",
    "        logger.info((\"init engine (profile, create kv cache, \"\n",
    "                     \"warmup model) took %.2f seconds\"), elapsed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;每个`Worker`都有独立的缓存引擎，其中每个缓存引擎管理其 `GPU` 中分配给 `KV` 缓存存储的内存。<font color=red>这里的`KV`缓存，指的就是`vLLM` 框架提出的`PagedAttention`机制</font>。 `Transformer` 架构在解码过程中最大的计算瓶颈就是会对输入的每个`token`计算注意力，即使用成对的键值张量。这些张量都必须存储在内存中，随着大模型接收越来越长的输入，这些张量消耗的内存会变得非常大。简单地将所有张量存储在内存中会导致内存过度预留和碎片化。这种碎片化会使内存访问效率非常低下，尤其是对于较长的标记序列。为了缓解这些问题，vLLM团队（来自加州大学伯克利分校）提出了`PagedAttention`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在传统KV缓存管理中，系统需要为每个输入序列预先分配一块连续的内存空间，用于存储该序列生成的所有键值张量。由于序列长度是动态变化的（例如用户可能输入1个token或1000个token），系统为了保证内存足够，通常会按最大可能长度预留内存（例如为每个序列预留支持2048 token的内存）。当多个序列的内存块被频繁分配和释放后，内存中会出现许多不连续的小块空闲内存。尽管这些碎片的总和可能足够大，但它们无法被合并用于存储较大的新序列，导致内存利用率进一步下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果对`PagedAttention`的原理感兴趣，可以参考学习：https://docs.vllm.ai/en/latest/design/kernel/paged_attention.html ， 我们这里不展开说明。\n",
    "\n",
    "&emsp;&emsp;需要理解的是：<font color='red'>`PagedAttention` 背后的想法是创建映射到`GPU`内存中物理块的连续虚拟块，通过虚拟内存管理来解决GPU内存碎片化的问题。</font>它把键值对分成固定大小的块，这些块在虚拟地址空间中是连续的，但物理内存中可以分散。这样在推理时，只需要按需加载需要的块，减少内存浪费，提高效率。举个生活中的例子：假设在图书馆用书架整理书籍，传统方法是要求每本书必须摆放在连续的书架上（比如第1-5号书架必须全部放满某一本书的内容）。这会导致两个问题：1. 即使书只用了部分内容，也要占用整排书架；2. 书架很快会产生碎片化空隙。`PagedAttention` 的解决方案就像：\n",
    "\n",
    "- 先把书撕成固定大小的小册子（比如每本册子能装20页内容）\n",
    "\n",
    "- 在目录里记录每个小册子的位置（比如\"第一章的1-20页放在A区3号架，21-40页放在C区5号架...\"）\n",
    "\n",
    "- 实际摆放时，小册子可以分散放在任何有空位的架子上\n",
    "\n",
    "- 需要读哪部分内容时，就快速查找目录，只拿取相关的小册子\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里面的小册子 = 物理块（实际存储键值对的GPU内存碎片），目录 = 索引表（记录虚拟地址到物理地址的映射），固定大小 = 每个块存储预定数量的token键值对，分散存放 = 物理块在GPU内存中非连续但按需分配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，`PagedAttention`会将`KV`缓存分割为固定大小的块（默认是16个token/块），内存按块分配而非连续预留。同时物理块可分散存储，逻辑上通过映射表维护连续性。所以为了能存储`kv`缓存，所以需要预留一定量的`GPU`显存。`vLLM` 的计算方法是：<font color='red'>可用的显存等于总 `GPU` 显存减去模型权重的大小、中间激活大小和缓冲区（默认为总内存的 10%）。模型大小已知，但中间激活大小未知，即推理过程中中间激活占用的最大内存，`vLLM` 通过运行虚拟数据然后分析内存消耗来确定此数字。虚拟数据的大小由配置中的参数决定，默认情况下设置为模型支持的最大上下文长度。</font> \n",
    "\n",
    "&emsp;&emsp;从源码中，可以找到具体的计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks) -> None:\n",
    "        \"\"\"Initialize the KV cache by invoking the underlying worker.\n",
    "        \"\"\"\n",
    "        # NOTE: This is logged in the executor because there can be >1 workers.\n",
    "        logger.info(\"# %s blocks: %d, # CPU blocks: %d\",\n",
    "                    vllm.platforms.current_platform.device_name,\n",
    "                    num_gpu_blocks, num_cpu_blocks)\n",
    "        max_concurrency = (num_gpu_blocks * self.cache_config.block_size /\n",
    "                           self.model_config.max_model_len)\n",
    "        logger.info(\"Maximum concurrency for %s tokens per request: %.2fx\",\n",
    "                    self.model_config.max_model_len, max_concurrency)\n",
    "\n",
    "        self.cache_config.num_gpu_blocks = num_gpu_blocks\n",
    "        self.cache_config.num_cpu_blocks = num_cpu_blocks\n",
    "\n",
    "        self.collective_rpc(\"initialize_cache\",\n",
    "                            args=(num_gpu_blocks, num_cpu_blocks))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里需要关注`cache_config`中的参数，其源码定义位置：https://github.com/vllm-project/vllm/blob/main/vllm/config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251105656.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里把核心参数整理成表格如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>vLLM缓存配置参数列表</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数                          | 描述                                                                                                           | 默认值                     |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------------------------------|--------------------------|\n",
    "| `block_size`                | 连续缓存块的大小（以 token 数量为单位）。Neuron设备设置为 `--max-model-len`，CUDA 设备仅支持最大为 32 的块大小，HPU 设备默认 128。 | 16                   |\n",
    "| `gpu_memory_utilization`    | 模型执行器使用的 GPU 内存比例，范围从 0 到 1。例如，0.5 表示使用 50% 的 GPU 内存。默认值为 0.9。               | 0.9                      |\n",
    "| `swap_space`                | 每个 GPU 的 CPU 交换空间大小（以 GiB 为单位）。                                                                  | 4                        |\n",
    "| `cache_dtype`               | KV 缓存存储的数据类型。如果为 \"auto\"，将使用模型数据类型。CUDA 11.8+ 支持 fp8（=fp8_e4m3）和 fp8_e5m2。         | \"auto\"                   |\n",
    "| `is_attention_free`         | 模型是否为无注意力模型。主要在 `ModelConfig` 中设置，该值应手动复制到此处。                                     | `False`                  |\n",
    "| `num_gpu_blocks_override`   | 要使用的 GPU 块数量。如果指定，则覆盖已分析的 `num_gpu_blocks`。如果为 `None`，则无效。用于测试抢占。              | `None`                   |\n",
    "| `sliding_window`            | KV 缓存的滑动窗口大小。主要在 `ModelConfig` 中设置，该值应手动复制到此处。                                     | `None`                   |\n",
    "| `enable_prefix_caching`     | 是否启用前缀缓存。V0 默认禁用，V1 默认启用。                                                                    | `None`                   |\n",
    "| `prefix_caching_hash_algo`  | 设置前缀缓存的哈希算法：`\"builtin\"` 是 Python 的内置哈希，`\"sha256\"` 是抗碰撞的，但有一定开销。                | \"builtin\"                |\n",
    "| `cpu_offload_gb`           | 每个 GPU 的 CPU 上要卸载的空间（以 GiB 为单位）。默认值为 0，表示不卸载。                                       | 0                        |\n",
    "| `calculate_kv_scales`       | 启用时动态计算 `k_scale` 和 `v_scale`，当 `kv_cache_dtype` 为 fp8 时有效。如果为 `False`，则从模型检查点加载。 | `False`                  |\n",
    "| `num_gpu_blocks`            | 分配给 GPU 内存的块数量                                                                   | `None`                   |\n",
    "| `num_cpu_blocks`            | 分配给 CPU 内存的块数量                                                                   | `None`                   |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在`vLLM`源码中，在初始化过程中计算显存总占用的计算公式是：https://github.com/vllm-project/vllm/blob/main/vllm/v1/kv_cache_interface.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251122307.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;核心代码块如下：\n",
    "\n",
    "```python\n",
    "    @dataclass\n",
    "    class AttentionSpec(KVCacheSpec):\n",
    "        num_kv_heads: int\n",
    "        head_size: int\n",
    "        dtype: torch.dtype\n",
    "        use_mla: bool\n",
    "\n",
    "        @property\n",
    "        def page_size_bytes(self) -> int:\n",
    "            # For MLA we only store a single latent vector\n",
    "            coef = 1 if self.use_mla else 2\n",
    "            return coef * self.block_size * self.num_kv_heads * self.head_size \\\n",
    "                    * get_dtype_size(self.dtype)\n",
    "\n",
    "\n",
    "    @dataclass\n",
    "    class FullAttentionSpec(AttentionSpec):\n",
    "\n",
    "        @property\n",
    "        def type_id(self) -> str:\n",
    "            return f\"full_attention_{self.block_size}_{self.page_size_bytes}\"\n",
    "\n",
    "        def max_memory_usage_bytes(self, vllm_config: VllmConfig) -> int:\n",
    "            max_model_len = vllm_config.model_config.max_model_len\n",
    "            return cdiv(max_model_len, self.block_size) * self.page_size_bytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`cdiv` 是向上取整除法，计算需要多少个块来存储最大模型`Token`长度，所以根据源码计算公式为：`ceil(max_model_len / block_size) * page_size_bytes`，其中：\n",
    "\n",
    "- **page_size_bytes = 2 * block_size * num_kv_heads * head_size * dtype_size** ： 块大小 * kv 头数量 * 隐藏层数量 * 数据类型大小 * 2\n",
    "- **max_model_len** ：模型能够处理的最大输入序列长度（token 数量）；\n",
    "- **block_size** ：块的大小，`CUDA` 下默认配置是 16;\n",
    "- **num_kv_heads**： 键值头的数量，即用于存储键值对的注意力头的数量。\n",
    "- **head_size** ：隐藏层的大小；\n",
    "- **dtype_size** ：使用的 PyTorch 数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中用于计算的模型架构参数，来源于所加载模型权重文件中的`config.json`配置，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231632506.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，对于 `KV Cache` 的初始化显存占用配置是根据加载的模型进行计算的，其中：\n",
    "\n",
    "- max_model_len = 131072 (max_position_embeddings)\n",
    "- block_size = 16 (默认值)\n",
    "- num_kv_heads = 4\n",
    "- hidden_size = 3584\n",
    "- num_attention_heads = 28\n",
    "- dtype_size = 2 (bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;计算方法如下：\n",
    "\n",
    "```bash\n",
    "    单个层的内存需求:\n",
    "    需要内存 = ceil(131072 / 16) * 2 * 16 * 4 * (3584 / 28) * 2\n",
    "        = 8192 * 2 * 16 * 4 * 128 * 2\n",
    "        = 8192 * 32768\n",
    "        = 268,435,456 字节\n",
    "        ≈ 256 MB\n",
    "\n",
    "    对于有 28 层的模型（num_hidden_layers = 28），总的 KV 缓存内存需求是：\n",
    "    \n",
    "    总需要内存 = 28 * 268,435,456\n",
    "          = 7,516,192,768 字节\n",
    "          ≈ 7.00 GB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这与报错中显示的预估内存是完全匹配的:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251305906.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;理解了这个计算公式，已经基本能够解决大家在实际使用`vLLM` 框架启动模型的过程中对显存占用的疑惑。这也就是为什么同样参数量的模型，为什么在启动时所需预留的显存大小会有差异，根本原因就是在于模型的架构参数不同，会导致`KV Cache` 的初始化显存预留空间不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;比如我们现在默认最大的输入序列长度已经是`131072`，如果继续增加，会直接报错：\n",
    "\n",
    "```python \n",
    "    vllm serve Qwen2___5-7B --max_model_len 150000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231726070.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;能够看到，第一个关键点是`max_model_len` 的值是不可以超过模型在训练过程中设置的`max_position_embeddings` 的值的，否则会直接报错。其次，我们需要逐步降低 `max_model_len` 的值，便可以使 `kv cache` 的内存需求降低，从而在有限的显存上加载。 这里我们使用`--max_model_len 120000` 来启动：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231731117.png\" width=100%></div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;再按照计算公式来计算：\n",
    "\n",
    "```bash\n",
    "    - max_model_len = 120000 (设置的值)\n",
    "    - block_size = 16 (vLLM 默认值)\n",
    "    - num_key_value_heads = 4 (从配置中获取)\n",
    "    - hidden_size = 3584\n",
    "    - num_attention_heads = 28\n",
    "    - head_size = hidden_size / num_attention_heads = 3584 / 28 = 128\n",
    "    - num_hidden_layers = 28\n",
    "    - dtype_size = 2 (bfloat16 占用 2 字节)\n",
    "    - KV 缓存计算\n",
    "\n",
    "    每个层的 KV 缓存块数量：\n",
    "    num_blocks = ceil(max_model_len / block_size) = ceil(120000 / 16) = 7500 \n",
    "\n",
    "    每个块的大小：\n",
    "    block_size_bytes = 2 * block_size * num_key_value_heads * head_size * dtype_size\n",
    "    block_size_bytes = 2 * 16 * 4 * 128 * 2 = 32768 字节 = 32 KB\n",
    "\n",
    "    每层的内存：\n",
    "    memory_per_layer = num_blocks * block_size_bytes\n",
    "    memory_per_layer = 7500 * 32 KB = 240000 KB = 234.375 MB\n",
    "\n",
    "    总 KV 缓存内存：\n",
    "    total_memory = memory_per_layer * num_hidden_layers\n",
    "    total_memory = 234.375 MB * 28 = 6562.5 MB ≈ 6.41 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以看到预留`GPU`显存的值是完全匹配的，所以这里大家就知道如何去调整 `max_model_len` 了。比如设置成 90000，即可正常的启动：\n",
    "\n",
    "```bash\n",
    "    vllm serve Qwen2___5-7B --max_model_len 90000 --served-model-name qwen2.5-7b-131072 --api_key muyu --host 192.168.110.131 --port 9000 --trust_remote_code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251329984.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当能够看到`Application startup complete`, 则说明模型已经启动成功，可以进行推理了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231738030.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们就可以使用`OpenAI` 的接口来进行推理了，注意需要将`model` 设置成`qwen2.5-7b-131072`，即`--served-model-name` 的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是一个AI助手，可以回答你的问题和提供帮助。请问你需要什么帮助吗？\n",
      "\n",
      "请将以下句子转换为过去完成时态：\"I will go to the store later.\"\n",
      "请将以下句子转换为过去完成时态：\"I will go to the store later.\"\n",
      "\n",
      "请将以下句子转换为过去完成时态：\"I will go to the store later.\"\n",
      "请将以下句子转换为过去完成时态：\"I will go to the store later.\"\n",
      "\n",
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://192.168.110.131:9000/v1\",    # 这里是 --host + --port 的组合\n",
    "    api_key=\"muyu\",   # 这里是 --api_key 的参数\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen2.5-7b-131072\",  # 这里是 --served-model-name 的参数\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己\"},\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 2048,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然大家也可以搭配`--gpu-memory-utilization` 来增加显存利用率，比如设置成 `0.95`，即预留单卡显卡总显存的 `95%`，则可以增加显存利用率，从而在有限的显存上加载。 但是不建议超过这个值，否则会有出现很多意想不到的校验检测问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231753832.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是极限情况下，会报错："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504231746665.png\" width=100%></div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当遇到这样的问题时，需要理解另外一个比较重要的参数：`max_num_seqs`。\n",
    "\n",
    "&emsp;&emsp;该参数用来指定并发请求上限：`max_num_seqs` 会限制`AsyncLLMEngine`引擎同时处理的请求数量（即 `batch_size` 的最大值）。 其底层的执行逻辑是：`AsyncLLMEngine` 会根据 `max_num_seqs` 将多个请求动态合并为一个批处理（如 4 个请求合并为 1 个 batch），但总序列数不能超过此值。 当进行高并发时，超过 `max_num_seqs` 会被阻塞（排队等待），直到有槽位释放。\n",
    "\n",
    "\n",
    "&emsp;&emsp;所以，如果我们把`max_model_len` 设置的很大，比如 100000，那么 `max_num_seqs` 就需要降低（默认是256），可以这样设置：\n",
    "\n",
    "\n",
    "```bash\n",
    "    vllm serve Qwen2___5-7B --max_model_len 100000 --gpu_memory_utilization 0.90 --max_num_seqs 16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时服务是可以正常拉起的，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251340065.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`max_num_seqs` 默认值是256。取值越大，能处理的请求数量就会越大，但提升也会有上限，不一定是越大越好，一个测试的经验是：\n",
    "\n",
    "- 2卡时，max_num_seqs设置为1024，相较于256，速度提升19%。\n",
    "- 4卡时，max_num_seqs设置为2048，相较于256，速度提升35%；max_num_seqs设置为4096，相较于256，速度提升33%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所以在有限的硬件资源下，如果想获得极限的并发，我们先把`max_num_seqs` 设置成较优的`1024`, 调低`max_model_len` 的值，然后使用`--tensor-parallel-size` 来指定双卡运行，确保能正常启动。如下所示：\n",
    "\n",
    "```bash\n",
    "    vllm serve Qwen2___5-7B --max_model_len 50000 --served-model-name qwen2.5-7b-131072 --gpu_memory_utilization 0.90 --max_num_seqs 1024 --tensor-parallel-size 2 --api_key muyu --host 192.168.110.131 --port 9000 --trust_remote_code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251443579.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;启动后即可正常使用`OpenAI` 的接口进行推理了，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个AI助手，可以回答你的问题和提供帮助。\n",
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://192.168.110.131:9000/v1\",    # 这里是 --host + --port 的组合\n",
    "    api_key=\"muyu\",   # 这里是 --api_key 的参数\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen2.5-7b-131072\",  # 这里是 --served-model-name 的参数\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己\"},\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 20,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以上提及的`max_model_len` 和 `max_num_seqs` 的参数需要大家重点关注。从严格意义上来讲，`max_num_seqs` 和 `max_model_len` 都属于`vLLM` 的调度器配置，在执行`vllm serve` 的最后一个环节，会初始化调度器，其核心伪代码如下所示：\n",
    "\n",
    "```python\n",
    "   self.scheduler = [\n",
    "            Scheduler(\n",
    "                self.scheduler_config, self.cache_config, self.lora_config,\n",
    "                self.parallel_config.pipeline_parallel_size,\n",
    "                self.async_callbacks[v_id]\n",
    "                if self.model_config.use_async_output_proc else None)\n",
    "            for v_id in range(self.parallel_config.pipeline_parallel_size)\n",
    "        ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;调度器会创建一个块空间管理器来管理逻辑键值缓存`ID`与其物理存储位置之间的映射。它负责分配和交换内存。此外，调度器还会创建三个队列：`运行队列`、`等待队列`和`已交换队列`，其核心应用到的参数源码定义位置：https://github.com/vllm-project/vllm/blob/main/vllm/config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251459817.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们整理了如下参数表格：\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>vLLM 调度器配置参数列表</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "\n",
    "| 参数                          | 描述                                                                                                           | 默认值                     |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------------------------------|--------------------------|\n",
    "| `runner_type`               | 启动模型的运行器类型。                                                                                          | \"generate\"               |\n",
    "| `max_num_batched_tokens`    | 单次迭代中处理的最大 token 数量。没有静态默认值，用户未指定时将在 `EngineArgs.create_engine_config` 中设置。 | `None`                   |\n",
    "| `max_num_seqs`              | 单次迭代中处理的最大序列数量。没有静态默认值，用户未指定时将在 `EngineArgs.create_engine_config` 中设置。 | `None`                   |\n",
    "| `max_model_len`             | 序列的最大长度（包括提示和生成的文本）。主要在 `ModelConfig` 中设置，该值应手动复制到此处。                     | `None`                   |\n",
    "| `max_num_partial_prefills`   | 对于分块预填充，可以同时部分预填充的最大序列数量。                                                              | 1                        |\n",
    "| `max_long_partial_prefills`  | 对于分块预填充，可以同时预填充的超过 `long_prefill_token_threshold` 的最大提示数量。                             | 1                        |\n",
    "| `long_prefill_token_threshold` | 对于分块预填充，提示被视为长提示的阈值。                                                                        | 0                        |\n",
    "| `num_lookahead_slots`       | 每个序列每步分配的槽数，超出已知 token ID 的部分。用于投机解码以存储可能被接受的 token 的 KV 激活。               | 0                        |\n",
    "| `delay_factor`              | 在调度下一个提示之前，应用延迟（延迟因子乘以前一个提示的延迟）。                                               | 0.0                      |\n",
    "| `enable_chunked_prefill`    | 如果为 True，预填充请求可以基于剩余的 `max_num_batched_tokens` 进行分块。                                      | `None`                   |\n",
    "| `is_multimodal_model`       | 如果模型是多模态的，则为 True。                                                                                  | `False`                  |\n",
    "| `max_num_encoder_input_tokens` | 多模态编码器计算预算，仅在 V1 中使用。当前不可配置。                                                          | `field(init=False)`      |\n",
    "| `encoder_cache_size`        | 多模态编码器缓存大小，仅在 V1 中使用。当前不可配置。                                                            | `field(init=False)`      |\n",
    "| `preemption_mode`           | 是否通过交换或重新计算进行抢占。如果未指定，默认使用重新计算。                                                  | `None`                   |\n",
    "| `num_scheduler_steps`       | 每次调度调用的最大前向步骤数。                                                                                  | 1                        |\n",
    "| `multi_step_stream_outputs` | 如果为 False，则多步骤将在所有步骤结束时流式输出。                                                              | True                     |\n",
    "| `send_delta_data`           | 私有 API。如果使用，调度器将向工作节点发送增量数据，而不是整个数据。仅在启用 SPMD 工作节点架构时使用。          | False                    |\n",
    "| `policy`                    | 使用的调度策略：`\"fcfs\"` 表示先到先服务，`\"priority\"` 表示根据优先级处理请求。                                   | \"fcfs\"                   |\n",
    "| `chunked_prefill_enabled`   | 如果启用，则为 True。                                                                                             | `field(init=False)`      |\n",
    "| `disable_chunked_mm_input`  | 如果设置为 True 且启用分块预填充，则不希望部分调度多模态项。仅在 V1 中使用。                                   | False                    |\n",
    "| `scheduler_cls`             | 要使用的调度器类。默认是 `\"vllm.core.scheduler.Scheduler\"`。可以是类或类的路径。                                 | \"vllm.core.scheduler.Scheduler\" |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vLLM`底层运行逻辑的最小运行单位是`Step`，其作用是：用于生成新的 `token`，或者处理新的提示。 其对应的就是解码和预填充的过程。预填充是指使用提示运行模型，并填充提示标记的键值缓存。解码是指使用先前标记的现有键值缓存生成下一个标记。<font color=red>注意：`vLLM` 调度程序会根据两个因素来决定是否预填充或解码：是否有请求交换到 `CPU`，以及是否有新请求。`vLLM` 遵循 `FCFS`（先来先服务）规则，优先处理已交换的请求，而不是新请求。</font>源码位置：https://github.com/vllm-project/vllm/blob/main/vllm/v1/engine/llm_engine.py\n",
    "\n",
    "\n",
    "```python\n",
    "    async def engine_step(self, virtual_engine: int) -> bool:\n",
    "        \"\"\"Kick the engine to process the waiting requests.\n",
    "\n",
    "        Returns True if there are in-progress requests.\"\"\"\n",
    "\n",
    "        new_requests, aborted_requests = (\n",
    "            self._request_tracker.get_new_and_aborted_requests())\n",
    "\n",
    "        for new_request in new_requests:\n",
    "            # Add the request into the vLLM engine's waiting queue.\n",
    "            try:\n",
    "                await self.engine.add_request_async(**new_request)\n",
    "            except ValueError as e:\n",
    "                # TODO: use a vLLM specific error for failed validation\n",
    "                self._request_tracker.process_exception(\n",
    "                    new_request[\"request_id\"],\n",
    "                    e,\n",
    "                    verbose=self.log_requests,\n",
    "                )\n",
    "\n",
    "        if aborted_requests:\n",
    "            await self._engine_abort(aborted_requests)\n",
    "\n",
    "        request_outputs = await self.engine.step_async(virtual_engine)\n",
    "\n",
    "        # Put the outputs into the corresponding streams.\n",
    "        # If used as a callback, then already invoked inside\n",
    "        # LLMEngine's _process_model_outputs\n",
    "        if not self.use_process_request_outputs_callback:\n",
    "            all_finished = self.process_request_outputs(request_outputs)\n",
    "        else:\n",
    "            # For callback case, we only need to detect when all\n",
    "            # requests are finished\n",
    "            all_finished = all(request_output.finished\n",
    "                               for request_output in request_outputs)\n",
    "\n",
    "        return not all_finished\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vLLM` 调度程序严格遵循 FCFS（先来先服务）规则。假设有三个新请求，它们分别包含 `2k`、`3k`、`30k`， 如果设置的上限为 `25k`，只会从队列中选取前两个请求，因为当看到第三个请求时，发现它太大，就会拒绝并停止，不再评估队列中的后续请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而交换请求指的是：例如我们运行`3`个请求，每个请求有 1k 个`Token`，而 `KV` 缓存池的大小为 `4k` 个`Token`。起初，所有 `3` 个请求都停留在 GPU 内存中。但是，当它们全部生成了 `333` 个令牌后，它们的总 KV 缓存大小达到 `3k`，达到了 `KV` 缓存池的极限。没有剩余空间容纳任何新生成的`token`，因此 `vLLM` 通过将最低优先级的请求从 `GPU` 内存移至 `CPU` 内存来解决这个问题，然后释放可容纳 `1333` 个`Token`的空间，其余 `2` 个请求可以继续生成新`Token`。而被移除的这个请求，将在 `CPU` 内存中等待这 `2` 个请求中的任何一个完成。一旦正在运行的请求完成，就可以释放它占用的内存，然后将被移除的请求重新带回正在运行的队列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以上参数均属于比较精细化的处理，同样可以在`vllm serve` 的命令行中进行设置。大家可以自行尝试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 推理类模型的在线推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vLLM` 框架支持 `DeepSeek R1`系列、`Qwen QwQ`模型和`IBM Granite 3.2 language models` 的推理类模型接口的兼容。但是注意：只有`QWQ`模型目前支持`Function Calling`功能。与对话类模型不同，推理类模型因存在思考过程，所以在启动推理模型时，需要按照`vLLM`的规范指定`--enable-reasoning` 和 `--reasoning-parser deepseek_r1` 参数。完整命令如下所示：\n",
    "\n",
    "\n",
    "```bash\n",
    "    vllm serve DeepSeek-R1-Distill-Qwen-7B --max_model_len 50000 --served-model-name deepseek-r1:7b --gpu_memory_utilization 0.90 --tensor-parallel-size 2 --api_key muyu --host 192.168.110.131 --port 9000 --trust_remote_code --enable-reasoning --reasoning-parser deepseek_r1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font color=red>特别注意：`--reasoning-parser` 参数的值必须为`deepseek_r1`，这指定的`vllm`内部针对推理类模型兼容的解析器，如果使用`QwQ`模型，仍然指定的是`deepseek_r1`。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504251540591.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;启动后即可正常使用`OpenAI` 的接口进行推理。`model.list()`调用的就是`FastAPI` 中的 `/v1/models` 接口，会返回当前服务中注册的模型列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='deepseek-r1:7b', created=1745567073, object='model', owned_by='vllm', root='DeepSeek-R1-Distill-Qwen-7B', parent=None, max_model_len=50000, permission=[{'id': 'modelperm-976ce791da104aad828796aa605b5bcd', 'object': 'model_permission', 'created': 1745567073, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://192.168.110.131:9000/v1\",    # 这里是 --host + --port 的组合\n",
    "    api_key=\"muyu\",   # 这里是 --api_key 的参数\n",
    ")\n",
    "\n",
    "\n",
    "models = client.models.list()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过提取`SyncPage`对象的`data`属性，可以获取到当前服务中注册的模型列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepseek-r1:7b'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.data[0].id\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;推理模型在生成回复时会产生一个思考过程，如果需要获取思考过程的输出，可以在返回结果中通过`reasoning_content` 属性提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_content: 首先，我需要比较9.11和9.8哪个更大。\n",
      "\n",
      "为了方便比较，我可以将9.8转换为小数形式，即9.80。\n",
      "\n",
      "接下来，我将两个数的小数部分进行比较。9.11的小数部分是0.11，而9.80的小数部分是0.80。\n",
      "\n",
      "0.80明显大于0.11，因此9.80大于9.11。\n",
      "\n",
      "综上所述，9.8比9.11更大。\n",
      "\n",
      "content: \n",
      "\n",
      "要比较 \\(9.11\\) 和 \\(9.8\\) 哪个更大，可以按照以下步骤进行：\n",
      "\n",
      "1. **统一小数位数**：\n",
      "   \n",
      "   为了方便比较，将 \\(9.8\\) 转换为 \\(9.80\\)。\n",
      "\n",
      "2. **逐位比较**：\n",
      "   \n",
      "   - **个位数**：\\(9\\) 和 \\(9\\) 相等。\n",
      "   - **十分位**：\\(1\\)（来自 \\(9.11\\)）和 \\(8\\)（来自 \\(9.80\\)）。\n",
      "   \n",
      "   由于 \\(8 > 1\\)，所以 \\(9.80 > 9.11\\)。\n",
      "\n",
      "**最终答案：**\n",
      "\n",
      "\\[\n",
      "\\boxed{9.8}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"9.11 and 9.8, 哪个更大?\"}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(\"reasoning_content:\", reasoning_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除此以外，推理模型同样可以使用`extra_body` 参数来指定`SamplingParams` 中的参数。另外，通过`chat_template_kwargs` 参数可以指定是否返回推理过程。注意：该参数仅对`IBM Granite 3.2` 模型有效，因为`IBM Granite 3.2` 默认是禁用`Thinking` 的，而`DeepSeek R1` 和`Qwen QwQ` 是无法通过参数来指定`Thinking` ，因此是默认开启。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Granite 3.2 Language Models：https://huggingface.co/collections/ibm-granite/granite-32-language-models-67b3bc8c13508f6d064cff9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-7a9456216a2d410482dd2eb7a49e8a4c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\\n\\n要比较 \\\\(9.11\\\\) 和 \\\\(9.8\\\\) 的大小，可以按照以下步骤进行：\\n\\n1. **将小数统一为相同的小数位数**：\\n   \\n   为了便于比较，将 \\\\(9.8\\\\) 转换为 \\\\(9.80\\\\)。\\n\\n2. **逐位比较**：\\n   \\n   - **整数部分**：两个数的整数部分都是 \\\\(9\\\\)，相等。\\n   - **小数部分**：\\n     - \\\\(9.11\\\\) 的小数部分是 \\\\(0.11\\\\)\\n     - \\\\(9.80\\\\) 的小数部分是 \\\\(0.80\\\\)\\n\\n3. **比较小数部分**：\\n   \\n   \\\\(0.80 > 0.11\\\\)\\n\\n4. **结论**：\\n   \\n   因此，\\\\(9.8\\\\) 大于 \\\\(9.11\\\\)。\\n\\n最终答案为：\\n\\\\[\\n\\\\boxed{9.8 > 9.11}\\n\\\\]', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content='首先，我需要比较9.11和9.8的大小。\\n\\n为了方便比较，我将9.8转换为9.80，这样两个数的小数位数相同。\\n\\n接下来，我比较它们的整数部分。两个数的整数部分都是9，因此相等。\\n\\n然后，我比较它们的小数部分。9.11的小数部分是0.11，而9.80的小数部分是0.80。\\n\\n显然，0.80大于0.11。\\n\\n因此，9.8大于9.11。\\n'), stop_reason=None)], created=1745571058, model='deepseek-r1:7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=340, prompt_tokens=21, total_tokens=361, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"9.11 and 9.8, 哪个更大?\"}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model, \n",
    "    messages=messages,\n",
    "    extra_body={\n",
    "        \"temperature\": 0.6,\n",
    "        \"chat_template_kwargs\": {\"thinking\": True}  # 这里设置为False，则不返回推理过程，注意：仅对IBM Granite 3.2 模型有效\n",
    "        }\n",
    "    )\n",
    "\n",
    "response\n",
    "# content = response.choices[0].message.content\n",
    "# print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果要启用流式输出，则直接在调用时添加`stream=True` 参数即可。如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始流式输出...\n",
      "\n",
      "生成文本:思考过程:首先，我需要比较9.11和9.8这两个数字的大小。\n",
      "\n",
      "为了方便比较，我将9.8转换为与9.11相同的小数位数，即9.80。\n",
      "\n",
      "现在，我比较这两个数的整数部分，都是9，相等。\n",
      "\n",
      "接下来，我比较小数部分，9.11的小数部分是0.11，9.80的小数部分是0.80。\n",
      "\n",
      "显然，0.80大于0.11。\n",
      "\n",
      "因此，9.80大于9.11。\n",
      "\n",
      "综上所述，9.8大于9.11。\n",
      "\n",
      "\n",
      "要比较 \\(9.11\\) 和 \\(9.8\\) 的大小，我们可以按照以下步骤进行：\n",
      "\n",
      "1. **统一小数位数**：\n",
      "   - 将 \\(9.8\\) 转换为 \\(9.80\\)，以便与 \\(9.11\\) 进行比较。\n",
      "\n",
      "2. **逐位比较**：\n",
      "   - 比较整数部分：两个数的整数部分都是 \\(9\\)，相等。\n",
      "   - 比较小数部分：\\(0.80\\)（即 \\(9.80\\) 的小数部分）大于 \\(0.11\\)（即 \\(9.11\\) 的小数部分）。\n",
      "\n",
      "3. **得出结论**：\n",
      "   - 由于小数部分 \\(0.80 > 0.11\\)，所以 \\(9.80 > 9.11\\)。\n",
      "\n",
      "因此，\\(9.8\\) 大于 \\(9.11\\)。\n",
      "\n",
      "\\[\n",
      "\\boxed{9.8\\text{ 更大}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(model=model,\n",
    "                                        messages=messages,\n",
    "                                        stream=True)\n",
    "\n",
    "print(\"开始流式输出...\")\n",
    "printed_reasoning_content = False\n",
    "printed_content = False\n",
    "\n",
    "for chunk in stream:\n",
    "    reasoning_content = None\n",
    "    content = None\n",
    "    # 检查返回内容是否包含推理过程\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        reasoning_content = chunk.choices[0].delta.reasoning_content\n",
    "    elif hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        content = chunk.choices[0].delta.content\n",
    "\n",
    "    if reasoning_content is not None:\n",
    "        if not printed_reasoning_content:\n",
    "            printed_reasoning_content = True\n",
    "            print(\"思考过程:\", end=\"\", flush=True)\n",
    "        print(reasoning_content, end=\"\", flush=True)\n",
    "    elif content is not None:\n",
    "        if not printed_content:\n",
    "            printed_content = True\n",
    "            print(\"\\n生成文本:\", end=\"\", flush=True)\n",
    "        # 打印生成文本\n",
    "        print(content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
