{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5961d4be-674e-444c-8637-d1935d4cd499",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <font face=\"ä»¿å®‹\">è¯¾ç¨‹è¯´æ˜ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd3d26-46cb-4047-a92e-1aa8c48a582e",
   "metadata": {},
   "source": [
    "- ä½“éªŒè¯¾å†…å®¹èŠ‚é€‰è‡ª[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(ç§‹æ‹›å†²åˆºç­)](https://ix9mq.xetslk.com/s/2S2Vpy)å®Œæ•´ç‰ˆä»˜è´¹è¯¾ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a1acc-303c-4ded-9f71-f9272f714611",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ä½“éªŒè¯¾æ—¶é—´æœ‰é™ï¼Œè‹¥æƒ³æ·±åº¦å­¦ä¹ å¤§æ¨¡å‹æŠ€æœ¯ï¼Œæ¬¢è¿å¤§å®¶æŠ¥åç”±æˆ‘ä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(ç§‹æ‹›å†²åˆºç­)](https://ix9mq.xetslk.com/s/2S2Vpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4860662-450c-48e2-befd-d451f3bb7779",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/b3a518f1a9821408a79363cf694f5172.jpg\" alt=\"b3a518f1a9821408a79363cf694f5172\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025ab78-f1d2-4244-892d-5878cbc15430",
   "metadata": {},
   "source": [
    "**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(ç§‹æ‹›å†²åˆºç­)](https://ix9mq.xetslk.com/s/2S2Vpy)ä¸ºã€100+å°æ—¶ã€‘ä½“ç³»å¤§è¯¾ï¼Œæ€»å…±20å¤§æ¨¡å—ç²¾è®²ç²¾æï¼Œé›¶åŸºç¡€ç›´è¾¾å¤§æ¨¡å‹ä¼ä¸šçº§åº”ç”¨ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebb635-69a6-49e1-97a1-2ad89d693b63",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506172010074.png\" alt=\"a55d48e952ed59f8d93e050594843bc\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67f848-3d95-41df-b36b-2cd369ed71a9",
   "metadata": {},
   "source": [
    "éƒ¨åˆ†é¡¹ç›®æˆæœæ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df68f223-921d-4c8a-a5d0-8f8e02c76b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6da6ff-b5c2-4eee-a365-813ec792b711",
   "metadata": {},
   "source": [
    "- **MateGené¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3366f0-ddf5-40f4-8be4-7451cfeefc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/MG%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/MG%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312dedc4-00d4-4064-b94e-8c51741104a7",
   "metadata": {},
   "source": [
    "- **æ™ºèƒ½å®¢æœé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42edbc3-b18d-4337-96af-1bcea59425da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E8%A7%86%E9%A2%91.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E8%A7%86%E9%A2%91.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e4e69-d24c-496c-89df-a0373ef0fa37",
   "metadata": {},
   "source": [
    "- **Difyé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226a85cf-0d4c-4fd1-8882-b12364cf9acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26637d82-13f4-4799-a342-5cef3575c08a",
   "metadata": {},
   "source": [
    "- **LangChain&LangGraphæ­å»ºMulti-Agnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4524b1f8-4f66-4624-a3fb-2e66581a7b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746611f0-cfd8-43b5-a846-b78220af4510",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œè‹¥æ˜¯å¯¹å¤§æ¨¡å‹åº•å±‚åŸç†æ„Ÿå…´è¶£ï¼Œä¹Ÿæ¬¢è¿æŠ¥åç”±æˆ‘å’Œèœèœè€å¸ˆå…±åŒä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹åŸç†ä¸å®æˆ˜è¯¾ç¨‹ã€‹(ç§‹æ‹›å†²åˆºç­)](https://ix9mq.xetslk.com/s/3AME7R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a1b9f-b705-4c0d-8af4-1ac6e8f36932",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/aaf3bafd8ff8120d5fb079f092268961.png\" alt=\"aaf3bafd8ff8120d5fb079f092268961\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29db0c1-7429-4221-b439-c46084556756",
   "metadata": {},
   "source": [
    "**ä¸¤é—¨å¤§æ¨¡å‹è¯¾ç¨‹ç§‹æ‹›å†²åˆºç­é¢„å”®è¿›è¡Œä¸­ï¼Œç›´æ’­é—´å¯äº«è¶…å€¼ç‰¹ä»·+å…¨å¥—å­¦ä¹ ç¦åˆ©ï¼Œåˆè´­è¿˜æœ‰æ›´å¤šä¼˜æƒ å“¦~<span style=\"color:red;\">è¯¦ç»†ä¿¡æ¯æ‰«ç æ·»åŠ åŠ©æ•™ï¼Œå›å¤â€œå¤§æ¨¡å‹â€ï¼Œå³å¯é¢†å–è¯¾ç¨‹å¤§çº²&æŸ¥çœ‹è¯¾ç¨‹è¯¦æƒ…ğŸ‘‡</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2db96-0d4b-4ebf-9d82-e3f4d785aa50",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/26449c9c3e90ea66e0af9150ad00e0c6.png\" alt=\"26449c9c3e90ea66e0af9150ad00e0c6\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca27f1-6cd3-4307-a010-e9ffeb7c9fe7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/bff56b8959b8a17d9d0a6d1e72f5d9b8.png\" alt=\"bff56b8959b8a17d9d0a6d1e72f5d9b8\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa736e25-0b0c-4b34-9a76-af416338eb8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e146c6-0d8f-44bd-8843-0897d46d504f",
   "metadata": {},
   "source": [
    "# <center>GPT-OSSé«˜æ•ˆå¾®è°ƒå®æˆ˜</center>\n",
    "# <center>Part 4.GPT-OSS Agentæ€§èƒ½å¾®è°ƒ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0cffd-d40b-4416-9e90-6cc45583b60e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;åœ¨ç†Ÿæ‚‰äº†Unslothæ ¸å¿ƒåŠŸèƒ½ä»¥åŠåŸºç¡€çš„å¾®è°ƒæ•°æ®é›†å‡†å¤‡æ–¹æ³•ä¹‹åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•è¿›è¡Œä¸€é¡¹æ›´åŠ å¤æ‚çš„å¾®è°ƒå·¥ä½œâ€”â€”å›´ç»•GPT-OSSæ¨¡å‹è¿›è¡ŒAgentæ€§èƒ½å¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a74b7-b493-4f16-96af-989af2b1dc67",
   "metadata": {},
   "source": [
    "## ä¹ã€Agentæ€§èƒ½å¾®è°ƒæ•°æ®é›†å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9429e-a422-4fa5-80b3-7c9d89f41874",
   "metadata": {},
   "source": [
    "### 1. Hermes tool useæ•°æ®é›†ä»‹ç»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d52f16-50b2-429a-b135-64af9428f79a",
   "metadata": {},
   "source": [
    "- æ•°æ®é›†åœ°å€ï¼šhttps://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156c4c6-2a6f-46c4-8194-441615696ead",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250825191926108.png\" alt=\"image-20250825191926108\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf9820-c3ec-48e4-bf4c-35100ef80e98",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**interstellarninja / hermes_reasoning_tool_use** æ˜¯ä¸€ä¸ªå›´ç»•â€œä¼šæ€è€ƒã€ä¼šç”¨å·¥å…·â€çš„å¼€æºæ•°æ®é›†ï¼šé‡‡ç”¨ Apache-2.0 è®¸å¯ã€å•ä¸€ `train` åˆ‡åˆ†çº¦ **51,004** æ¡æ ·æœ¬ï¼ˆçº¦ 392 MBï¼‰ï¼Œä»¥ ShareGPT ä¼šè¯æ ¼å¼å­˜å‚¨ï¼ˆParquetï¼‰ï¼Œå­—æ®µå« `conversations / tools / task / category / source / scenario_category`ï¼Œä¾¿äºç›´æ¥ç”¨  Datasets/axolotl åŠ è½½å¾®è°ƒã€‚å…¶å¯¹è¯æ¨¡æ¿åœ¨ `system` ä¸­æ˜ç¡®è¦æ±‚**å°†å†…åœ¨æ¨ç†å†™å…¥ `<think>â€¦</think>`**ï¼Œå¹¶ä»¥ `<tool_call>â€¦</tool_call>` è¾“å‡º **OpenAI é£æ ¼çš„å‡½æ•°è°ƒç”¨ JSONï¼ˆå« pydantic schemaï¼‰**ï¼Œéå¸¸å¥‘åˆâ€œæ€è€ƒé“¾ + å‡½æ•°è°ƒç”¨â€çš„ç›‘ç£ä¿¡å·ã€‚\n",
    " æ ·æœ¬è¦†ç›– BFCL v3 çš„å››ç±»åœºæ™¯ï¼š**single_turn**ã€**multi_turn**ã€**multi_step** ä¸ **relevance**ï¼ˆæ— åˆé€‚å·¥å…·æ—¶åº”æ‹’ç»ï¼‰ï¼Œæœ‰åŠ©äºè®­ç»ƒæ¨¡å‹â€œä½•æ—¶/å¦‚ä½•/æ˜¯å¦â€è°ƒç”¨å·¥å…·ã€‚æ„å»ºæµç¨‹åŸºäº **Nous Research çš„ Atropos RL æ ˆ**ï¼Œåœ¨è‡ªå®šä¹‰ `MultiTurnToolCallingEnv` ä¸­è¿›è¡Œ **GRPO** roll-outsï¼Œå¹¶é€šè¿‡å¥–åŠ±å¡‘å½¢ä¸ä¸¥æ ¼æ ¡éªŒï¼ˆå¦‚ `_validate_think_plus_calls`ã€`_check_sequential_tools`ï¼‰ä»…ä¿ç•™ç»“æ„ä¸è¯­ä¹‰å‡åˆæ ¼çš„è½¨è¿¹ã€‚\n",
    "\n",
    "&emsp;&emsp;å¹¶ä¸”ï¼Œæ•°æ®é›†åŒ…å«å››ç§è°ƒç”¨æ ¼å¼ï¼Œ`single_turn`ï¼ˆå•è½®å•è°ƒç”¨ï¼‰ã€`multi_turn`ï¼ˆå¤šè½®å¤šè°ƒç”¨ï¼‰ã€`multi_step`ï¼ˆå•è½®å†…**â‰¥2 é¡ºåºè°ƒç”¨**ï¼‰ã€`relevance`ï¼ˆ**æ— åˆé€‚å·¥å…·åº”æ‹’ç»**ï¼‰ã€‚è¿™å¤©ç„¶è¦†ç›–äº†â€œä½•æ—¶/å¦‚ä½•/æ˜¯å¦â€è°ƒç”¨å·¥å…·çš„å†³ç­–å­¦ä¹ ã€‚\n",
    "\n",
    "&emsp;&emsp;æ€»ä½“è€Œè¨€ï¼Œè¿™æ˜¯ä¸€ä¸ª**å…¼å…·é•¿é“¾æ¨ç†æ ‡æ³¨ä¸å·¥å…·è°ƒç”¨è½¨è¿¹**çš„å®ç”¨æ•°æ®é›†ï¼Œæ—¢å¯ç”¨äº **SFT**ï¼Œä¹Ÿå¯ä½œä¸º **GRPO é¢„çƒ­**æˆ–è¯„æµ‹åŸºåº§ï¼Œå¿«é€Ÿæå‡æ¨¡å‹åœ¨çœŸå®ä»£ç†åœºæ™¯ä¸‹çš„å·¥å…·ä½¿ç”¨ç¨³å¥æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdde1e5-a46d-4f3d-8142-e6265cd1fdbc",
   "metadata": {},
   "source": [
    "- æ•°æ®é›†è¯»å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b16ff39f-4a98-4271-8bc8-9883fd346371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, Features, Value\n",
    "import random, json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd4eaa0-d7fa-4b1a-bb2b-7450892ad83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since interstellarninja/hermes_reasoning_tool_use couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since interstellarninja/hermes_reasoning_tool_use couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/interstellarninja___hermes_reasoning_tool_use/default/0.0.0/55d824b623303055d5a76eb6ab12861b80a4ee20 (last modified on Mon Aug 25 20:12:37 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/interstellarninja___hermes_reasoning_tool_use/default/0.0.0/55d824b623303055d5a76eb6ab12861b80a4ee20 (last modified on Mon Aug 25 20:12:37 2025).\n"
     ]
    }
   ],
   "source": [
    "# åŸå§‹æ•°æ®\n",
    "ds_full = load_dataset(\"interstellarninja/hermes_reasoning_tool_use\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0e3d022-1da1-4b60-bd19-eb785f2320f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions. Here are the available tools: <tools> [{\"name\": \"calculate_age\", \"description\": \"Calculates the age based on the birthdate.\", \"parameters\": {\"birthdate\": {\"description\": \"The birthdate in the format \\'YYYY-MM-DD\\'.\", \"type\": \"str\"}}}, {\"name\": \"find_duplicate_number\", \"description\": \"Finds the duplicate number in a list of integers.\", \"parameters\": {\"nums\": {\"description\": \"The list of integers.\", \"type\": \"List[int]\"}}}, {\"name\": \"word_frequency\", \"description\": \"Calculates the frequency of each word in a text.\", \"parameters\": {\"text\": {\"description\": \"The input text.\", \"type\": \"str\"}}}, {\"name\": \"reverse_words\", \"description\": \"Reverses the order of words in a sentence.\", \"parameters\": {\"sentence\": {\"description\": \"The input sentence.\", \"type\": \"str\"}}}] </tools> Use the following pydantic model JSON schema for each tool call you will make: {\\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}},\\'required\\': [\\'arguments\\', \\'name\\'], \\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\'} For each function call return a json object with function name and arguments within <tool_call> </tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\\n\\n'},\n",
       "  {'from': 'human',\n",
       "   'value': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': '<think>\\nOkay, the user has two separate questions here. The first is to find the word frequency of the sentence \"The quick brown fox jumps over the lazy dog\". The second is to reverse the words in the sentence \"Hello world this is a test\". Let me break down each part step by step.\\n\\nStarting with the word frequency: I need to use the word_frequency tool. The input text is given as a string, so I\\'ll pass that into the tool. That should return a dictionary where each key is a word and the value is its frequency.\\n\\nNext, the reverse the sentence part. The user wants the words reversed. That\\'s a straightforward reverse of words in the sentence. The reverse_words tool should handle this. The sentence is \"Hello world this is a test\", so I\\'ll pass that into the reverse_words function, which will return the sentence with words reversed.\\n\\nSo I need to make two separate function calls: one for word_frequency and another for reverse_words. I\\'ll use the word_frequency tool with the sentence as the text parameter, and reverse_words with the same sentence. The outputs from each tool will be returned as JSON objects.\\n</think>\\n<tool_call>\\n{\"name\":\"word_frequency\",\"arguments\":{\"text\":\"The quick brown fox jumps over the lazy dog\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\":\"reverse_words\",\"arguments\":{\"sentence\":\"Hello world this is a test\"}}\\n</tool_call>'}],\n",
       " 'tools': '[{\"name\": \"calculate_age\", \"description\": \"Calculates the age based on the birthdate.\", \"parameters\": {\"birthdate\": {\"description\": \"The birthdate in the format \\'YYYY-MM-DD\\'.\", \"type\": \"str\"}}}, {\"name\": \"find_duplicate_number\", \"description\": \"Finds the duplicate number in a list of integers.\", \"parameters\": {\"nums\": {\"description\": \"The list of integers.\", \"type\": \"List[int]\"}}}, {\"name\": \"word_frequency\", \"description\": \"Calculates the frequency of each word in a text.\", \"parameters\": {\"text\": {\"description\": \"The input text.\", \"type\": \"str\"}}}, {\"name\": \"reverse_words\", \"description\": \"Reverses the order of words in a sentence.\", \"parameters\": {\"sentence\": {\"description\": \"The input sentence.\", \"type\": \"str\"}}}]',\n",
       " 'task': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\",\n",
       " 'category': 'Word Frequency',\n",
       " 'source': 'Salesforce-Xlam',\n",
       " 'scenario_category': 'single'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ac872-4e8c-4862-b2b9-40db3c5d27e9",
   "metadata": {},
   "source": [
    "Hermes Reasoning Tool Use æ˜¯ä¸€ä¸ª **â€œæ¨ç† + å·¥å…·è°ƒç”¨ï¼ˆfunction callingï¼‰â€** çš„æ•™å­¦/å¾®è°ƒæ•°æ®é›†ã€‚å®ƒä¸ä»…åŒ…å«è‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œè¿˜æ˜¾å¼æ ‡æ³¨äº†ï¼š\n",
    "\n",
    "* ç³»ç»Ÿæç¤ºï¼ˆsystemï¼‰é‡Œæä¾›çš„**å·¥å…·ç­¾å**ï¼ˆå‡½æ•°åã€å‚æ•°ç­‰ï¼‰ï¼›\n",
    "* æ¨¡å‹åœ¨å¯¹è¯ä¸­**è°ƒç”¨å·¥å…·**çš„åŠ¨ä½œï¼ˆå‡½æ•°å + å‚æ•°ï¼‰ï¼›\n",
    "* å·¥å…·çš„**è¿”å›ç»“æœ**ï¼ˆå¸¦ `<tool_response>`ï¼‰ï¼›\n",
    "* æ¨¡å‹çš„**æ€ç»´è¿‡ç¨‹**ï¼ˆä½¿ç”¨ `<think>â€¦</think>` åŒ…è£¹çš„å†…åœ¨æ¨ç†ï¼‰ã€‚\n",
    "\n",
    "å› æ­¤å®ƒç‰¹åˆ«é€‚åˆè®­ç»ƒ/è¯„ä¼°ï¼š\n",
    "**ä½•æ—¶**è°ƒç”¨å“ªä¸ªå·¥å…·ã€**å¦‚ä½•**æ„é€ å‚æ•°ã€**å¦‚ä½•**æŠŠå·¥å…·ç»“æœèå…¥æœ€ç»ˆç­”å¤ã€ä»¥åŠ**å¤šæ­¥å·¥å…·è°ƒç”¨**çš„ç­–åˆ’ä¸æ‰§è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20148d9-a342-4146-9d2e-14ad2d121df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tools', 'task', 'category', 'source', 'scenario_category'],\n",
       "    num_rows: 51004\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80383f0f-80ab-4bb9-a8b1-315ed250e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\",\n",
       " \"Search for 'laptops' on Tmall and retrieve the second page of results.\",\n",
       " 'I would like to extract details from a LinkedIn company page. Could you assist me in fetching the information from this URL: https://www.linkedin.com/company/abc-corporation?',\n",
       " 'I need to compare the weights of two groups of athletes. Group X weighs [70, 72, 74, 76, 78] kg and Group Y weighs [71, 73, 75, 77, 79] kg. Perform the t-test with a significance level of 0.10.',\n",
       " 'I\\'d like to learn Spanish and need to understand my current proficiency level. Also, once I know my level, I need a translation of the following text: \"I want to learn Spanish quickly\" into Spanish.',\n",
       " 'Retrieve the metadata information for Ethereum (ETH) on the Binance exchange, including metrics related to the asset.',\n",
       " 'Can you perform a static analysis on floors 3, 7, and 11?',\n",
       " 'How can I view the analysis for Tesla, Inc. from a week ago?',\n",
       " 'I need to find out about battles between June 12th and July 15th involving Germany and France. What types of military events took place?',\n",
       " \"Generate a static map image of size 1600x1600 centered at the coordinates (55.7558, 37.6176) with a zoom level of 11, using the 'toursprung-terrain' maptype, and include a delta_zoom of 2. Output the URL.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full['task'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a634e412-cf72-4fcb-bfa4-83544978c436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Salesforce-Xlam',\n",
       " 'Nvidia-When2Call',\n",
       " 'ToolAce',\n",
       " 'Nvidia-When2Call',\n",
       " 'ToolAce',\n",
       " 'Nvidia-When2Call',\n",
       " 'Nvidia-When2Call',\n",
       " 'Nvidia-When2Call',\n",
       " 'ToolAce',\n",
       " 'Nvidia-When2Call']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full['source'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd840bb2-629b-4c63-ae54-b20ba58e029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['single',\n",
       " 'single',\n",
       " 'multiturn',\n",
       " 'single',\n",
       " 'multistep',\n",
       " 'relevance',\n",
       " 'relevance',\n",
       " 'relevance',\n",
       " 'relevance',\n",
       " 'single']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full['scenario_category'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759d30e-5076-42f7-8d7f-49cc393f6666",
   "metadata": {},
   "source": [
    "æ•°æ®é›†æ¯æ¡æ ·æœ¬é€šå¸¸åŒ…å«ä»¥ä¸‹é¡¶å±‚å­—æ®µï¼š\n",
    "\n",
    "* `conversations`ï¼š**å¯¹è¯è½®æ¬¡**ï¼ˆlistï¼‰ã€‚æ¯è½®æœ‰ï¼š\n",
    "\n",
    "  * `from`ï¼š`system` / `human` / `gpt` / `tool`\n",
    "  * `value`ï¼šè¯¥è½®æ–‡æœ¬ã€‚\n",
    "\n",
    "    * system è½®é‡Œå¸¸è§ `<tools> â€¦ </tools>`ï¼ˆæä¾›å‡½æ•°ç­¾åï¼‰\n",
    "    * gpt è½®é‡Œå¯èƒ½å« `<think>â€¦</think>` ä¸ä¸€ä¸ªæˆ–å¤šä¸ª `<tool_call> â€¦ </tool_call>`\n",
    "    * tool è½®é‡Œå« `<tool_response> â€¦ </tool_response>`\n",
    "* `tools`ï¼šå·¥å…·æ¸…å•ï¼ˆæœ‰æ—¶æ˜¯ **å­—ç¬¦ä¸²åŒ– JSON**ï¼Œæœ‰æ—¶æ˜¯ list\\[dict]ï¼‰ã€‚ä¸ system é‡Œçš„ `<tools>` ä¸€è‡´æˆ–äº’è¡¥ã€‚\n",
    "* `task`ï¼šè¯¥æ ·æœ¬çš„äººç±»ä»»åŠ¡ç®€è¿°ã€‚\n",
    "* `category`ï¼šä»»åŠ¡ç±»åˆ«ï¼ˆå¦‚ `Is Hotel Available`ã€`Tool Use`ã€`Get Stock Price` ç­‰ï¼‰ã€‚\n",
    "* `source`ï¼šæ¥æºï¼ˆå¦‚ `Nvidia-When2Call`ã€`ToolAce`ã€`Glaive`ã€`Nous-Hermes`ã€`Salesforce-Xlam`ï¼‰ã€‚\n",
    "* `scenario_category`ï¼šåœºæ™¯ç±»å‹ï¼š\n",
    "\n",
    "  * `single`ï¼šå•è½®å¯¹è¯ + å•æ¬¡æˆ–å¤šæ¬¡è°ƒç”¨ï¼ˆåœ¨ä¸€è½®å†…å‘èµ·å¤šä¸ªå·¥å…·è°ƒç”¨ï¼‰\n",
    "  * `multiturn`ï¼šå¤šè½®å¾€è¿”\n",
    "  * `multistep`ï¼šéœ€è¦æŒ‰é¡ºåºå®Œæˆå¤šä¸ªå·¥å…·æ­¥éª¤\n",
    "  * `relevance`ï¼šå¼ºè°ƒé€‰æ‹©**ç›¸å…³**å·¥å…·ï¼ˆå¹²æ‰°é¡¹è¾ƒå¤šï¼‰\n",
    "\n",
    "> âš ï¸ çœŸå®æ•°æ®é‡Œ **ç±»å‹å¹¶ä¸æ€»ç»Ÿä¸€**ï¼š\n",
    ">\n",
    "> * `tools` å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯èƒ½æ˜¯å¯¹è±¡æ•°ç»„ï¼›\n",
    "> * `parameters.type` å¯èƒ½å†™æˆ `\"dict\"`ï¼ˆç†æƒ³åº”ä¸º `\"object\"`ï¼‰ï¼›\n",
    "> * `required` å¯èƒ½æ”¾åœ¨å·¥å…·**å¤–å±‚**è€Œä¸æ˜¯ `parameters.required`ï¼›\n",
    "> * æœ‰çš„å±æ€§ç¼ºå¤± `description`ã€‚\n",
    ">   è¿™å°±æ˜¯æˆ‘ä»¬æ¸…æ´—è§„èŒƒåŒ–è¦è§£å†³çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d1fa5-5235-41f1-a0df-4cbb2b5c881f",
   "metadata": {},
   "source": [
    "- å•å·¥å…·è°ƒç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503bc12e-01c5-49f2-b547-2664c2d3dcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions. Here are the available tools: <tools> [{\"name\": \"calculate_age\", \"description\": \"Calculates the age based on the birthdate.\", \"parameters\": {\"birthdate\": {\"description\": \"The birthdate in the format \\'YYYY-MM-DD\\'.\", \"type\": \"str\"}}}, {\"name\": \"find_duplicate_number\", \"description\": \"Finds the duplicate number in a list of integers.\", \"parameters\": {\"nums\": {\"description\": \"The list of integers.\", \"type\": \"List[int]\"}}}, {\"name\": \"word_frequency\", \"description\": \"Calculates the frequency of each word in a text.\", \"parameters\": {\"text\": {\"description\": \"The input text.\", \"type\": \"str\"}}}, {\"name\": \"reverse_words\", \"description\": \"Reverses the order of words in a sentence.\", \"parameters\": {\"sentence\": {\"description\": \"The input sentence.\", \"type\": \"str\"}}}] </tools> Use the following pydantic model JSON schema for each tool call you will make: {\\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}},\\'required\\': [\\'arguments\\', \\'name\\'], \\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\'} For each function call return a json object with function name and arguments within <tool_call> </tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\\n\\n'},\n",
       " {'from': 'human',\n",
       "  'value': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\"},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the user has two separate questions here. The first is to find the word frequency of the sentence \"The quick brown fox jumps over the lazy dog\". The second is to reverse the words in the sentence \"Hello world this is a test\". Let me break down each part step by step.\\n\\nStarting with the word frequency: I need to use the word_frequency tool. The input text is given as a string, so I\\'ll pass that into the tool. That should return a dictionary where each key is a word and the value is its frequency.\\n\\nNext, the reverse the sentence part. The user wants the words reversed. That\\'s a straightforward reverse of words in the sentence. The reverse_words tool should handle this. The sentence is \"Hello world this is a test\", so I\\'ll pass that into the reverse_words function, which will return the sentence with words reversed.\\n\\nSo I need to make two separate function calls: one for word_frequency and another for reverse_words. I\\'ll use the word_frequency tool with the sentence as the text parameter, and reverse_words with the same sentence. The outputs from each tool will be returned as JSON objects.\\n</think>\\n<tool_call>\\n{\"name\":\"word_frequency\",\"arguments\":{\"text\":\"The quick brown fox jumps over the lazy dog\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\":\"reverse_words\",\"arguments\":{\"sentence\":\"Hello world this is a test\"}}\\n</tool_call>'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full[0]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e1e40-7c49-4e6a-a0ba-8cf9f461b375",
   "metadata": {},
   "source": [
    "- å¤šå·¥å…·å¹¶è¡Œè°ƒç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c90a355-d200-43a1-a615-7a876e9186dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. If available tools are not relevant in assisting with user query, just respond in natural conversational language. Don\\'t make assumptions about what values to plug into functions. After calling & executing the functions, you will be provided with function results within <tool_response> </tool_response> XML tags. Here are the available tools:\\n<tools>\\n[\\n{\"name\": \"products_v2_list\", \"description\": \"Fetches a list of products from the ASOS store with various filtering and sorting options.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"store\": {\"description\": \"The store identifier obtained from the countries/list API.\", \"type\": \"str\", \"default\": \"US\"}, \"offset\": {\"description\": \"The offset to skip already viewed products.\", \"type\": \"int\", \"default\": \"0\"}, \"categoryid\": {\"description\": \"The category identifier from the categories/list API.\", \"type\": \"int\", \"default\": \"4209\"}, \"limit\": {\"description\": \"The number of items per page.\", \"type\": \"int\", \"default\": \"48\"}, \"attribute_1046\": {\"description\": \"Filter by style, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"pricemin\": {\"description\": \"Minimum price filter.\", \"type\": \"int, optional\", \"default\": \"\"}, \"country\": {\"description\": \"Country code; default is \\'US\\'.\", \"type\": \"str, optional\", \"default\": \"US\"}, \"attribute_10147\": {\"description\": \"Filter by leather/non-leather, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"sort\": {\"description\": \"Sorting option, one of \\'pricedesc\\', \\'priceasc\\', or \\'freshness\\'; default is \\'freshness\\'.\", \"type\": \"str, optional\", \"default\": \"freshness\"}, \"q\": {\"description\": \"Search query for products by name (do not use with categoryId).\", \"type\": \"str, optional\", \"default\": \"\"}, \"base_colour\": {\"description\": \"Filter by color, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"range\": {\"description\": \"Filter by sale/new season, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"attribute_1047\": {\"description\": \"Filter by product type, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"currency\": {\"description\": \"Currency code obtained from countries/list API; default is \\'USD\\'.\", \"type\": \"str, optional\", \"default\": \"USD\"}, \"attribute_10155\": {\"description\": \"Filter by range, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"pricemax\": {\"description\": \"Maximum price filter.\", \"type\": \"int, optional\", \"default\": \"\"}, \"sizeschema\": {\"description\": \"Size schema identifier obtained from countries/list API; default is \\'US\\'.\", \"type\": \"str, optional\", \"default\": \"US\"}, \"brand\": {\"description\": \"Filter by brand, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"size\": {\"description\": \"Filter by size, multiple values separated by comma.\", \"type\": \"str, optional\", \"default\": \"\"}, \"lang\": {\"description\": \"Language code; default is \\'en-US\\'.\", \"type\": \"str, optional\", \"default\": \"en-US\"}}}, \"required\": [\"store\", \"offset\", \"categoryid\", \"limit\"]},\\n{\"name\": \"tmall_search_by_keyword\", \"description\": \"Searches for products on Tmall by keyword.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"page\": {\"description\": \"The page number of the search results to retrieve.\", \"type\": \"int\", \"default\": \"1\"}, \"keyword\": {\"description\": \"The keyword to search for products.\", \"type\": \"str\", \"default\": \"\\\\u6d77\\\\u4e45\\\\u6c34\\\\u74f6\"}}}, \"required\": [\"page\", \"keyword\"]},\\n{\"name\": \"navigations_get_node_content\", \"description\": \"Fetches the content of a node in a navigation hierarchy.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"is_id\": {\"description\": \"The \\'id\\' field value returned from the /navigations/get-root endpoint.\", \"type\": \"int\", \"default\": \"26066300130\"}, \"cat_id\": {\"description\": \"The \\'cat_id\\' field value returned from the /navigations/get-tabs endpoint.\", \"type\": \"int\", \"default\": \"2026\"}, \"language\": {\"description\": \"The 2-letter language code (default is \\'en\\').\", \"type\": \"str, optional\", \"default\": \"en\"}, \"currency\": {\"description\": \"The 3-letter currency code (default is \\'USD\\').\", \"type\": \"str, optional\", \"default\": \"USD\"}, \"country\": {\"description\": \"The 2-letter country code (default is \\'US\\').\", \"type\": \"str, optional\", \"default\": \"US\"}}}, \"required\": [\"is_id\", \"cat_id\"]}\\n]\\n</tools>\\nFor each function call return a JSON object, with the following pydantic model json schema for each:\\n{\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}, \\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}},\\'required\\': [\\'name\\', \\'arguments\\']}\\nEach function call should be enclosed within <tool_call> </tool_call> XML tags.\\nExample:\\n<tool_call>\\n{\\'name\\': <function-name>,\\'arguments\\': <args-dict>}\\n</tool_call>\\n\\n'},\n",
       " {'from': 'human',\n",
       "  'value': \"Search for 'laptops' on Tmall and retrieve the second page of results.\"},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the user wants to search for \\'laptops\\' on Tmall and retrieve the second page of results. Let\\'s think about how to do this.\\n\\nFirst, the available functions are listed. The tmall_search_by_keyword function seems suitable here. Looking at its parameters, I need to provide the page number and the keyword. The keyword here is \\'laptops\\', and the page is the second one, so page=2.\\n\\nSo the function call should use tmall_search_by_keyword with page=2 and keyword=\\'laptops\\'. That should give the results for the second page.\\n\\nWait, does the function return the correct data structure? The user might want the product details, like names, prices, maybe images. The documentation says tmall_search_by_keyword returns a list of products, each with title, price, picture, and other details. So yes, this should work.\\n</think>\\n<tool_call>\\n{\"name\":\"tmall_search_by_keyword\",\"arguments\":{\"page\":2,\"keyword\":\"laptops\"}}\\n</tool_call>'},\n",
       " {'from': 'tool',\n",
       "  'value': '<tool_response>\\n{\"name\":\"tmall_search_by_keyword\",\"content\":{\"data\":[{\"id\":\"10000000\",\"title\":\"Acer Aspire 5 Slim Laptop\",\"price\":\"699.99\",\"picture\":\"https://example.com/acer-aspire-5.jpg\",\"description\":\"15.6 inches FHD IPS Display, 8GB RAM, 256GB SSD\",\"brand\":\"Acer\",\"category\":\"Laptops\",\"rating\":4.5},{\"id\":\"10000001\",\"title\":\"HP Pavilion x360 14\",\"price\":\"599.99\",\"picture\":\"https://example.com/hp-pavilion-x360.jpg\",\"description\":\"14 inches Touchscreen Convertible, 8GB RAM, 256GB SSD\",\"brand\":\"HP\",\"category\":\"Laptops\",\"rating\":4.3},{\"id\":\"10000002\",\"title\":\"Dell Inspiron 15 3501 Laptop\",\"price\":\"549.99\",\"picture\":\"https://example.com/dell-inspiron.jpg\",\"description\":\"15.6 inches HD Display, 8GB RAM, 256GB SSD\",\"brand\":\"Dell\",\"category\":\"Laptops\",\"rating\":4.1},{\"id\":\"10000003\",\"title\":\"Lenovo IdeaPad 3 Laptop\",\"price\":\"529.99\",\"picture\":\"https://example.com/lenovo-ideapad.jpg\",\"description\":\"15.6 inches HD Display, 8GB RAM, 256GB SSD\",\"brand\":\"Lenovo\",\"category\":\"Laptops\",\"rating\":4.2},{\"id\":\"10000004\",\"title\":\"ASUS VivoBook 15 Laptop\",\"price\":\"649.99\",\"picture\":\"https://example.com/asus-vivobook.jpg\",\"description\":\"15.6 inches FHD Display, 8GB RAM, 256GB SSD\",\"brand\":\"ASUS\",\"category\":\"Laptops\",\"rating\":4.4},{\"id\":\"10000005\",\"title\":\"Samsung Galaxy Book Pro Laptop\",\"price\":\"849.99\",\"picture\":\"https://example.com/samsung-galaxy-book.jpg\",\"description\":\"13.3 inches FHD OLED Display, 8GB RAM, 256GB SSD\",\"brand\":\"Samsung\",\"category\":\"Laptops\",\"rating\":4.6}],\"page\":2,\"total_results\":50,\"total_pages\":25}}\\n</tool_response>'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full[1]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7ada8-69ed-45c7-930d-64f46cb93c82",
   "metadata": {},
   "source": [
    "- å¤šè½®å¯¹è¯+å·¥å…·è°ƒç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bf4c35-2b13-4522-85ce-f40375f05787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. If available tools are not relevant in assisting with user query, just respond in natural conversational language. Don\\'t make assumptions about what values to plug into functions. After calling & executing the functions, you will be provided with function results within <tool_response> </tool_response> XML tags. Here are the available tools:\\n<tools>\\n[{\"name\":\"extract\",\"description\":\"Extracts data from a LinkedIn URL (prostring or company page).\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"url\":{\"description\":\"The LinkedIn URL to extract data from\",\"type\":\"string\"},\"html\":{\"description\":\"Whether to output raw HTML in.raw property. Disabled by default.\",\"type\":\"boolean\",\"default\":true}},\"required\":[\"url\"]},\"required\":null},{\"name\":\"get_all_motivational_quotes\",\"description\":\"This endpoint fetches all motivational quotes. You can do pagination as well using the\\'skip\\' and \\'limit\\' parameters.\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"skip\":{\"description\":\"The number of quotes to skip from the beginning of the result set.\",\"type\":\"float\"},\"limit\":{\"description\":\"The maximum number of quotes to return.\",\"type\":\"float\"}},\"required\":[]},\"required\":null},{\"name\":\"get_sub_regions\",\"description\":\"Get a list of sub-regions in the world, filtered by the specified region.\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"region\":{\"description\":\"The name of the region to get sub-regions for.\",\"type\":\"string\"},\"language\":{\"description\":\"The language to use for the response.\",\"type\":\"string\"}},\"required\":[\"region\"]},\"required\":null},{\"name\":\"consulta_estabelecimento\",\"description\":\"Retrieve information about a company (estabelecimento) based on its CNPJ (Corporate Number) from various sources such as Receita Federal, Simples Nacional, and Cadastro de Contribuintes.\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"CNPJ\":{\"description\":\"The CNPJ of the company to retrieve information about\",\"type\":\"string\"},\"maxAge\":{\"description\":\"The maximum age of the data in days\",\"type\":\"float\",\"default\":30},\"simplesHistory\":{\"description\":\"Whether to include Simples Nacional history\",\"type\":\"boolean\",\"default\":false},\"registrations\":{\"description\":\"UFs (states) to include in the Cadastro de Contribuintes information, separated by commas. Use \\'BR\\' to include all states.\",\"type\":\"string\",\"default\":\"BR\"},\"simples\":{\"description\":\"Whether to include Simples Nacional information\",\"type\":\"boolean\",\"default\":true}},\"required\":[\"CNPJ\"]},\"required\":null},{\"name\":\"realtor_school_list\",\"description\":\"This API returns a list of Realtor schools in a given city, state, and school level. The response includes school details such as name, rating, and reviews.\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"city\":{\"description\":\"The city to filter schools by\",\"type\":\"string\"},\"state_code\":{\"description\":\"The state code to filter schools by\",\"type\":\"string\"},\"school_level\":{\"description\":\"The school level to filter schools by (elementary, middle, high, private, charter)\",\"type\":\"string\"},\"page\":{\"description\":\"The page number to retrieve (default: 1)\",\"type\":\"int\"}},\"required\":[\"city\",\"state_code\",\"school_level\",\"page\"]},\"required\":null}]\\n</tools>\\nFor each function call return a JSON object, with the following pydantic model json schema for each:\\n{\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}, \\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}},\\'required\\': [\\'name\\', \\'arguments\\']}\\nEach function call should be enclosed within <tool_call> </tool_call> XML tags.\\nExample:\\n<tool_call>\\n{\\'name\\': <function-name>,\\'arguments\\': <args-dict>}\\n</tool_call>\\n\\n'},\n",
       " {'from': 'human',\n",
       "  'value': 'I would like to extract details from a LinkedIn company page. Could you assist me in fetching the information from this URL: https://www.linkedin.com/company/abc-corporation?'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the user wants to extract details from a LinkedIn company page. The URL provided is https://www.linkedin.com/company/company-name. I need to use the extract tool to get this done. The extract tool has a parameter for the URL and an optional HTML flag. Since the user didn\\'t specify any other parameters, I\\'ll just provide the URL.\\n\\nWait, the URL passed is a company page, so the extract function should handle that. The result will include things like the company\\'s name, industry, employees, headquarters, etc. The user didn\\'t mention any specific details to extract, so I should just go with the default extraction. The HTML flag is set to true by default, but since they might want the raw data, I should include it in the parameters. But the documentation says it\\'s optional. The extract tool should handle the data extraction from the company page.\\n</think>\\n<tool_call>\\n{\"name\":\"extract\",\"arguments\":{\"url\":\"https://www.linkedin.com/company/abc-corporation\"}}\\n</tool_call>'},\n",
       " {'from': 'tool',\n",
       "  'value': '<tool_response>\\n{\"tool_call_id\": \"call_45cde0fe39464c18b23c900c4a2375be\", \"name\": \"extract\", \"content\": {\"data\": {\"name\": \"ABC Corporation\", \"industry\": \"Information Technology and Services\", \"website\": \"http://www.abccorporation.com\", \"headquarters\": \"San Francisco, CA\", \"yearFounded\": 1998, \"companySize\": \"11-50 employees\", \"specialties\": [\"Cloud computing\", \"Software as a Service (SaaS)\", \"IT consulting\"]}, \"raw\": \"<!DOCTYPE html><html><head><title>ABC Corporation</title></head><body><h1>ABC Corporation</h1><p>Industry: Information Technology and Services</p><p>Website: http://www.abccorporation.com</p><p>Headquarters: San Francisco, CA</p><p>Year Founded: 1998</p><p>Company Size: 11-50 employees</p><p>Specialties: Cloud computing, Software as a Service (SaaS), IT consulting</p></body></html>\"}}\\n</tool_response>'},\n",
       " {'from': 'gpt',\n",
       "  'value': \"<think>\\nOkay, the user provided a LinkedIn URL for the ABC Corporation. The extract tool successfully processed it and returned the company's details such as name, industry, website, headquarters, year founded, company size, and specialties. Since the user didn't ask for raw HTML, I can summarize the key data points without including the HTML.\\n</think>The LinkedIn page for ABC Corporation shows they are an IT services company operating in San Francisco, CA, founded in 1998 with 11-50 employees. Their specialties include cloud computing, Software as a Service (SaaS), and IT consulting. You can find more details on their website at http://www.abccorporation.com.\"},\n",
       " {'from': 'human',\n",
       "  'value': 'Could you share some motivational quotes that might inspire my team during this business project?'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nThe user wants motivational quotes to inspire their team during a business project. I should use the get_all_motivational_quotes tool to fetch some. Since they didn\\'t specify a language, I\\'ll use the default, maybe English. Also, no parameters for skip or limit were given, so using the default pagination. The quotes might be from famous people or general motivational messages that are suitable for a business context. The tool might return a list, so need to select the top few that are inspiring and relevant.\\n</think>\\n<tool_call>\\n{\"name\":\"get_all_motivational_quotes\",\"arguments\":{}}\\n</tool_call>'},\n",
       " {'from': 'tool',\n",
       "  'value': '<tool_response>\\n{\"tool_call_id\": \"call_85d0d52000614fb0aa94a4557275c8e8\", \"name\": \"get_all_motivational_quotes\", \"content\": {\"quotes\": [{\"text\": \"The only way to do great work is to love what you do.\", \"author\": \"Steve Jobs\", \"id\": 1}, {\"text\": \"Believe you can and you\\'re halfway there.\", \"author\": \"Theodore Roosevelt\", \"id\": 2}, {\"text\": \"It always seems impossible until it\\'s done.\", \"author\": \"Nelson Mandela\", \"id\": 3}, {\"text\": \"You are never too old to set another goal or to dream a new dream.\", \"author\": \"C.S. Lewis\", \"id\": 4}]}}\\n</tool_response>'},\n",
       " {'from': 'gpt',\n",
       "  'value': 'Here are some motivational quotes to inspire your team:\\n1. **Steve Jobs:** \"The only way to do great work is to love what you do.\"  \\n2. **Theodore Roosevelt:** \"Believe you can and you\\'re halfway there.\"  \\n3. **Nelson Mandela:** \"It always seems impossible until it\\'s done.\"  \\n4. **C.S. Lewis:** \"You are never too old to set another goal or to dream a new dream.\"'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full[2]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244b3e0-783f-49fb-8169-3127027cd884",
   "metadata": {},
   "source": [
    "- æ— å¹²é¢„ä¸‹å¤šè½®å·¥å…·è°ƒç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5221e32b-e0ca-4a79-80a3-db426a2a5f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. If available tools are not relevant in assisting with user query, just respond in natural conversational language. Don\\'t make assumptions about what values to plug into functions. After calling & executing the functions, you will be provided with function results within <tool_response> </tool_response> XML tags. Here are the available tools:\\n<tools>\\n[{\"name\":\"get_language_proficiency\",\"description\":\"Retrieve language proficiency information\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"language\":{\"type\":\"string\",\"description\":\"The language for which you want to get the proficiency (e.g., English, Spanish, French)\"},\"level\":{\"type\":\"string\",\"description\":\"The desired level of proficiency (e.g., Beginner, Intermediate, Advanced)\"}},\"required\":[\"language\"]},\"required\":null},{\"name\":\"get_language_translation\",\"description\":\"Translate a given text to another language\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"text\":{\"type\":\"string\",\"description\":\"The text to be translated\"},\"source_language\":{\"type\":\"string\",\"description\":\"The source language of the text (e.g., English, Spanish, French)\"},\"target_language\":{\"type\":\"string\",\"description\":\"The target language for translation (e.g., English, Spanish, French)\"}},\"required\":[\"text\",\"source_language\",\"target_language\"]},\"required\":null}]\\n</tools>\\nFor each function call return a JSON object, with the following pydantic model json schema for each:\\n{\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}, \\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}},\\'required\\': [\\'name\\', \\'arguments\\']}\\nEach function call should be enclosed within <tool_call> </tool_call> XML tags.\\nExample:\\n<tool_call>\\n{\\'name\\': <function-name>,\\'arguments\\': <args-dict>}\\n</tool_call>\\n\\n'},\n",
       " {'from': 'human',\n",
       "  'value': 'I\\'d like to learn Spanish and need to understand my current proficiency level. Also, once I know my level, I need a translation of the following text: \"I want to learn Spanish quickly\" into Spanish.\\n\\n'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the user wants to know their Spanish language proficiency and also needs a translation of a text into Spanish. Let\\'s start with the first part: getting their proficiency level. The get_language_proficiency tool requires specifying the language and level. The user specified Spanish, but they didn\\'t mention their current level. Hmm, maybe I should assume the default for unknown users which is \"Beginner\". But wait, the user said they want to learn Spanish, so maybe they are a beginner. Alternatively, maybe the tool allows for the user to input their current level. Let me check the tool parameters. The function requires both \"language\" and \"level\" as required parameters. Since the user hasn\\'t specified a level, perhaps I should default it to \"Beginner\" as a safe assumption.\\n\\nOnce we have the proficiency level, the user wants the translation of \"I want to learn Spanish quickly\" into Spanish. The translation tool requires source and target languages. The source is English, target is Spanish. So I\\'ll call get_language_translation with those parameters.\\n\\nSo first step: get_language_proficiency with language=\"Spanish\" and level=\"Beginner\". Then, get_language_translation with text=\"I want to learn Spanish quickly\", source_language=\"English\", target_language=\"Spanish\".\\n\\nAfter getting the responses, I\\'ll output the proficiency level and the translated text.\\n</think>\\n<tool_call>\\n{\"name\":\"get_language_proficiency\",\"arguments\":{\"language\":\"Spanish\",\"level\":\"Beginner\"}}\\n</tool_call>'},\n",
       " {'from': 'tool',\n",
       "  'value': '<tool_response>\\n{\"tool_call_id\": \"call_27ce8bdc10d34f5cbcdda87903bed07a\", \"name\": \"get_language_proficiency\", \"content\": {\"proficiency\": \"Beginner\"}}\\n</tool_response>'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the first tool response gave the Spanish proficiency level as Beginner. Now, the user wants the text \"I want to learn Spanish quickly\" translated into Spanish. Using the get_language_translation tool, the source language is English and target is Spanish. The text will be translated accordingly.\\n\\nI\\'ll call the translation tool with the given parameters.\\n</think>\\n<tool_call>\\n{\"name\":\"get_language_translation\",\"arguments\":{\"text\":\"I want to learn Spanish quickly\",\"source_language\":\"English\",\"target_language\":\"Spanish\"}}\\n</tool_call>'},\n",
       " {'from': 'tool',\n",
       "  'value': '<tool_response>\\n{\"tool_call_id\": \"call_fb5f7d2466414a0184b5207edfb63e2c\", \"name\": \"get_language_translation\", \"content\": {\"translation\": \"Quiero aprender espa\\\\u00f1ol r\\\\u00e1pidamente\"}}\\n</tool_response>'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nOkay, the user wants their Spanish proficiency and the translation of \"I want to learn Spanish quickly\" into Spanish. The first tool response gave their proficiency as Beginner, and the second tool translated the text to \"Quiero aprender espaÃ±ol rÃ¡pidamente\". That should cover both parts of their request.\\n</think>Your current Spanish proficiency level is **Beginner**. The translated text is: **\"Quiero aprender espaÃ±ol rÃ¡pidamente\"** (I want to learn Spanish quickly).'},\n",
       " {'from': 'gpt',\n",
       "  'value': '<think>\\nThe user has received the answers to their two questions: their Spanish proficiency level (Beginner) and the translation of their text. The task is complete.\\n</think>Your Spanish proficiency level is assessed as Beginner, and the translated text is \"Quiero aprender espaÃ±ol rÃ¡pidamente\". If you have any further questions or need additional assistance, please let me know!'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full[4]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f250ec-562c-42cf-99bc-d76eb76930e1",
   "metadata": {},
   "source": [
    "### 2. Hermes tool useæ•°æ®é›†æ¸…æ´—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b800664-984c-4edf-a34c-8146b448e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, ast\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbd3a03-500b-494f-8e36-8fededd97f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 1) ç±»å‹æ˜ å°„ï¼šæŠŠç®€å†™ç±»å‹è½¬ä¸º JSON Schema =========\n",
    "def _map_type(t: str):\n",
    "    t = (t or \"\").strip()\n",
    "    if t.lower() in (\"str\", \"string\"):\n",
    "        return {\"type\": \"string\"}\n",
    "    if t.lower() in (\"int\", \"integer\"):\n",
    "        return {\"type\": \"integer\"}\n",
    "    if t.lower() in (\"float\", \"number\"):\n",
    "        return {\"type\": \"number\"}\n",
    "    if t.lower() in (\"bool\", \"boolean\"):\n",
    "        return {\"type\": \"boolean\"}\n",
    "    if t.startswith(\"List[\") and t.endswith(\"]\"):\n",
    "        inner = t[5:-1].strip()\n",
    "        inner_schema = _map_type(inner)\n",
    "        return {\"type\": \"array\", \"items\": {\"type\": inner_schema.get(\"type\", \"string\")}}\n",
    "    # å…œåº•\n",
    "    return {\"type\": \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b24356-f706-47d9-a741-cb23fb019f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast, re\n",
    "\n",
    "# 1) å…œåº•çš„ç±»å‹æ˜ å°„\n",
    "def _map_base_type(t: str) -> str:\n",
    "    t = (t or \"string\")\n",
    "    if isinstance(t, str):\n",
    "        t = t.strip().lower()\n",
    "    else:\n",
    "        t = str(t).strip().lower()\n",
    "    # å»æ‰ like \"bool, optional\"\n",
    "    t = t.split(\",\")[0].strip()\n",
    "    MAP = {\n",
    "        \"str\": \"string\", \"string\": \"string\",\n",
    "        \"int\": \"integer\", \"integer\": \"integer\",\n",
    "        \"float\": \"number\", \"number\": \"number\",\n",
    "        \"bool\": \"boolean\", \"boolean\": \"boolean\",\n",
    "        \"dict\": \"object\", \"object\": \"object\",\n",
    "        \"array\": \"array\", \"list\": \"array\",\n",
    "    }\n",
    "    return MAP.get(t, \"string\")\n",
    "\n",
    "# 2) å°†å„ç§ parameters å½’ä¸€åŒ–ä¸º JSON Schema\n",
    "def _normalize_parameters_to_jsonschema(params) -> dict:\n",
    "    if not isinstance(params, dict):\n",
    "        return {\"type\": \"object\", \"properties\": {}}\n",
    "\n",
    "    if params.get(\"type\") in (\"object\", \"dict\") and \"properties\" in params:\n",
    "        schema = dict(params)\n",
    "        if schema[\"type\"] == \"dict\":\n",
    "            schema[\"type\"] = \"object\"\n",
    "        props = {}\n",
    "        for p, spec in (schema.get(\"properties\") or {}).items():\n",
    "            if not isinstance(spec, dict):\n",
    "                spec = {}\n",
    "            base = _map_base_type(spec.get(\"type\", \"string\"))\n",
    "            props[p] = {\n",
    "                \"type\": base,\n",
    "                \"description\": spec.get(\"description\", \"\"),  # â† å¿…æœ‰æè¿°\n",
    "            }\n",
    "        req = schema.get(\"required\")\n",
    "        if not isinstance(req, list):\n",
    "            req = list(props.keys())\n",
    "        return {\"type\": \"object\", \"properties\": props, \"required\": req}\n",
    "\n",
    "    # ç®€å†™ -> æ­£è§„åŒ–\n",
    "    props, required = {}, []\n",
    "    for p, spec in (params.items() if isinstance(params, dict) else []):\n",
    "        if not isinstance(spec, dict):\n",
    "            spec = {}\n",
    "        base = _map_base_type(spec.get(\"type\", \"string\"))\n",
    "        props[p] = {\n",
    "            \"type\": base,\n",
    "            \"description\": spec.get(\"description\", \"\"),  # â† å¿…æœ‰æè¿°\n",
    "        }\n",
    "        required.append(p)\n",
    "    return {\"type\": \"object\", \"properties\": props, \"required\": required}\n",
    "\n",
    "# 3) Hermes tools â†’ gpt-oss toolsï¼ˆè¶…ç¨³å¥ç‰ˆï¼‰\n",
    "def hermes_tools_to_gptoss_tools(hermes_tools):\n",
    "    out = []\n",
    "    if not hermes_tools:\n",
    "        return out\n",
    "\n",
    "    for t in hermes_tools:\n",
    "        if isinstance(t, str):\n",
    "            try:\n",
    "                t = json.loads(t)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    import ast\n",
    "                    t = ast.literal_eval(t)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "\n",
    "        fn = t[\"function\"] if (t.get(\"type\") == \"function\" and isinstance(t.get(\"function\"), dict)) else t\n",
    "        name = (fn.get(\"name\") or \"\").strip()\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        desc = fn.get(\"description\") or \"\"  # â† å¿…æœ‰ description\n",
    "        params = fn.get(\"parameters\") or {}\n",
    "        schema = _normalize_parameters_to_jsonschema(params)\n",
    "\n",
    "        # ---- è¡¥é½ properties é‡Œçš„ type/description ----\n",
    "        if schema.get(\"type\") == \"object\" and isinstance(schema.get(\"properties\"), dict):\n",
    "            for p, spec in list(schema[\"properties\"].items()):\n",
    "                if not isinstance(spec, dict):\n",
    "                    spec = {}\n",
    "                spec.setdefault(\"type\", \"string\")\n",
    "                spec.setdefault(\"description\", \"\")\n",
    "                schema[\"properties\"][p] = spec\n",
    "        # ---------------------------------------------\n",
    "\n",
    "        out.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": name,\n",
    "                \"description\": desc,\n",
    "                \"parameters\": schema,\n",
    "            },\n",
    "        })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241a6843-118f-4d44-ba4a-a8523666873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 3) ä» system æ–‡æœ¬é‡Œæå–/å»æ‰ <tools>... å— =========\n",
    "_TOOLS_BLOCK_RE = re.compile(r\"<tools>(.*?)</tools>\", re.DOTALL)\n",
    "\n",
    "def strip_tools_block(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    return _TOOLS_BLOCK_RE.sub(\"\", text).strip()\n",
    "\n",
    "def extract_tools_from_system(text: str):\n",
    "    \"\"\"å°½åŠ›ä» <tools>...</tools> ä¸­è§£æå‡º Python åˆ—è¡¨æˆ– JSON åˆ—è¡¨ï¼›å¤±è´¥åˆ™è¿”å› [].\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    m = _TOOLS_BLOCK_RE.search(text)\n",
    "    if not m:\n",
    "        return []\n",
    "    raw = m.group(1).strip()\n",
    "    # å°è¯• JSON\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # å°è¯• Python å­—é¢é‡ï¼ˆHermes å¸¸ç”¨å•å¼•å·ï¼‰\n",
    "    try:\n",
    "        val = ast.literal_eval(raw)\n",
    "        return val if isinstance(val, list) else []\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a85c62-445d-482f-b512-613cbde687d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 4) è§£æ assistant çš„ <think> ä¸ <tool_call> =========\n",
    "_THINK_RE = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "_TOOLCALL_RE = re.compile(r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\", re.DOTALL)\n",
    "\n",
    "def parse_assistant_content(value: str):\n",
    "    \"\"\"è¿”å› (thinking:str|None, tool_calls:list[{'function':{'name', 'arguments'}}])\"\"\"\n",
    "    if not value:\n",
    "        return None, []\n",
    "    thinking = None\n",
    "    m = _THINK_RE.search(value)\n",
    "    if m:\n",
    "        thinking = m.group(1).strip()\n",
    "\n",
    "    calls = []\n",
    "    for m in _TOOLCALL_RE.finditer(value):\n",
    "        blob = m.group(1)\n",
    "        try:\n",
    "            obj = json.loads(blob)\n",
    "        except Exception:\n",
    "            # å•å¼•å·é£æ ¼\n",
    "            try:\n",
    "                obj = ast.literal_eval(blob)\n",
    "            except Exception:\n",
    "                continue\n",
    "        name = obj.get(\"name\")\n",
    "        args = obj.get(\"arguments\", {})\n",
    "        if not isinstance(args, dict):\n",
    "            # å†è¯•ä¸€æ¬¡è§£æ\n",
    "            try:\n",
    "                args = json.loads(args) if isinstance(args, str) else {}\n",
    "            except Exception:\n",
    "                args = {}\n",
    "        if name:\n",
    "            calls.append({\"function\": {\"name\": name, \"arguments\": args}})\n",
    "    return thinking, calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb01fabc-4b81-4059-89aa-0b0ba9d3271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 5) è§£æ tool çš„ <tool_response> JSON å†…å®¹ä¸ºå­—ç¬¦ä¸² =========\n",
    "_TOOLRESP_RE = re.compile(r\"<tool_response>\\s*(\\{.*?\\})\\s*</tool_response>\", re.DOTALL)\n",
    "\n",
    "def parse_tool_response_value(value: str) -> str:\n",
    "    \"\"\"\n",
    "    è¾“å…¥ Hermes çš„ tool.valueï¼Œè¾“å‡ºä¸€ä¸ª JSON å­—ç¬¦ä¸²ï¼ˆå°½é‡è§„èŒƒåŒ–ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    m = _TOOLRESP_RE.search(value)\n",
    "    raw = (m.group(1).strip() if m else value.strip())\n",
    "    # å°è¯• JSON\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "        return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # å°è¯• Python å­—é¢é‡\n",
    "    try:\n",
    "        obj = ast.literal_eval(raw)\n",
    "        return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        # ç•™åŸæ–‡ï¼ˆè®©æ¨¡æ¿åš tojsonï¼‰ï¼Œè‡³å°‘æ˜¯å­—ç¬¦ä¸²\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbba2f36-f8f5-42ae-9cb7-e3fedd48bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_NOISE_PATTERNS = [\n",
    "    r\"Here are the available tools:.*\",\n",
    "    r\"Use the following pydantic model.*\",\n",
    "    r\"Each function call should be enclosed.*\",\n",
    "    r\"You are a function calling AI model\\..*\",\n",
    "    r\"within\\s*<tools>\\s*</tools>\\s*XML tags.*\",\n",
    "]\n",
    "DEV_NOISE_RE = re.compile(\"|\".join(DEV_NOISE_PATTERNS), re.DOTALL)\n",
    "\n",
    "def clean_developer_text(s: str) -> str:\n",
    "    s = _TOOLS_BLOCK_RE.sub(\"\", s or \"\")  # å…ˆå»æ‰ <tools>â€¦</tools>\n",
    "    s = DEV_NOISE_RE.sub(\"\", s).strip()\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a463fd-b934-46e3-a406-5ee2524c4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# åŸæœ‰å™ªéŸ³æ¨¡å¼\n",
    "DEV_NOISE_PATTERNS = [\n",
    "    r\"Here are the available tools:.*\",\n",
    "    r\"Use the following pydantic model.*\",\n",
    "    r\"Each function call should be enclosed.*\",\n",
    "    r\"You are a function calling AI model\\..*\",\n",
    "    r\"within\\s*<tools>\\s*</tools>\\s*XML tags.*\",\n",
    "    # æ–°å¢ï¼šæ¸…ç† <think> ç›¸å…³è¯´æ˜\n",
    "    r\"You should enclose.*?<think>.*?</think>.*?(solution|response).*\",\n",
    "    r\"enclose your thoughts.*?<think>.*?</think>.*\",\n",
    "    r\"internal monologue.*?<think>.*?</think>.*\",\n",
    "]\n",
    "DEV_NOISE_RE = re.compile(\"|\".join(DEV_NOISE_PATTERNS), re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "def final_clean_developer_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    æ¸…ç† Hermes system æç¤ºï¼Œå»æ‰å·¥å…·å£°æ˜ã€pydantic schemaã€<think> æ ‡ç­¾è¯´æ˜ï¼Œ\n",
    "    åªä¿ç•™å¹²å‡€çš„é€šç”¨ developer æç¤ºã€‚\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # å…ˆå»æ‰ <tools>...</tools> å—\n",
    "    s = _TOOLS_BLOCK_RE.sub(\"\", s)\n",
    "    # å»æ‰å®šä¹‰çš„å™ªéŸ³æ®µè½\n",
    "    s = DEV_NOISE_RE.sub(\"\", s).strip()\n",
    "    # å‹ç¼©å¤šä½™ç©ºè¡Œ\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d8cf29-1580-4222-90aa-85d9e4371519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "\n",
    "_UNPAIRED_THINK_OPEN  = re.compile(r\"<think>\\s*\", re.IGNORECASE)\n",
    "_UNPAIRED_THINK_CLOSE = re.compile(r\"\\s*</think>\", re.IGNORECASE)\n",
    "\n",
    "def _strip_unpaired_think_tags(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" if text is None else str(text)\n",
    "    # å»æ‰å•ç‹¬å‡ºç°ï¼ˆæˆ–æœªé…å¯¹ï¼‰çš„ <think> / </think> æ ‡ç­¾\n",
    "    text = _UNPAIRED_THINK_OPEN.sub(\"\", text)\n",
    "    text = _UNPAIRED_THINK_CLOSE.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "def _drop_none_fields(msg: dict) -> dict:\n",
    "    # åˆ æ‰å€¼ä¸º None çš„é”®\n",
    "    return {k: v for k, v in msg.items() if v is not None}\n",
    "\n",
    "def sanitize_messages_for_gptoss(messages: list) -> list:\n",
    "    cleaned = []\n",
    "    for m in messages:\n",
    "        m = dict(m)  # å¤åˆ¶ä¸€ä»½\n",
    "        role = m.get(\"role\")\n",
    "\n",
    "        # 1) å»æ‰ thinking=None\n",
    "        if \"thinking\" in m and (m[\"thinking\"] is None or m[\"thinking\"] == \"\"):\n",
    "            m.pop(\"thinking\", None)\n",
    "\n",
    "        # 2) å¯¹ content åšå…œåº•æ¸…æ´—ï¼ˆç§»é™¤æ®‹ç•™ think æ ‡ç­¾ï¼‰\n",
    "        if \"content\" in m:\n",
    "            m[\"content\"] = _strip_unpaired_think_tags(m[\"content\"])\n",
    "\n",
    "        # 3) tool æ¶ˆæ¯çš„ content å¿…é¡»æ˜¯å­—ç¬¦ä¸²\n",
    "        if role == \"tool\":\n",
    "            c = m.get(\"content\", \"\")\n",
    "            if not isinstance(c, str):\n",
    "                try:\n",
    "                    m[\"content\"] = json.dumps(c, ensure_ascii=False)\n",
    "                except Exception:\n",
    "                    m[\"content\"] = str(c)\n",
    "\n",
    "        # 4) assistant+tool_calls æƒ…å†µï¼šä¸èƒ½å¸¦ content\n",
    "        if role == \"assistant\" and \"tool_calls\" in m and m.get(\"tool_calls\"):\n",
    "            m.pop(\"content\", None)  # ä¿å®ˆèµ·è§æ¸…æ‰\n",
    "\n",
    "        # 5) åˆ é™¤ None å­—æ®µ\n",
    "        m = _drop_none_fields(m)\n",
    "\n",
    "        # 6) ä¸¢å¼ƒå®Œå…¨ç©ºçš„æ¶ˆæ¯ï¼ˆæå°‘æ•°è„æ•°æ®ï¼‰\n",
    "        if role and (m.get(\"content\") or m.get(\"tool_calls\") or m.get(\"thinking\") or role == \"tool\"):\n",
    "            cleaned.append(m)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514a1fd5-8d6a-4291-8f12-2d12ffd30dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6) å•æ¡æ ·æœ¬è½¬æ¢ï¼šHermes â†’ gpt-oss (messages + tools) =========\n",
    "def hermes_to_gptoss_example(ex: dict):\n",
    "    conv = ex.get(\"conversations\") or []\n",
    "    top_tools_raw = ex.get(\"tools\")  # å¯èƒ½æ˜¯ str / list[str] / list[dict] / dict / None\n",
    "\n",
    "    messages = []\n",
    "    idx = 0\n",
    "    sys_text = \"\"\n",
    "\n",
    "    if len(conv) and conv[0].get(\"from\") == \"system\":\n",
    "        sys_text = conv[0].get(\"value\", \"\") or \"\"\n",
    "        # è‹¥é¡¶å±‚æ—  toolsï¼Œä» system çš„ <tools>... æŠ½\n",
    "        if not top_tools_raw:\n",
    "            extracted = extract_tools_from_system(sys_text)\n",
    "            if extracted:\n",
    "                top_tools_raw = extracted\n",
    "\n",
    "        dev_text = final_clean_developer_text(sys_text)\n",
    "        if dev_text.strip():\n",
    "            messages.append({\"role\": \"developer\", \"content\": dev_text.strip()})\n",
    "        idx = 1\n",
    "\n",
    "    # 1) å·¥å…·å½’ä¸€åŒ–ï¼ˆä½ å·²æœ‰ï¼‰\n",
    "    top_tools_norm = parse_tools_any(top_tools_raw)\n",
    "    gptoss_tools = hermes_tools_to_gptoss_tools(top_tools_norm or [])\n",
    "\n",
    "    # 2) ä¸»å¾ªç¯ï¼ˆåŠ ä¸€ç‚¹ç‚¹å¥å£®æ€§ï¼‰\n",
    "    for turn in conv[idx:]:\n",
    "        who = (turn.get(\"from\") or \"\").lower()\n",
    "        val = turn.get(\"value\", \"\")\n",
    "        if not isinstance(val, str):  # é˜²å¾¡ï¼šç¡®ä¿æ˜¯å­—ç¬¦ä¸²\n",
    "            val = str(val)\n",
    "\n",
    "        if who in (\"human\", \"user\"):\n",
    "            if val.strip():\n",
    "                messages.append({\"role\": \"user\", \"content\": val})\n",
    "\n",
    "        elif who in (\"gpt\", \"assistant\"):\n",
    "            thinking, calls = parse_assistant_content(val)\n",
    "            if calls:\n",
    "                for j, call in enumerate(calls):\n",
    "                    msg = {\"role\": \"assistant\", \"tool_calls\": [call]}\n",
    "                    if j == 0 and thinking:   # åªæœ‰ç¬¬ä¸€æ¡å¸¦ thinking\n",
    "                        msg[\"thinking\"] = thinking\n",
    "                    messages.append(msg)\n",
    "            else:\n",
    "                # éå·¥å…·è°ƒç”¨å›åˆ\n",
    "                content = val\n",
    "                if thinking is not None:\n",
    "                    content = _THINK_RE.sub(\"\", val).strip()\n",
    "                messages.append(\n",
    "                    {\"role\": \"assistant\", **({\"thinking\": thinking} if thinking else {}), \"content\": content}\n",
    "                )\n",
    "\n",
    "        elif who == \"tool\":\n",
    "            tool_payload = parse_tool_response_value(val)  # -> JSON å­—ç¬¦ä¸²\n",
    "            messages.append({\"role\": \"tool\", \"content\": tool_payload})\n",
    "\n",
    "    # 3) æ”¶å°¾æ¸…ç†ï¼ˆä½ å·²æœ‰ï¼‰\n",
    "    messages = sanitize_messages_for_gptoss(messages)\n",
    "    return {\"messages\": messages, \"tools\": gptoss_tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c2a462-46db-4989-8415-c201add083ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 7) æ‰¹é‡æ˜ å°„ + æ¸²æŸ“æˆ textï¼ˆç”¨äº SFTï¼‰ =========\n",
    "def convert_hermes_dataset_to_gptoss_text(ds, tokenizer, reasoning_effort=\"low\"):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ï¼šdatasets.Datasetï¼ˆHermesï¼‰ï¼Œä»¥åŠå·²åŠ è½½å¥½çš„ gpt-oss tokenizer\n",
    "    è¾“å‡ºï¼šæ–°çš„ Datasetï¼ŒåŒ…å«ï¼š\n",
    "      - messagesï¼ˆä¾›è°ƒè¯•ï¼‰\n",
    "      - tools_objï¼ˆä¾›è°ƒè¯•ï¼‰\n",
    "      - textï¼ˆå–‚ç»™ SFTTrainer çš„åˆ—ï¼‰\n",
    "    \"\"\"\n",
    "    def map_one(ex):\n",
    "        r = hermes_to_gptoss_example(ex)\n",
    "        return {\"messages\": r[\"messages\"], \"tools_obj\": r[\"tools\"]}\n",
    "\n",
    "    ds2 = ds.map(map_one)\n",
    "\n",
    "    def render_batch(batch):\n",
    "        out = []\n",
    "        for msgs, tools in zip(batch[\"messages\"], batch[\"tools_obj\"]):\n",
    "            txt = tokenizer.apply_chat_template(\n",
    "                msgs,\n",
    "                tools=tools,                   # â† å…³é”®ï¼šå·¥å…·é€šè¿‡å‚æ•°ä¼ å…¥\n",
    "                add_generation_prompt=False,   # è®­ç»ƒé›†é€šå¸¸ False\n",
    "                reasoning_effort=reasoning_effort,  # \"low\"/\"medium\"/\"high\"\n",
    "                tokenize=False,\n",
    "            )\n",
    "            out.append(txt)\n",
    "        return {\"text\": out}\n",
    "\n",
    "    ds3 = ds2.map(render_batch, batched=True)\n",
    "    return ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a885fee9-2472-4270-bf02-e4eefa49e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast\n",
    "\n",
    "def parse_tools_any(obj):\n",
    "    \"\"\"\n",
    "    æŠŠ Hermes çš„ tools å­—æ®µå½’ä¸€åŒ–ä¸º [dict, ...]ï¼š\n",
    "    - æ”¯æŒ strï¼ˆJSON æˆ– Python å­—é¢é‡ï¼Œå«å•å¼•å·ï¼‰\n",
    "    - æ”¯æŒ dictï¼ˆå•ä¸ªå·¥å…·ï¼Œè½¬æˆ [dict]ï¼‰\n",
    "    - æ”¯æŒ listï¼ˆå…¶ä¸­å…ƒç´ å¯ä¸º str æˆ– dictï¼‰\n",
    "    - å…¶ä»–ç±»å‹è¿”å› []\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return []\n",
    "\n",
    "    # çº¯å­—ç¬¦ä¸²ï¼šå°è¯• json -> ast\n",
    "    if isinstance(obj, str):\n",
    "        s = obj.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        for loader in (json.loads, ast.literal_eval):\n",
    "            try:\n",
    "                val = loader(s)\n",
    "                return parse_tools_any(val)  # é€’å½’å½’ä¸€åŒ–\n",
    "            except Exception:\n",
    "                pass\n",
    "        return []  # å®åœ¨è§£æä¸äº†å°±ç©º\n",
    "\n",
    "    # å­—å…¸ï¼šåŒ…æˆå•å…ƒç´ åˆ—è¡¨\n",
    "    if isinstance(obj, dict):\n",
    "        return [obj]\n",
    "\n",
    "    # åˆ—è¡¨ï¼šé€ä¸ªå…ƒç´ è§£æ\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for it in obj:\n",
    "            if isinstance(it, dict):\n",
    "                out.append(it)\n",
    "            elif isinstance(it, str):\n",
    "                it_s = it.strip()\n",
    "                ok = None\n",
    "                for loader in (json.loads, ast.literal_eval):\n",
    "                    try:\n",
    "                        val = loader(it_s)\n",
    "                        ok = val\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if isinstance(ok, dict):\n",
    "                    out.append(ok)\n",
    "                elif isinstance(ok, list):\n",
    "                    out.extend(parse_tools_any(ok))\n",
    "                # å…¶ä»–æƒ…å†µä¸¢å¼ƒè¿™ä¸ªå…ƒç´ \n",
    "        return out\n",
    "\n",
    "    # å…¶ä»–ç±»å‹ä¸¢å¼ƒ\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09454ecd-4a36-4eea-9d51-b4fa3a191699",
   "metadata": {},
   "source": [
    "å®šä¹‰å¥½äº†æ•°æ®æ¸…æ´—å‡½æ•°ä¹‹åï¼Œæ¥ä¸‹æ¥æµ‹è¯•èƒ½å¦å°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºgpt-ossæ¨¡å‹èƒ½å¤Ÿç”¨äºè®­ç»ƒçš„æ–‡æœ¬æ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70cd4a52-e6c1-46cc-a765-ad114ae6d511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tools', 'task', 'category', 'source', 'scenario_category'],\n",
       "    num_rows: 51004\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bb869e5-adb6-4ffd-a343-a95b4309ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹ Hermes æ•°æ®ï¼š\n",
      "{'conversations': [{'from': 'system', 'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions. Here are the available tools: <tools> [{\"name\": \"calculate_age\", \"description\": \"Calculates the age based on the birthdate.\", \"parameters\": {\"birthdate\": {\"description\": \"The birthdate in the format \\'YYYY-MM-DD\\'.\", \"type\": \"str\"}}}, {\"name\": \"find_duplicate_number\", \"description\": \"Finds the duplicate number in a list of integers.\", \"parameters\": {\"nums\": {\"description\": \"The list of integers.\", \"type\": \"List[int]\"}}}, {\"name\": \"word_frequency\", \"description\": \"Calculates the frequency of each word in a text.\", \"parameters\": {\"text\": {\"description\": \"The input text.\", \"type\": \"str\"}}}, {\"name\": \"reverse_words\", \"description\": \"Reverses the order of words in a sentence.\", \"parameters\": {\"sentence\": {\"description\": \"The input sentence.\", \"type\": \"str\"}}}] </tools> Use the following pydantic model JSON schema for each tool call you will make: {\\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}},\\'required\\': [\\'arguments\\', \\'name\\'], \\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\'} For each function call return a json object with function name and arguments within <tool_call> </tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\\n\\n'}, {'from': 'human', 'value': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\"}, {'from': 'gpt', 'value': '<think>\\nOkay, the user has two separate questions here. The first is to find the word frequency of the sentence \"The quick brown fox jumps over the lazy dog\". The second is to reverse the words in the sentence \"Hello world this is a test\". Let me break down each part step by step.\\n\\nStarting with the word frequency: I need to use the word_frequency tool. The input text is given as a string, so I\\'ll pass that into the tool. That should return a dictionary where each key is a word and the value is its frequency.\\n\\nNext, the reverse the sentence part. The user wants the words reversed. That\\'s a straightforward reverse of words in the sentence. The reverse_words tool should handle this. The sentence is \"Hello world this is a test\", so I\\'ll pass that into the reverse_words function, which will return the sentence with words reversed.\\n\\nSo I need to make two separate function calls: one for word_frequency and another for reverse_words. I\\'ll use the word_frequency tool with the sentence as the text parameter, and reverse_words with the same sentence. The outputs from each tool will be returned as JSON objects.\\n</think>\\n<tool_call>\\n{\"name\":\"word_frequency\",\"arguments\":{\"text\":\"The quick brown fox jumps over the lazy dog\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\":\"reverse_words\",\"arguments\":{\"sentence\":\"Hello world this is a test\"}}\\n</tool_call>'}], 'tools': '[{\"name\": \"calculate_age\", \"description\": \"Calculates the age based on the birthdate.\", \"parameters\": {\"birthdate\": {\"description\": \"The birthdate in the format \\'YYYY-MM-DD\\'.\", \"type\": \"str\"}}}, {\"name\": \"find_duplicate_number\", \"description\": \"Finds the duplicate number in a list of integers.\", \"parameters\": {\"nums\": {\"description\": \"The list of integers.\", \"type\": \"List[int]\"}}}, {\"name\": \"word_frequency\", \"description\": \"Calculates the frequency of each word in a text.\", \"parameters\": {\"text\": {\"description\": \"The input text.\", \"type\": \"str\"}}}, {\"name\": \"reverse_words\", \"description\": \"Reverses the order of words in a sentence.\", \"parameters\": {\"sentence\": {\"description\": \"The input sentence.\", \"type\": \"str\"}}}]', 'task': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\", 'category': 'Word Frequency', 'source': 'Salesforce-Xlam', 'scenario_category': 'single'}\n"
     ]
    }
   ],
   "source": [
    "# 2) éšä¾¿å–ä¸€æ¡ï¼ˆæ¯”å¦‚ç¬¬ 0 æ¡ï¼‰\n",
    "sample = ds_full[0]\n",
    "print(\"åŸå§‹ Hermes æ•°æ®ï¼š\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f4f8ea-932a-4612-9daa-b199586f43d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è½¬æ¢åçš„ gpt-oss messagesï¼š\n",
      "{'role': 'developer', 'content': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.'}\n",
      "{'role': 'user', 'content': \"What is the word frequency of 'The quick brown fox jumps over the lazy dog'? And can you reverse the words in the sentence 'Hello world this is a test'?\"}\n",
      "{'role': 'assistant', 'tool_calls': [{'function': {'name': 'word_frequency', 'arguments': {'text': 'The quick brown fox jumps over the lazy dog'}}}], 'thinking': 'Okay, the user has two separate questions here. The first is to find the word frequency of the sentence \"The quick brown fox jumps over the lazy dog\". The second is to reverse the words in the sentence \"Hello world this is a test\". Let me break down each part step by step.\\n\\nStarting with the word frequency: I need to use the word_frequency tool. The input text is given as a string, so I\\'ll pass that into the tool. That should return a dictionary where each key is a word and the value is its frequency.\\n\\nNext, the reverse the sentence part. The user wants the words reversed. That\\'s a straightforward reverse of words in the sentence. The reverse_words tool should handle this. The sentence is \"Hello world this is a test\", so I\\'ll pass that into the reverse_words function, which will return the sentence with words reversed.\\n\\nSo I need to make two separate function calls: one for word_frequency and another for reverse_words. I\\'ll use the word_frequency tool with the sentence as the text parameter, and reverse_words with the same sentence. The outputs from each tool will be returned as JSON objects.'}\n",
      "{'role': 'assistant', 'tool_calls': [{'function': {'name': 'reverse_words', 'arguments': {'sentence': 'Hello world this is a test'}}}]}\n"
     ]
    }
   ],
   "source": [
    "# 3) å•æ¡è½¬æ¢ï¼ˆä¸ç”¨æ•´ä¸ª mapï¼‰\n",
    "converted = hermes_to_gptoss_example(sample)\n",
    "print(\"\\nè½¬æ¢åçš„ gpt-oss messagesï¼š\")\n",
    "for m in converted[\"messages\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8f87b1-bbf1-45a2-85c0-4a8560ea050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a5184d-55af-460e-b20a-53d6bc01ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192\n",
    "dtype = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a04cfe-1ee3-4a6d-bbc2-8f65d2f6aa0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.7: Fast Gpt_Oss patching. Transformers: 4.56.0.dev0.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gpt_Oss does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./gpt-oss-20b-bf16\",\n",
    "    dtype = dtype, \n",
    "    max_seq_length = max_seq_length, \n",
    "    load_in_4bit = True,  \n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66c02e19-89b0-4529-9bac-571cdbee4f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ¸²æŸ“åçš„è®­ç»ƒæ–‡æœ¬ï¼š\n"
     ]
    }
   ],
   "source": [
    "# 4) æ¸²æŸ“æˆ gpt-oss æ–‡æœ¬\n",
    "text = tokenizer.apply_chat_template(\n",
    "    converted[\"messages\"],\n",
    "    tools=converted[\"tools\"],\n",
    "    add_generation_prompt=False,\n",
    "    reasoning_effort=\"low\",\n",
    "    tokenize=False,\n",
    ")\n",
    "print(\"\\næ¸²æŸ“åçš„è®­ç»ƒæ–‡æœ¬ï¼š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da6bb3f0-fa1b-4bbe-babb-484b6f9aacf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Calculates the age based on the birthdate.\\ntype calculate_age = (_: {\\n// The birthdate in the format \\'YYYY-MM-DD\\'.\\nbirthdate: string\\n}) => any;\\n\\n// Finds the duplicate number in a list of integers.\\ntype find_duplicate_number = (_: {\\n// The list of integers.\\nnums: number[]\\n}) => any;\\n\\n// Calculates the frequency of each word in a text.\\ntype word_frequency = (_: {\\n// The input text.\\ntext: string\\n}) => any;\\n\\n// Reverses the order of words in a sentence.\\ntype reverse_words = (_: {\\n// The input sentence.\\nsentence: string\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>What is the word frequency of \\'The quick brown fox jumps over the lazy dog\\'? And can you reverse the words in the sentence \\'Hello world this is a test\\'?<|end|><|start|>assistant to=functions.word_frequency<|channel|>commentary json<|message|>{\"text\": \"The quick brown fox jumps over the lazy dog\"}<|call|><|start|>assistant to=functions.reverse_words<|channel|>commentary json<|message|>{\"sentence\": \"Hello world this is a test\"}<|call|>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0977db3-988e-4d81-b7d3-07419edeb699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹ Hermes æ•°æ®ï¼š\n",
      "{'conversations': [{'from': 'system', 'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. If available tools are not relevant in assisting with user query, just respond in natural conversational language. Don\\'t make assumptions about what values to plug into functions. After calling & executing the functions, you will be provided with function results within <tool_response> </tool_response> XML tags. Here are the available tools:\\n<tools>\\n[{\"name\":\"get_biodegradation_rate\",\"description\":\"Retrieve the rate of biodegradation for a specific substance\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"substance\":{\"type\":\"string\",\"description\":\"The substance for which you want to retrieve the biodegradation rate (e.g., plastic, oil, pesticide)\"}},\"required\":[\"substance\"]},\"required\":null},{\"name\":\"calculate_co2_emissions\",\"description\":\"Calculate the CO2 emissions produced by a specific process\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"process\":{\"type\":\"string\",\"description\":\"The specific process for which you want to calculate CO2 emissions (e.g., manufacturing, transportation)\"},\"amount\":{\"type\":\"float\",\"description\":\"The amount of the process (e.g., number of units, distance traveled)\"}},\"required\":[\"process\",\"amount\"]},\"required\":null},{\"name\":\"get_microbial_diversity\",\"description\":\"Retrieve the microbial diversity in a specific environment\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"environment\":{\"type\":\"string\",\"description\":\"The environment for which you want to retrieve the microbial diversity (e.g., soil, water, air)\"}},\"required\":[\"environment\"]},\"required\":null},{\"name\":\"get_toxicity_level\",\"description\":\"Retrieve the toxicity level of a specific substance\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"substance\":{\"type\":\"string\",\"description\":\"The substance for which you want to retrieve the toxicity level (e.g., heavy metals, pollutants, chemicals)\"}},\"required\":[\"substance\"]},\"required\":null},{\"name\":\"measure_air_quality\",\"description\":\"Measure the air quality at a specific location\",\"parameters\":{\"type\":\"dict\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The location for which you want to measure the air quality (e.g., city, address)\"},\"duration\":{\"type\":\"int\",\"description\":\"The duration of the air quality measurement in minutes\"}},\"required\":[\"location\",\"duration\"]},\"required\":null}]\\n</tools>\\nFor each function call return a JSON object, with the following pydantic model json schema for each:\\n{\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}, \\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}},\\'required\\': [\\'name\\', \\'arguments\\']}\\nEach function call should be enclosed within <tool_call> </tool_call> XML tags.\\nExample:\\n<tool_call>\\n{\\'name\\': <function-name>,\\'arguments\\': <args-dict>}\\n</tool_call>\\n\\n'}, {'from': 'human', 'value': 'I am conducting an environmental impact study involving several substances. Could you help me find out the toxicity level of lead and then measure the air quality in New York City for the next 60 minutes based on the presence of lead?\\n\\n'}, {'from': 'gpt', 'value': '<think>\\nOkay, the user wants to find the toxicity level of lead and measure air quality in NYC for 60 minutes. Let me start with the first task: getting the toxicity level of lead. I\\'ll use the \\'get_toxicity_level\\' tool with the substance \\'lead\\'.\\n\\nOnce I get that result, they also want air quality measurements in NYC for 60 minutes. So after the first response, I should call the\\'measure_air_quality\\' tool with the location set to NYC and duration 60.\\n</think>\\n<tool_call>\\n{\"name\":\"get_toxicity_level\",\"arguments\":{\"substance\":\"lead\"}}\\n</tool_call>'}, {'from': 'tool', 'value': '<tool_response>\\n{\"tool_call_id\": \"call_1142e3bd37684f34a097b1722b664989\", \"name\": \"get_toxicity_level\", \"content\": {\"level\": \"high\"}}\\n</tool_response>'}, {'from': 'gpt', 'value': '<think>\\nAlright, the toxicity level of lead is high. Now the next step is to measure the air quality in NYC for 60 minutes. I\\'ll use the\\'measure_air_quality\\' tool with location as \\'New York City\\' and duration 60.\\n\\nWait, the user didn\\'t specify any particular location within NYC, so I\\'ll assume they mean the city as a whole. Let me make sure I input that correctly.\\n</think>\\n<tool_call>\\n{\"name\":\"measure_air_quality\",\"arguments\":{\"location\":\"New York City\",\"duration\":60}}\\n</tool_call>'}, {'from': 'tool', 'value': '<tool_response>\\n{\"tool_call_id\": \"call_007f402863ee4a6da707337447343a52\", \"name\": \"measure_air_quality\", \"content\": {\"air_quality_index\": 50}}\\n</tool_response>'}, {'from': 'gpt', 'value': \"<think>\\nOkay, the user asked for the toxicity level of lead and the air quality in NYC. The first tool provided a high toxicity level for lead, which is important to note in environmental impact studies. For the air quality measurement, it came back as 50, which is moderate. The air quality index (AQI) of 50 falls under the 'Good' category, so the air quality in NYC is good. Since the user didn't specify any other parameters, these two results should cover their request. I'll summarize the findings to them.\\n</think>The toxicity level of lead is **high**, and the air quality in New York City for the next 60 minutes is **good** (AQI of 50). This indicates a moderate level of air quality, suggesting that there are no immediate health concerns related to air pollution.\"}, {'from': 'gpt', 'value': '<think>\\nOkay, the user wants to know the biodegradation rate of plastic and the CO2 emissions from burning oil. Let me start by calling the biodegradation tool for plastic. After that, for CO2 emissions, I need to specify the process as \\'burning\\' and the amount, but the user didn\\'t mention the amount. Hmm, maybe they mean burning a certain amount, but it\\'s not specified. Maybe I should ask for the amount as part of the process. Wait, the user might have missed providing that. Let me check the original query again. Oh, the user just says \"calculate CO2 emissions produced by burning oil\", so perhaps they expect a general estimate. Maybe I should clarify with a friendly prompt, but the instructions say to proceed as given. Alternatively, maybe use a default amount or ask if they can specify, but perhaps the tools require a numeric value. Wait, looking at the calculate_co2_emissions tool parameters, it requires an amount. So without that, I might need to prompt the user, but according to the instructions, I should call the tool even if the parameters aren\\'t fully specified. Wait, but the user might have meant that the amount is variable. Hmm, perhaps the tool expects a specific value. Maybe I should proceed by asking the user for the amount, but the original query doesn\\'t specify. Since it\\'s part of the available tools, perhaps I should call it with a placeholder, but that might be an issue. Alternatively, maybe the user expects a general estimate without a specific amount. Maybe I should proceed with a general calculation, but the tool requires an amount. This is a problem. Wait, the user\\'s query says \"calculate CO2 emissions produced by burning oil\". So they might want a general calculation, but the tool needs a numerical amount. I need to figure out if there\\'s a standard amount to use or if I should prompt the user. However, the instructions say to use the given parameters as is. Therefore, perhaps I should proceed by indicating that the CO2 calculation requires an amount, which was missing. Let me proceed with the first tool call for biodegradation rate of plastic, then proceed to the CO2 emissions tool with a placeholder or error message. Wait, perhaps the user intended to provide the amount but missed it. Since I can\\'t assume, maybe I should inform them that I need the amount for the CO2 calculation. But according to the initial instructions, should I proceed or wait? The instructions say to call exactly one tool at a time and wait for its response. Since the CO2 tool requires an amount and it\\'s missing, perhaps I should inform the user to provide that before proceeding. But the system might expect me to proceed as per the query. Hmm, this is a challenge. Let me think again.\\n\\nThe user\\'s exact query is: \"Calculate the CO2 emissions produced by burning oil.\" Without specifying the amount, like how much oil is being burned. So the calculate_co2_emissions tool requires both process (which is \\'burning\\') and amount (the quantity, like litres, tonnes, etc.). Since that\\'s missing, I should inform the user that the amount is missing. But the problem is that the user might not realize that. Given that the instructions say to proceed sequentially, perhaps I should call the first tool, then for the second, indicate that the amount is required. But the initial instructions say to decide whether to call another tool based on the response. So, for the CO2 call, I should call it with a placeholder amount, like 1000 units, but that\\'s assuming. Alternatively, maybe the user expects using a common scenario. Alternatively, perhaps the user wants a general estimate without specific numbers, but the tool requires a number. Since I can\\'t proceed without that, perhaps I should inform the user to provide the amount. But according to the initial problem statement, the user is supposed to be in a scenario where they are asking via the AI, and the assistant should handle it. Maybe I\\'ll proceed by calling the CO2 tool with a placeholder amount and explain that. But I\\'m not sure. Alternatively, maybe the user expects the tool to handle it. But in the absence of specific data, I can\\'t proceed, so I should request the user to provide the amount. However, given the instructions, perhaps I should proceed to call the tools as per the initial query and note the missing data. Hmm, this is a bit of a problem. Let\\'s proceed step by step.\\n\\nFirst, biodegradation rate of plastic:\\n\\nCall: get_biodegradation_rate with substance=\\'plastic\\'\\n\\nThen, calculate_co2_emissions with process=\\'burning oil\\' and amount=... (but amount is missing). Therefore, first, I\\'ll call the biodegradation tool, then the CO2 tool with a placeholder amount, but that might not be accurate. Alternatively, maybe the user intended to provide an example, but didn\\'t. Since I can\\'t assume, I\\'ll inform the user that the'}], 'tools': '[{\"name\": \"get_biodegradation_rate\", \"description\": \"Retrieve the rate of biodegradation for a specific substance\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"substance\": {\"type\": \"string\", \"description\": \"The substance for which you want to retrieve the biodegradation rate (e.g., plastic, oil, pesticide)\"}}, \"required\": [\"substance\"]}, \"required\": null}, {\"name\": \"calculate_co2_emissions\", \"description\": \"Calculate the CO2 emissions produced by a specific process\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"process\": {\"type\": \"string\", \"description\": \"The specific process for which you want to calculate CO2 emissions (e.g., manufacturing, transportation)\"}, \"amount\": {\"type\": \"float\", \"description\": \"The amount of the process (e.g., number of units, distance traveled)\"}}, \"required\": [\"process\", \"amount\"]}, \"required\": null}, {\"name\": \"get_microbial_diversity\", \"description\": \"Retrieve the microbial diversity in a specific environment\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"environment\": {\"type\": \"string\", \"description\": \"The environment for which you want to retrieve the microbial diversity (e.g., soil, water, air)\"}}, \"required\": [\"environment\"]}, \"required\": null}, {\"name\": \"get_toxicity_level\", \"description\": \"Retrieve the toxicity level of a specific substance\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"substance\": {\"type\": \"string\", \"description\": \"The substance for which you want to retrieve the toxicity level (e.g., heavy metals, pollutants, chemicals)\"}}, \"required\": [\"substance\"]}, \"required\": null}, {\"name\": \"measure_air_quality\", \"description\": \"Measure the air quality at a specific location\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location for which you want to measure the air quality (e.g., city, address)\"}, \"duration\": {\"type\": \"int\", \"description\": \"The duration of the air quality measurement in minutes\"}}, \"required\": [\"location\", \"duration\"]}, \"required\": null}]', 'task': 'I am conducting an environmental impact study involving several substances. Could you help me find out the toxicity level of lead and then measure the air quality in New York City for the next 60 minutes based on the presence of lead?', 'category': 'Get Toxicity Level', 'source': 'ToolAce', 'scenario_category': 'multistep'}\n"
     ]
    }
   ],
   "source": [
    "sample = ds[15]\n",
    "print(\"åŸå§‹ Hermes æ•°æ®ï¼š\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "343e3223-1368-435d-b43f-dbac5425deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è½¬æ¢åçš„ gpt-oss messagesï¼š\n",
      "{'role': 'developer', 'content': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.'}\n",
      "{'role': 'user', 'content': 'I am conducting an environmental impact study involving several substances. Could you help me find out the toxicity level of lead and then measure the air quality in New York City for the next 60 minutes based on the presence of lead?\\n\\n'}\n",
      "{'role': 'assistant', 'tool_calls': [{'function': {'name': 'get_toxicity_level', 'arguments': {'substance': 'lead'}}}], 'thinking': \"Okay, the user wants to find the toxicity level of lead and measure air quality in NYC for 60 minutes. Let me start with the first task: getting the toxicity level of lead. I'll use the 'get_toxicity_level' tool with the substance 'lead'.\\n\\nOnce I get that result, they also want air quality measurements in NYC for 60 minutes. So after the first response, I should call the'measure_air_quality' tool with the location set to NYC and duration 60.\"}\n",
      "{'role': 'tool', 'content': '{\"tool_call_id\": \"call_1142e3bd37684f34a097b1722b664989\", \"name\": \"get_toxicity_level\", \"content\": {\"level\": \"high\"}}'}\n",
      "{'role': 'assistant', 'tool_calls': [{'function': {'name': 'measure_air_quality', 'arguments': {'location': 'New York City', 'duration': 60}}}], 'thinking': \"Alright, the toxicity level of lead is high. Now the next step is to measure the air quality in NYC for 60 minutes. I'll use the'measure_air_quality' tool with location as 'New York City' and duration 60.\\n\\nWait, the user didn't specify any particular location within NYC, so I'll assume they mean the city as a whole. Let me make sure I input that correctly.\"}\n",
      "{'role': 'tool', 'content': '{\"tool_call_id\": \"call_007f402863ee4a6da707337447343a52\", \"name\": \"measure_air_quality\", \"content\": {\"air_quality_index\": 50}}'}\n",
      "{'role': 'assistant', 'thinking': \"Okay, the user asked for the toxicity level of lead and the air quality in NYC. The first tool provided a high toxicity level for lead, which is important to note in environmental impact studies. For the air quality measurement, it came back as 50, which is moderate. The air quality index (AQI) of 50 falls under the 'Good' category, so the air quality in NYC is good. Since the user didn't specify any other parameters, these two results should cover their request. I'll summarize the findings to them.\", 'content': 'The toxicity level of lead is **high**, and the air quality in New York City for the next 60 minutes is **good** (AQI of 50). This indicates a moderate level of air quality, suggesting that there are no immediate health concerns related to air pollution.'}\n",
      "{'role': 'assistant', 'content': 'Okay, the user wants to know the biodegradation rate of plastic and the CO2 emissions from burning oil. Let me start by calling the biodegradation tool for plastic. After that, for CO2 emissions, I need to specify the process as \\'burning\\' and the amount, but the user didn\\'t mention the amount. Hmm, maybe they mean burning a certain amount, but it\\'s not specified. Maybe I should ask for the amount as part of the process. Wait, the user might have missed providing that. Let me check the original query again. Oh, the user just says \"calculate CO2 emissions produced by burning oil\", so perhaps they expect a general estimate. Maybe I should clarify with a friendly prompt, but the instructions say to proceed as given. Alternatively, maybe use a default amount or ask if they can specify, but perhaps the tools require a numeric value. Wait, looking at the calculate_co2_emissions tool parameters, it requires an amount. So without that, I might need to prompt the user, but according to the instructions, I should call the tool even if the parameters aren\\'t fully specified. Wait, but the user might have meant that the amount is variable. Hmm, perhaps the tool expects a specific value. Maybe I should proceed by asking the user for the amount, but the original query doesn\\'t specify. Since it\\'s part of the available tools, perhaps I should call it with a placeholder, but that might be an issue. Alternatively, maybe the user expects a general estimate without a specific amount. Maybe I should proceed with a general calculation, but the tool requires an amount. This is a problem. Wait, the user\\'s query says \"calculate CO2 emissions produced by burning oil\". So they might want a general calculation, but the tool needs a numerical amount. I need to figure out if there\\'s a standard amount to use or if I should prompt the user. However, the instructions say to use the given parameters as is. Therefore, perhaps I should proceed by indicating that the CO2 calculation requires an amount, which was missing. Let me proceed with the first tool call for biodegradation rate of plastic, then proceed to the CO2 emissions tool with a placeholder or error message. Wait, perhaps the user intended to provide the amount but missed it. Since I can\\'t assume, maybe I should inform them that I need the amount for the CO2 calculation. But according to the initial instructions, should I proceed or wait? The instructions say to call exactly one tool at a time and wait for its response. Since the CO2 tool requires an amount and it\\'s missing, perhaps I should inform the user to provide that before proceeding. But the system might expect me to proceed as per the query. Hmm, this is a challenge. Let me think again.\\n\\nThe user\\'s exact query is: \"Calculate the CO2 emissions produced by burning oil.\" Without specifying the amount, like how much oil is being burned. So the calculate_co2_emissions tool requires both process (which is \\'burning\\') and amount (the quantity, like litres, tonnes, etc.). Since that\\'s missing, I should inform the user that the amount is missing. But the problem is that the user might not realize that. Given that the instructions say to proceed sequentially, perhaps I should call the first tool, then for the second, indicate that the amount is required. But the initial instructions say to decide whether to call another tool based on the response. So, for the CO2 call, I should call it with a placeholder amount, like 1000 units, but that\\'s assuming. Alternatively, maybe the user expects using a common scenario. Alternatively, perhaps the user wants a general estimate without specific numbers, but the tool requires a number. Since I can\\'t proceed without that, perhaps I should inform the user to provide the amount. But according to the initial problem statement, the user is supposed to be in a scenario where they are asking via the AI, and the assistant should handle it. Maybe I\\'ll proceed by calling the CO2 tool with a placeholder amount and explain that. But I\\'m not sure. Alternatively, maybe the user expects the tool to handle it. But in the absence of specific data, I can\\'t proceed, so I should request the user to provide the amount. However, given the instructions, perhaps I should proceed to call the tools as per the initial query and note the missing data. Hmm, this is a bit of a problem. Let\\'s proceed step by step.\\n\\nFirst, biodegradation rate of plastic:\\n\\nCall: get_biodegradation_rate with substance=\\'plastic\\'\\n\\nThen, calculate_co2_emissions with process=\\'burning oil\\' and amount=... (but amount is missing). Therefore, first, I\\'ll call the biodegradation tool, then the CO2 tool with a placeholder amount, but that might not be accurate. Alternatively, maybe the user intended to provide an example, but didn\\'t. Since I can\\'t assume, I\\'ll inform the user that the'}\n"
     ]
    }
   ],
   "source": [
    "converted = hermes_to_gptoss_example(sample)\n",
    "print(\"\\nè½¬æ¢åçš„ gpt-oss messagesï¼š\")\n",
    "for m in converted[\"messages\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0512a742-7254-40d4-8789-2332cc4d91e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ¸²æŸ“åçš„è®­ç»ƒæ–‡æœ¬ï¼š\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    converted[\"messages\"],\n",
    "    tools=converted[\"tools\"],\n",
    "    add_generation_prompt=False,\n",
    "    reasoning_effort=\"low\",\n",
    "    tokenize=False,\n",
    ")\n",
    "print(\"\\næ¸²æŸ“åçš„è®­ç»ƒæ–‡æœ¬ï¼š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b454d4d6-4483-4f2b-8a3a-336b5430f762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Retrieve the rate of biodegradation for a specific substance\\ntype get_biodegradation_rate = (_: {\\n// The substance for which you want to retrieve the biodegradation rate (e.g., plastic, oil, pesticide)\\nsubstance: string\\n}) => any;\\n\\n// Calculate the CO2 emissions produced by a specific process\\ntype calculate_co2_emissions = (_: {\\n// The specific process for which you want to calculate CO2 emissions (e.g., manufacturing, transportation)\\nprocess: string,\\n// The amount of the process (e.g., number of units, distance traveled)\\namount: any\\n}) => any;\\n\\n// Retrieve the microbial diversity in a specific environment\\ntype get_microbial_diversity = (_: {\\n// The environment for which you want to retrieve the microbial diversity (e.g., soil, water, air)\\nenvironment: string\\n}) => any;\\n\\n// Retrieve the toxicity level of a specific substance\\ntype get_toxicity_level = (_: {\\n// The substance for which you want to retrieve the toxicity level (e.g., heavy metals, pollutants, chemicals)\\nsubstance: string\\n}) => any;\\n\\n// Measure the air quality at a specific location\\ntype measure_air_quality = (_: {\\n// The location for which you want to measure the air quality (e.g., city, address)\\nlocation: string,\\n// The duration of the air quality measurement in minutes\\nduration: any\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>I am conducting an environmental impact study involving several substances. Could you help me find out the toxicity level of lead and then measure the air quality in New York City for the next 60 minutes based on the presence of lead?\\n\\n<|end|><|start|>assistant to=functions.get_toxicity_level<|channel|>commentary json<|message|>{\"substance\": \"lead\"}<|call|><|start|>functions.get_toxicity_level to=assistant<|channel|>commentary<|message|>\"{\\\\\"tool_call_id\\\\\": \\\\\"call_1142e3bd37684f34a097b1722b664989\\\\\", \\\\\"name\\\\\": \\\\\"get_toxicity_level\\\\\", \\\\\"content\\\\\": {\\\\\"level\\\\\": \\\\\"high\\\\\"}}\"<|end|><|start|>assistant to=functions.measure_air_quality<|channel|>commentary json<|message|>{\"location\": \"New York City\", \"duration\": 60}<|call|><|start|>functions.measure_air_quality to=assistant<|channel|>commentary<|message|>\"{\\\\\"tool_call_id\\\\\": \\\\\"call_007f402863ee4a6da707337447343a52\\\\\", \\\\\"name\\\\\": \\\\\"measure_air_quality\\\\\", \\\\\"content\\\\\": {\\\\\"air_quality_index\\\\\": 50}}\"<|end|><|start|>assistant<|channel|>final<|message|>The toxicity level of lead is **high**, and the air quality in New York City for the next 60 minutes is **good** (AQI of 50). This indicates a moderate level of air quality, suggesting that there are no immediate health concerns related to air pollution.<|end|><|start|>assistant<|message|>Okay, the user wants to know the biodegradation rate of plastic and the CO2 emissions from burning oil. Let me start by calling the biodegradation tool for plastic. After that, for CO2 emissions, I need to specify the process as \\'burning\\' and the amount, but the user didn\\'t mention the amount. Hmm, maybe they mean burning a certain amount, but it\\'s not specified. Maybe I should ask for the amount as part of the process. Wait, the user might have missed providing that. Let me check the original query again. Oh, the user just says \"calculate CO2 emissions produced by burning oil\", so perhaps they expect a general estimate. Maybe I should clarify with a friendly prompt, but the instructions say to proceed as given. Alternatively, maybe use a default amount or ask if they can specify, but perhaps the tools require a numeric value. Wait, looking at the calculate_co2_emissions tool parameters, it requires an amount. So without that, I might need to prompt the user, but according to the instructions, I should call the tool even if the parameters aren\\'t fully specified. Wait, but the user might have meant that the amount is variable. Hmm, perhaps the tool expects a specific value. Maybe I should proceed by asking the user for the amount, but the original query doesn\\'t specify. Since it\\'s part of the available tools, perhaps I should call it with a placeholder, but that might be an issue. Alternatively, maybe the user expects a general estimate without a specific amount. Maybe I should proceed with a general calculation, but the tool requires an amount. This is a problem. Wait, the user\\'s query says \"calculate CO2 emissions produced by burning oil\". So they might want a general calculation, but the tool needs a numerical amount. I need to figure out if there\\'s a standard amount to use or if I should prompt the user. However, the instructions say to use the given parameters as is. Therefore, perhaps I should proceed by indicating that the CO2 calculation requires an amount, which was missing. Let me proceed with the first tool call for biodegradation rate of plastic, then proceed to the CO2 emissions tool with a placeholder or error message. Wait, perhaps the user intended to provide the amount but missed it. Since I can\\'t assume, maybe I should inform them that I need the amount for the CO2 calculation. But according to the initial instructions, should I proceed or wait? The instructions say to call exactly one tool at a time and wait for its response. Since the CO2 tool requires an amount and it\\'s missing, perhaps I should inform the user to provide that before proceeding. But the system might expect me to proceed as per the query. Hmm, this is a challenge. Let me think again.\\n\\nThe user\\'s exact query is: \"Calculate the CO2 emissions produced by burning oil.\" Without specifying the amount, like how much oil is being burned. So the calculate_co2_emissions tool requires both process (which is \\'burning\\') and amount (the quantity, like litres, tonnes, etc.). Since that\\'s missing, I should inform the user that the amount is missing. But the problem is that the user might not realize that. Given that the instructions say to proceed sequentially, perhaps I should call the first tool, then for the second, indicate that the amount is required. But the initial instructions say to decide whether to call another tool based on the response. So, for the CO2 call, I should call it with a placeholder amount, like 1000 units, but that\\'s assuming. Alternatively, maybe the user expects using a common scenario. Alternatively, perhaps the user wants a general estimate without specific numbers, but the tool requires a number. Since I can\\'t proceed without that, perhaps I should inform the user to provide the amount. But according to the initial problem statement, the user is supposed to be in a scenario where they are asking via the AI, and the assistant should handle it. Maybe I\\'ll proceed by calling the CO2 tool with a placeholder amount and explain that. But I\\'m not sure. Alternatively, maybe the user expects the tool to handle it. But in the absence of specific data, I can\\'t proceed, so I should request the user to provide the amount. However, given the instructions, perhaps I should proceed to call the tools as per the initial query and note the missing data. Hmm, this is a bit of a problem. Let\\'s proceed step by step.\\n\\nFirst, biodegradation rate of plastic:\\n\\nCall: get_biodegradation_rate with substance=\\'plastic\\'\\n\\nThen, calculate_co2_emissions with process=\\'burning oil\\' and amount=... (but amount is missing). Therefore, first, I\\'ll call the biodegradation tool, then the CO2 tool with a placeholder amount, but that might not be accurate. Alternatively, maybe the user intended to provide an example, but didn\\'t. Since I can\\'t assume, I\\'ll inform the user that the<|return|>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6acfb-04f7-4d90-b1b0-20241ecabe85",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠ½å–200æ¡æ•°æ®è¿›è¡Œæ‰¹é‡è½¬åŒ–ä¸å¿«é€Ÿå¾®è°ƒæ•ˆæœéªŒè¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "563fe36f-ca78-45c4-8b8b-2f7e4fed5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, Features, Value\n",
    "import random, json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bdfdc9f-931f-4bab-b71b-09305123a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºé‡‡æ · 200 æ¡ï¼ˆå¯æ¢æˆä½ æƒ³è¦çš„ç´¢å¼•é›†åˆï¼‰\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "idxs = random.sample(range(len(ds_full)), 200)\n",
    "ds_sample = ds_full.select(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7af613f3-7a95-46fc-966d-95487c2e12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tools', 'task', 'category', 'source', 'scenario_category'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_sample)  # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9af64f7-5f8d-4cbe-a259-0c5e83831984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\\n\\nYou are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. If available tools are not relevant in assisting with user query, just respond in natural conversational language. Don\\'t make assumptions about what values to plug into functions. After calling & executing the functions, you will be provided with function results within <tool_response> </tool_response> XML tags. Here are the available tools:\\n<tools>\\n[\\n{\"name\": \"is_hotel_available\", \"description\": \"Checks the availability of a hotel for a given date range.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"hotel\": {\"description\": \"The name of the hotel.\", \"type\": \"str\"}, \"city\": {\"description\": \"The city where the hotel is located.\", \"type\": \"str\"}, \"checkin\": {\"description\": \"The check-in date in the format \\\\\"YYYY-MM-DD\\\\\".\", \"type\": \"str\"}, \"checkout\": {\"description\": \"The check-out date in the format \\\\\"YYYY-MM-DD\\\\\".\", \"type\": \"str\"}}}, \"required\": [\"hotel\", \"city\", \"checkin\", \"checkout\"]}\\n]\\n</tools>\\nFor each function call return a JSON object, with the following pydantic model json schema for each:\\n{\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'type\\':\\'string\\'}, \\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}},\\'required\\': [\\'name\\', \\'arguments\\']}\\nEach function call should be enclosed within <tool_call> </tool_call> XML tags.\\nExample:\\n<tool_call>\\n{\\'name\\': <function-name>,\\'arguments\\': <args-dict>}\\n</tool_call>\\n\\n'},\n",
       "  {'from': 'human',\n",
       "   'value': 'I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '<think>\\nOkay, the user wants to check the availability of the Queens Hotel in Berlin from March 15th to March 20th, 2022. Let me start by recalling the available tools. The only tool provided here is the hotel availability checker, which is called \"is_hotel_available\". \\n\\nSo, I need to construct the proper function call for that tool. The parameters required are hotel, city, checkin, and checkout. The user mentioned the hotel is Queens Hotel, city is Berlin, check-in is March 15, 2022 (which should be formatted as YYYY-MM-DD, so 2022-03-15), and check-out is March 20, 2022 (2022-03-20). \\n\\nI\\'ll call the is_hotel_available function with these parameters and then present the response. The Python code will need to use the function name and pass the dictionary with the required details. Once the function is called, the result will be in JSON format, which I should parse to show the availability information.\\n</think>\\n<tool_call>\\n{\"name\":\"is_hotel_available\",\"arguments\":{\"hotel\":\"Queens Hotel\",\"city\":\"Berlin, Germany\",\"checkin\":\"2022-03-15\",\"checkout\":\"2022-03-20\"}}\\n</tool_call>'},\n",
       "  {'from': 'tool',\n",
       "   'value': '<tool_response>\\n{\"name\": \"is_hotel_available\", \"content\": {\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\", \"availability\": \"Available\", \"rooms_left\": 5, \"price_per_night\": 189, \"total_price\": 945, \"cancellation_policy\": \"Flexible until 5 days before arrival\", \"special_offers\": []}}\\n</tool_response>'}],\n",
       " 'tools': '[{\"name\": \"is_hotel_available\", \"description\": \"Checks the availability of a hotel for a given date range.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\"hotel\": {\"description\": \"The name of the hotel.\", \"type\": \"str\"}, \"city\": {\"description\": \"The city where the hotel is located.\", \"type\": \"str\"}, \"checkin\": {\"description\": \"The check-in date in the format \\\\\"YYYY-MM-DD\\\\\".\", \"type\": \"str\"}, \"checkout\": {\"description\": \"The check-out date in the format \\\\\"YYYY-MM-DD\\\\\".\", \"type\": \"str\"}}}, \"required\": [\"hotel\", \"city\", \"checkin\", \"checkout\"]}]',\n",
       " 'task': 'I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?',\n",
       " 'category': 'Is Hotel Available',\n",
       " 'source': 'Nvidia-When2Call',\n",
       " 'scenario_category': 'single'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dedfb24d-1b4b-4236-83fe-bf017ddf06ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2510.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages_json': '[{\"role\": \"developer\", \"content\": \"You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\"}, {\"role\": \"user\", \"content\": \"I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"function\": {\"name\": \"is_hotel_available\", \"arguments\": {\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\"}}}], \"thinking\": \"Okay, the user wants to check the availability of the Queens Hotel in Berlin from March 15th to March 20th, 2022. Let me start by recalling the available tools. The only tool provided here is the hotel availability checker, which is called \\\\\"is_hotel_available\\\\\". \\\\n\\\\nSo, I need to construct the proper function call for that tool. The parameters required are hotel, city, checkin, and checkout. The user mentioned the hotel is Queens Hotel, city is Berlin, check-in is March 15, 2022 (which should be formatted as YYYY-MM-DD, so 2022-03-15), and check-out is March 20, 2022 (2022-03-20). \\\\n\\\\nI\\'ll call the is_hotel_available function with these parameters and then present the response. The Python code will need to use the function name and pass the dictionary with the required details. Once the function is called, the result will be in JSON format, which I should parse to show the availability information.\"}, {\"role\": \"tool\", \"content\": \"{\\\\\"name\\\\\": \\\\\"is_hotel_available\\\\\", \\\\\"content\\\\\": {\\\\\"hotel\\\\\": \\\\\"Queens Hotel\\\\\", \\\\\"city\\\\\": \\\\\"Berlin, Germany\\\\\", \\\\\"checkin\\\\\": \\\\\"2022-03-15\\\\\", \\\\\"checkout\\\\\": \\\\\"2022-03-20\\\\\", \\\\\"availability\\\\\": \\\\\"Available\\\\\", \\\\\"rooms_left\\\\\": 5, \\\\\"price_per_night\\\\\": 189, \\\\\"total_price\\\\\": 945, \\\\\"cancellation_policy\\\\\": \\\\\"Flexible until 5 days before arrival\\\\\", \\\\\"special_offers\\\\\": []}}\"}]', 'tools_json': '[{\"type\": \"function\", \"function\": {\"name\": \"is_hotel_available\", \"description\": \"Checks the availability of a hotel for a given date range.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"hotel\": {\"type\": \"string\", \"description\": \"The name of the hotel.\"}, \"city\": {\"type\": \"string\", \"description\": \"The city where the hotel is located.\"}, \"checkin\": {\"type\": \"string\", \"description\": \"The check-in date in the format \\\\\"YYYY-MM-DD\\\\\".\"}, \"checkout\": {\"type\": \"string\", \"description\": \"The check-out date in the format \\\\\"YYYY-MM-DD\\\\\".\"}}, \"required\": [\"hotel\", \"city\", \"checkin\", \"checkout\"]}}}]'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æ³¨æ„ï¼šä¸è¦è¿”å›åˆ—å 'tools'ï¼Œç”¨æ–°åå­—é¿å…ä¸åŸå§‹åˆ—å†²çª\n",
    "def map_convert_to_strings(ex):\n",
    "    out = hermes_to_gptoss_example(ex)  # -> {\"messages\": [...], \"tools\": [...]}\n",
    "    return {\n",
    "        \"messages_json\": json.dumps(out[\"messages\"], ensure_ascii=False),\n",
    "        \"tools_json\": json.dumps(out[\"tools\"], ensure_ascii=False),\n",
    "    }\n",
    "\n",
    "# æ˜¾å¼å£°æ˜ features ä¸º stringï¼Œæœç» Arrow çŒœç±»å‹\n",
    "features = Features({\n",
    "    \"messages_json\": Value(\"string\"),\n",
    "    \"tools_json\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "# ç§»é™¤åŸå§‹æ‰€æœ‰åˆ—ï¼Œåªä¿ç•™æˆ‘ä»¬éœ€è¦çš„ä¸¤åˆ—ï¼Œé¿å…åŒåå†²çª\n",
    "ds_conv = ds_sample.map(\n",
    "    map_convert_to_strings,\n",
    "    remove_columns=ds_sample.column_names,\n",
    "    features=features,\n",
    ")\n",
    "print(ds_conv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "775de61b-6e20-48b8-a2b1-fb758a3be860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages_json': '[{\"role\": \"developer\", \"content\": \"You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\"}, {\"role\": \"user\", \"content\": \"I am in New York City and I am looking for a restaurant within 500 meters. Can you help me?\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"function\": {\"name\": \"find_nearby_restaurants\", \"arguments\": {\"location\": \"New York City\", \"distance\": 500}}}], \"thinking\": \"Okay, the user is asking for nearby restaurants in New York City within 500 meters. Let me think about how to approach this. The available tools include find_nearby_restaurants. So the function find_nearby_restaurants takes a location and a distance as parameters. Since the user is in NYC, I need to specify the location as \\\\\"New York City\\\\\" and set the distance to 500 meters.\\\\n\\\\nWait, the function parameters require the location to be a string and the distance to be an integer. So I\\'ll construct the call with location as \\'New York City\\' and distance 500. That should give the required restaurants.\"}, {\"role\": \"tool\", \"content\": \"{\\\\\"restaurants\\\\\": [{\\\\\"name\\\\\": \\\\\"The Modern\\\\\", \\\\\"distance\\\\\": 300}, {\\\\\"name\\\\\": \\\\\"Le Bernardin\\\\\", \\\\\"distance\\\\\": 450}, {\\\\\"name\\\\\": \\\\\"Eleven Madison Park\\\\\", \\\\\"distance\\\\\": 500}]}\"}, {\"role\": \"assistant\", \"thinking\": \"Okay, the assistant\\'s response provided the nearby restaurants in NYC with their distances. The result shows three restaurants: The Modern (300m), Le Bernardin (450m), and Eleven Madison Park (500m). The user might want more details, so perhaps they could ask for more info like reviews or cuisine types. But since it\\'s just a simple request, the answer is sufficient. No further action is needed.\", \"content\": \"The system found these restaurants within 500 meters of your location in New York City:\\\\n1. **The Modern** - 300 meters away\\\\n2. **Le Bernardin** - 450 meters away\\\\n3. **Eleven Madison Park** - 500 meters away\\\\n\\\\nWould you like additional details about any of these restaurants?\"}]',\n",
       " 'tools_json': '[{\"type\": \"function\", \"function\": {\"name\": \"find_nearby_restaurants\", \"description\": \"Find nearby restaurants based on location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location to search for restaurants\"}, \"distance\": {\"type\": \"integer\", \"description\": \"The maximum distance in meters\"}}, \"required\": [\"location\", \"distance\"]}}}, {\"type\": \"function\", \"function\": {\"name\": \"calculate_tip\", \"description\": \"Calculate the tip amount for a bill\", \"parameters\": {\"type\": \"object\", \"properties\": {\"bill_amount\": {\"type\": \"number\", \"description\": \"The amount of the bill\"}, \"tip_percentage\": {\"type\": \"number\", \"description\": \"The percentage of tip to give\"}}, \"required\": [\"bill_amount\", \"tip_percentage\"]}}}]'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_conv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f387d64b-f1aa-4dc4-a23f-e7c97c5efd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /root/autodl-tmp/hermes_200_gptoss.jsonl\n"
     ]
    }
   ],
   "source": [
    "save_path = \"hermes_200_gptoss.jsonl\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in ds_conv:\n",
    "        f.write(json.dumps({\n",
    "            \"messages_json\": ex[\"messages_json\"],\n",
    "            \"tools_json\": ex[\"tools_json\"],\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved to:\", os.path.abspath(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2d71835-7d77-4a75-a872-4b474e66cf7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='./gpt-oss-20b-bf16', vocab_size=199998, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|return|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t199998: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199999: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200000: AddedToken(\"<|reserved_200000|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200001: AddedToken(\"<|reserved_200001|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200002: AddedToken(\"<|return|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200003: AddedToken(\"<|constrain|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200004: AddedToken(\"<|reserved_200004|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200005: AddedToken(\"<|channel|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200006: AddedToken(\"<|start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200007: AddedToken(\"<|end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200008: AddedToken(\"<|message|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200009: AddedToken(\"<|reserved_200009|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200010: AddedToken(\"<|reserved_200010|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200011: AddedToken(\"<|reserved_200011|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200012: AddedToken(\"<|call|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200013: AddedToken(\"<|reserved_200013|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200014: AddedToken(\"<|reserved_200014|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200015: AddedToken(\"<|reserved_200015|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200016: AddedToken(\"<|reserved_200016|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200017: AddedToken(\"<|reserved_200017|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200018: AddedToken(\"<|endofprompt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99c3f3af-04fe-42a6-91ac-1921afd6d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_tools_for_template(tools):\n",
    "    out = []\n",
    "    for t in tools or []:\n",
    "        if not isinstance(t, dict): \n",
    "            continue\n",
    "        fn = t.get(\"function\") if (t.get(\"type\") == \"function\" and isinstance(t.get(\"function\"), dict)) else t\n",
    "        fn.setdefault(\"description\", \"\")\n",
    "        params = fn.get(\"parameters\") or {}\n",
    "        if isinstance(params, dict):\n",
    "            if params.get(\"type\") == \"dict\":\n",
    "                params[\"type\"] = \"object\"                     # â‘  dict -> object\n",
    "            if params.get(\"type\") == \"object\":\n",
    "                props = params.setdefault(\"properties\", {})\n",
    "                if not isinstance(props, dict): \n",
    "                    params[\"properties\"] = {}\n",
    "                # æŠŠå¤–å±‚ required å¹¶è¿›å»\n",
    "                if \"required\" not in params and isinstance(fn.get(\"required\"), list):\n",
    "                    params[\"required\"] = fn[\"required\"]       # â‘¡ åˆå¹¶ required\n",
    "                # ä¿è¯æ¯ä¸ªå±æ€§æœ‰ type/description\n",
    "                for k, spec in list(props.items()):\n",
    "                    if not isinstance(spec, dict): \n",
    "                        spec = {}\n",
    "                    spec.setdefault(\"type\", \"string\")\n",
    "                    spec.setdefault(\"description\", \"\")\n",
    "                    props[k] = spec\n",
    "        out.append({\"type\": \"function\", \"function\": fn})\n",
    "    return out\n",
    "\n",
    "def _fix_tool_messages(messages):\n",
    "    for m in messages:\n",
    "        if m.get(\"role\") == \"tool\" and isinstance(m.get(\"content\"), str):\n",
    "            s = m[\"content\"]\n",
    "            try:\n",
    "                m[\"content\"] = json.loads(s)                  # â‘¢ å­—ç¬¦ä¸² -> å¯¹è±¡\n",
    "            except Exception:\n",
    "                try:\n",
    "                    import ast\n",
    "                    m[\"content\"] = ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return messages\n",
    "\n",
    "def render_batch(batch):\n",
    "    texts = []\n",
    "    for m_str, t_str in zip(batch[\"messages_json\"], batch[\"tools_json\"]):\n",
    "        messages = json.loads(m_str)\n",
    "        tools = json.loads(t_str)\n",
    "\n",
    "        messages = _fix_tool_messages(messages)\n",
    "        tools = _fix_tools_for_template(tools)\n",
    "\n",
    "        txt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=tools,\n",
    "            add_generation_prompt=False,\n",
    "            reasoning_effort=\"low\",\n",
    "            tokenize=False,\n",
    "        )\n",
    "        texts.append(txt)\n",
    "    return {\"text\": texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01ee9e2f-98d2-4eb1-82fe-2d7d9ee00f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1927.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_text = ds_conv.map(render_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "467f4eb4-1230-4a46-91fe-40b97e6f37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages_json', 'tools_json', 'text'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a219bb8-7dd5-4a4b-bccb-fca331113255",
   "metadata": {},
   "source": [
    "å®Œæ•´æ•°æ®é›†åœ°å€ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702f7e8-fc2b-4457-bbb7-74476b5cae96",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250825200207894.png\" alt=\"image-20250825200207894\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14423123-2c31-4ebc-8774-c1c7c1537e47",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250825200241826.png\" alt=\"image-20250825200241826\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8787f2-2fa4-46fc-9457-f0493957c911",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/bff56b8959b8a17d9d0a6d1e72f5d9b8.png\" alt=\"bff56b8959b8a17d9d0a6d1e72f5d9b8\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e3d94-90df-4eb2-ba12-fb986fe837c6",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠ½å–å…¶ä¸­å‡ æ¡æ•°æ®è¿›è¡ŒæŸ¥çœ‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "659a87ef-bc46-408e-a991-758e6eb61b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Checks the availability of a hotel for a given date range.\\ntype is_hotel_available = (_: {\\n// The name of the hotel.\\nhotel: string,\\n// The city where the hotel is located.\\ncity: string,\\n// The check-in date in the format \"YYYY-MM-DD\".\\ncheckin: string,\\n// The check-out date in the format \"YYYY-MM-DD\".\\ncheckout: string\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?<|end|><|start|>assistant to=functions.is_hotel_available<|channel|>commentary json<|message|>{\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\"}<|call|><|start|>functions.is_hotel_available to=assistant<|channel|>commentary<|message|>{\"name\": \"is_hotel_available\", \"content\": {\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\", \"availability\": \"Available\", \"rooms_left\": 5, \"price_per_night\": 189, \"total_price\": 945, \"cancellation_policy\": \"Flexible until 5 days before arrival\", \"special_offers\": []}}<|end|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "abd8e41a-c6c5-4297-9df3-4796d24a59eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Find nearby restaurants based on location\\ntype find_nearby_restaurants = (_: {\\n// The location to search for restaurants\\nlocation: string,\\n// The maximum distance in meters\\ndistance: number\\n}) => any;\\n\\n// Calculate the tip amount for a bill\\ntype calculate_tip = (_: {\\n// The amount of the bill\\nbill_amount: number,\\n// The percentage of tip to give\\ntip_percentage: number\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>I am in New York City and I am looking for a restaurant within 500 meters. Can you help me?<|end|><|start|>assistant to=functions.find_nearby_restaurants<|channel|>commentary json<|message|>{\"location\": \"New York City\", \"distance\": 500}<|call|><|start|>functions.find_nearby_restaurants to=assistant<|channel|>commentary<|message|>{\"restaurants\": [{\"name\": \"The Modern\", \"distance\": 300}, {\"name\": \"Le Bernardin\", \"distance\": 450}, {\"name\": \"Eleven Madison Park\", \"distance\": 500}]}<|end|><|start|>assistant<|channel|>analysis<|message|>Okay, the assistant\\'s response provided the nearby restaurants in NYC with their distances. The result shows three restaurants: The Modern (300m), Le Bernardin (450m), and Eleven Madison Park (500m). The user might want more details, so perhaps they could ask for more info like reviews or cuisine types. But since it\\'s just a simple request, the answer is sufficient. No further action is needed.<|end|><|start|>assistant<|channel|>final<|message|>The system found these restaurants within 500 meters of your location in New York City:\\n1. **The Modern** - 300 meters away\\n2. **Le Bernardin** - 450 meters away\\n3. **Eleven Madison Park** - 500 meters away\\n\\nWould you like additional details about any of these restaurants?<|return|>'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57dbb2b4-0565-48a2-8d4a-d00567d9296f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Search for a given keyword in the English translation of the Quran and return the verses containing the keyword and total matches in JSON format.\\ntype search_for_a_word_in_quran = (_: {\\n// The keyword or phrase to search for in the Quran.\\nsearchterm: string\\n}) => any;\\n\\n// Calculates the CO2 emissions in kilograms from the use of public transportation based on the distance traveled and type of transit.\\ntype carbonfootprintfrompublictransit = (_: {\\n// The distance traveled in kilometers.\\ndistance: string,\\n// The type of transportation, one of: \\'Taxi\\', \\'ClassicBus\\', \\'EcoBus\\', \\'Coach\\', \\'NationalTrain\\', \\'LightRail\\', \\'Subway\\', \\'FerryOnFoot\\', \\'FerryInCar\\'.\\ntype: string\\n}) => any;\\n\\n// Calculate the number of trees required to create a given weight of paper.\\ntype treeequivalent = (_: {\\n// The weight of the paper.\\nweight: string,\\n// The unit of measurement for the weight (e.g., \\'kg\\' or \\'lb\\').\\nunit: string\\n}) => any;\\n\\n// Fetches the entire Surah (chapter) of the Koran, including all verses and some additional information, based on the provided chapter ID.\\ntype get_entire_surah_chapter = (_: {\\n// The unique identifier for a Surah (chapter) in the Koran. Must be between 1 and 114, inclusive.\\nchapterid: number\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>Could you help me with two things? First, search the Quran for the term \\'kindness\\' and second, find out how many trees are needed to produce 600 lb of paper?<|end|><|start|>assistant to=functions.search_for_a_word_in_quran<|channel|>commentary json<|message|>{\"searchterm\": \"kindness\"}<|call|><|start|>assistant to=functions.treeequivalent<|channel|>commentary json<|message|>{\"weight\": \"600\", \"unit\": \"lb\"}<|call|>'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e8adcc85-3a4b-4396-bbe5-6b7deb364460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Fetches and returns a list of all available stock symbols from the real-time-quotes API.\\ntype stock_symbols = (_: {\\n// If True, includes stock names in the returned data. Defaults to None.\\nincludenames?: any, // default: \"false\"\\n}) => any;\\n\\n// Fetch the Total Value Locked (TVL) for a specified DeFi protocol from the RapidAPI Toolbench.\\ntype tvl_protocol = (_: {\\n// The slug identifier for the DeFi protocol.\\nprotocol?: any, // default: \"\"\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>I need the total value locked for MakerDAO.<|end|><|start|>assistant<|channel|>analysis<|message|>Okay, let me think about this. The user wants the total value locked (TVL) for MakerDAO. The tvl_protocol tool seems relevant here. The parameters for tvl_protocol require a specific protocol slug. I need to figure out the correct slug for MakerDAO. I remember that slugs are usually derived from the protocol\\'s name or a shortened version. MakerDAO is a well-known DeFi protocol, so maybe the slug is maker or makerdao. Let me check... Hmm, sometimes they might use the full name as the slug. Let me see if I can recall or infer the exact slug. Wait, I think the correct slug for MakerDAO is \"maker\" or maybe \"makerdao\". Let me check the available tools again. Since the parameters require the protocol as a string, I should use the most accurate value. I think the slug is \"maker\". So the function call would be tvl_protocol with protocol set to \"maker\".<|end|><|start|>assistant<|channel|>final<|message|>Apologies, but I\\'m unable to provide the Total Value Locked (TVL) for MakerDAO at this time. The `tvl_protocol` tool requires a specific protocol slug as input, which I don\\'t have. Without the correct slug identifier, I can\\'t fetch the requested data. Could you please provide the exact name or identifier for the MakerDAO protocol? Knowing the precise name or any other identifying details would help me retrieve the accurate TVL.<|return|>'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dc414830-4988-4821-95df-87eb1f017bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Fetch a list of hosts that ran an EcoIndex analysis.\\n\\n    Args:\\n        version (str): Engine version used for analysis (v0 or v1).\\n        q (str, optional): Filter for partial host name.\\n        date_from (str, optional): Start date for filtering (YYYY-MM-DD).\\n        size (int, optional): Number of elements per page. Defaults to 50.\\n        date_to (str, optional): End date for filtering (YYYY-MM-DD).\\n        page (int, optional): Page number to retrieve. Defaults to 1.\\n\\n    Returns:\\n        List of hosts ordered by number of requests made.\\n    \\ntype get_host_list_version_hosts_get = (_: {\\n// Engine version used to run the analysis (v0 or v1).\\nversion: string,\\n// Filter for partial host name. Defaults to None.\\nq?: string,\\n// Start date for filtering elements (format: YYYY-MM-DD). Defaults to None.\\ndate_from?: string,\\n// Number of elements per page. Defaults to 50.\\nsize?: number,\\n// End date for filtering elements (format: YYYY-MM-DD). Defaults to None.\\ndate_to?: string,\\n// Page number to retrieve. Defaults to 1.\\npage?: number\\n}) => any;\\n\\n// Fetch horoscope information for a given astrological sign.\\n\\n    Args:\\n        s (str): The astrological sign to fetch information for.\\n\\n    Returns:\\n        Horoscope information for the specified sign.\\n\\n    Raises:\\n        ValueError: If an invalid sign is provided.\\n    \\ntype sign = (_: {\\n// The astrological sign to fetch information for. Valid options include \\'aries\\', \\'taurus\\', \\'gemini\\', \\'cancer\\', \\'leo\\', \\'virgo\\', \\'libra\\',\\'scorpio\\',\\'sagittarius\\', \\'capricorn\\', \\'aquarius\\', and \\'pisces\\'.\\ns: string\\n}) => any;\\n\\n// Fetch values from a specified data subset within a dataset.\\n\\n    Args:\\n        subset (str): Identifier for the data subset.\\n        dataset (str): Identifier for the dataset.\\n        limit (int, optional): Limits output table lines. Defaults to 10. 0 means no limit.\\n        debug (int, optional): Debug status. Non-zero enables debug mode. Defaults to 0.\\n        format (str, optional): Output format. Defaults to \\'tsv\\'.\\n\\n    Returns:\\n        Values from the specified data subset.\\n    \\ntype tsv2 = (_: {\\n// Identifier for the data subset.\\nsubset: string,\\n// Identifier for the dataset.\\ndataset: string,\\n// Limits the number of lines in the output table. Defaults to 10. A value of 0 means no limit.\\nlimit?: number,\\n// Debug status. When set to a non-zero value, debug mode is enabled. Defaults to 0.\\ndebug?: number,\\n// Output format. Defaults to \\'tsv\\'.\\nformat?: string\\n}) => any;\\n\\n// Fetch order details given an order ID.\\n\\n    Args:\\n        orderid (int): The ID of the order to be fetched.\\n\\n    Returns:\\n        Order details for the specified ID.\\n\\n    Raises:\\n        Exception: For IDs > 5 and <= 10.\\n    \\ntype getorderbyid = (_: {\\n// The ID of the order to be fetched. For a valid response, use IDs with values <= 5 or > 10. Other values may generate exceptions.\\norderid: number\\n}) => any;\\n\\n// Fetch a list of quotes for a given genre.\\n\\n    Args:\\n        name (str): The name of the genre to fetch quotes for.\\n\\n    Returns:\\n        List of quotes for the specified genre.\\n    \\ntype get_quotes_by_genre = (_: {\\n// The name of the genre to fetch quotes for.\\nname: string\\n}) => any;\\n\\n// Fetch the list of recitations for a specific Ayah.\\n\\n    Args:\\n        recitation_id (int): The ID of the recitation.\\n        ayah_key (str): The key of the Ayah (e.g., \\'1:1\\' for first Ayah of first Surah).\\n\\n    Returns:\\n        List of recitations for the specified Ayah.\\n    \\ntype ayah_recitations = (_: {\\n// The ID of the recitation.\\nrecitation_id: number,\\n// The key of the Ayah, which is a combination of the Surah number and Ayah number (e.g., \\'1:1\\' for the first Ayah of the first Surah).\\nayah_key: string\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>What are today\\'s horoscope predictions for a person born under the sign of Virgo?<|end|><|start|>assistant to=functions.sign<|channel|>commentary json<|message|>{\"s\": \"virgo\"}<|call|><|start|>functions.sign to=assistant<|channel|>commentary<|message|>{\"tool_call_id\": \"call_X2BFdg8e0abUqscZyrg1VM1z\", \"name\": \"sign\", \"content\": \"{\\'sign\\': \\'Virgo\\', \\'date\\': \\'2024-06-28\\', \\'prediction\\': \\\\\"Today is a day of heightened intuition and analytical prowess for Virgo. Your ability to notice details others miss will be particularly strong, allowing you to solve complex problems with ease. In your professional life, your meticulous nature will be greatly appreciated - you may find yourself taking on a leadership role in an important project. Financially, it\\'s a good time to review your budget and make adjustments where necessary. In relationships, open and honest communication will strengthen your bonds. Take some time for self-care, perhaps through organizing your living space or indulging in a healthy, home-cooked meal.\\\\\", \\'lucky_number\\': 6,\\'mood\\': \\'Analytical\\', \\'compatibility\\': \\'Capricorn\\'}\"}<|end|><|start|>assistant<|channel|>final<|message|>Today\\'s Virgo horoscope (June 28, 2024):  \\n**Key Themes:**  \\n- Heightened intuition and attention to detail.  \\n- Leadership opportunities in projects requiring precision.  \\n- Financial review and budget adjustments.  \\n- Strengthened relationships through honest communication.  \\n\\nLucky number: 6  \\nMood: Analytical  \\nBest compatibility: Capricorn.  \\n\\nTake time today to appreciate your ability to solve complex problems and nurture connections. ğŸŒŸ<|end|><|start|>user<|message|>Can you provide the horoscope predictions for tomorrow for a person born under the sign of Virgo?\\n\\nPlease retrieve tomorrow\\'s prediction for Virgo using their zodiac sign name.<|end|><|start|>assistant to=functions.sign<|channel|>commentary json<|message|>{\"s\": \"virgo\", \"date\": \"2024-06-29\"}<|call|><|start|>functions.sign to=assistant<|channel|>commentary<|message|>{\"tool_call_id\": \"call_9dMxtk9NPuAXXi7zS2v4ismo\", \"name\": \"sign\", \"content\": \"{\\'sign\\': \\'Virgo\\', \\'date\\': \\'2024-06-29\\', \\'prediction\\': \\\\\"Tomorrow brings a day of heightened intuition and analytical prowess for Virgo. Your ability to notice subtle details will be particularly strong, allowing you to make significant progress on complex projects. In your professional life, your meticulous nature will be greatly appreciated - you may find yourself taking on a mentoring role or leading an important initiative. Financially, it\\'s an excellent time to review your long-term goals and make adjustments to your savings strategy. In relationships, clear and honest communication will be key to strengthening your bonds. Take some time for self-care, perhaps through organizing your living space or indulging in a healthy, home-cooked meal. Your practical approach combined with a touch of creativity will lead to innovative solutions.\\\\\", \\'lucky_number\\': 7,\\'mood\\': \\'Insightful\\', \\'compatibility\\': \\'Capricorn\\'}\"}<|end|><|start|>assistant<|channel|>analysis<|message|>The user wants tomorrow\\'s horoscope for Virgo. The tool returned a detailed prediction that includes the sign, date, prediction, lucky number, mood, and compatibility. The prediction talks about heightened intuition, leading projects requiring precision, mentoring roles, financial goals review, communication in relationships, self-care through organization or cooking, and a blend of analytical and creative approaches. I\\'ll summarize this in a concise, friendly manner to provide the user with the essential information they requested.<|end|><|start|>assistant<|channel|>final<|message|>Tomorrow\\'s Virgo horoscope (June 29, 2024):  \\n**Key Themes:**  \\n- Enhanced intuitive insight and attention to detail.  \\n- Leadership in mentoring or leading initiatives.  \\n- Review of long-term financial goals.  \\n- Strengthened relationships through clear communication.  \\n\\nLucky number: 7  \\nMood: Insightful  \\nBest compatibility: Capricorn.  \\n\\nTake time today to leverage your analytical skills while allowing creativity to inform your decisions. ğŸ§©ğŸ”®<|return|>'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d04cf661-31f4-41bc-9f42-e93e1f2b319b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-25\\n\\nReasoning: low\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\\nCalls to these tools must go to the commentary channel: \\'functions\\'.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\\n\\n# Tools\\n\\n## functions\\n\\nnamespace functions {\\n\\n// Fetches the follower list for a specified user from the TikTok API.\\ntype get_user_follower_list = (_: {\\n// The ID of the user whose followers are to be fetched.\\nuser_id: string,\\n// The number of followers to retrieve, with a maximum of 200. Defaults to \\'50\\'.\\ncount: string,\\n// A timestamp indicating the starting point for the followers list. If there are more followers to retrieve, this value should be updated. Defaults to \\'0\\'.\\ntime: string\\n}) => any;\\n\\n// Fetch the Instagram user ID associated with a given username.\\ntype get_user_id_by_username = (_: {\\n// The Instagram username for which the user ID needs to be retrieved.\\nusername: string\\n}) => any;\\n\\n// Fetch the challenge details from TikTok based on a provided hashtag.\\ntype challenge = (_: {\\n// The hashtag of the challenge to fetch details for.\\nhashtag: string\\n}) => any;\\n\\n} // namespace functions<|end|><|start|>user<|message|>Please get the follower count for TikTok user \\'987654321\\' and find the Instagram user ID for \\'anotheruser456\\'.<|end|><|start|>assistant to=functions.get_user_follower_list<|channel|>commentary json<|message|>{\"user_id\": \"987654321\", \"count\": \"50\", \"time\": \"0\"}<|call|><|start|>assistant to=functions.get_user_id_by_username<|channel|>commentary json<|message|>{\"username\": \"anotheruser456\"}<|call|>'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text[6]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03c179-b3e4-483b-b443-19398f690174",
   "metadata": {},
   "source": [
    "## ã€å¾®è°ƒå®æˆ˜äºŒã€‘å¾®è°ƒGPT-OSSæ¨¡å‹Agentæ€§èƒ½å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6421e-8ce6-4cec-8cf3-7646116ad2f4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†å¾®è°ƒæ•°æ®é›†ä¹‹åï¼Œå®é™…çš„å¾®è°ƒè¿‡ç¨‹å¹¶ä¸ä¼šå¤ªå¤æ‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772a5e4-4890-402c-9411-5830ac7f3a42",
   "metadata": {},
   "source": [
    "- **Step 1.è¯»å–æ•°æ®é›†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87eb435a-70ed-4a9a-9ec5-2f2da54b3b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages_json': '[{\"role\": \"developer\", \"content\": \"You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\"}, {\"role\": \"user\", \"content\": \"I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"function\": {\"name\": \"is_hotel_available\", \"arguments\": {\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\"}}}], \"thinking\": \"Okay, the user wants to check the availability of the Queens Hotel in Berlin from March 15th to March 20th, 2022. Let me start by recalling the available tools. The only tool provided here is the hotel availability checker, which is called \\\\\"is_hotel_available\\\\\". \\\\n\\\\nSo, I need to construct the proper function call for that tool. The parameters required are hotel, city, checkin, and checkout. The user mentioned the hotel is Queens Hotel, city is Berlin, check-in is March 15, 2022 (which should be formatted as YYYY-MM-DD, so 2022-03-15), and check-out is March 20, 2022 (2022-03-20). \\\\n\\\\nI\\'ll call the is_hotel_available function with these parameters and then present the response. The Python code will need to use the function name and pass the dictionary with the required details. Once the function is called, the result will be in JSON format, which I should parse to show the availability information.\"}, {\"role\": \"tool\", \"content\": \"{\\\\\"name\\\\\": \\\\\"is_hotel_available\\\\\", \\\\\"content\\\\\": {\\\\\"hotel\\\\\": \\\\\"Queens Hotel\\\\\", \\\\\"city\\\\\": \\\\\"Berlin, Germany\\\\\", \\\\\"checkin\\\\\": \\\\\"2022-03-15\\\\\", \\\\\"checkout\\\\\": \\\\\"2022-03-20\\\\\", \\\\\"availability\\\\\": \\\\\"Available\\\\\", \\\\\"rooms_left\\\\\": 5, \\\\\"price_per_night\\\\\": 189, \\\\\"total_price\\\\\": 945, \\\\\"cancellation_policy\\\\\": \\\\\"Flexible until 5 days before arrival\\\\\", \\\\\"special_offers\\\\\": []}}\"}]', 'tools_json': '[{\"type\": \"function\", \"function\": {\"name\": \"is_hotel_available\", \"description\": \"Checks the availability of a hotel for a given date range.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"hotel\": {\"type\": \"string\", \"description\": \"The name of the hotel.\"}, \"city\": {\"type\": \"string\", \"description\": \"The city where the hotel is located.\"}, \"checkin\": {\"type\": \"string\", \"description\": \"The check-in date in the format \\\\\"YYYY-MM-DD\\\\\".\"}, \"checkout\": {\"type\": \"string\", \"description\": \"The check-out date in the format \\\\\"YYYY-MM-DD\\\\\".\"}}, \"required\": [\"hotel\", \"city\", \"checkin\", \"checkout\"]}}}]'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"hermes_200_gptoss.jsonl\", split=\"train\")\n",
    "print(dataset[0])   # æŸ¥çœ‹ç¬¬ä¸€æ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b8336-696b-4759-ac97-839471d0f60a",
   "metadata": {},
   "source": [
    "- **Step 2.æ•°æ®æ ¼å¼è½¬åŒ–**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6c13a-5bbc-4b06-a393-e33cbf37fd18",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æœ‰äº†æ•°æ®é›†ä¹‹åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å°†æ•°æ®é›†è½¬åŒ–ä¸ºgpt-ossèƒ½å¤Ÿè¯†åˆ«çš„æ•°æ®æ ¼å¼ï¼Œé¦–å…ˆç¬¬ä¸€æ­¥æ˜¯éœ€è¦å°†å…¶è½¬åŒ–ä¸ºgpt-ossçš„åŸºæœ¬æ¶ˆæ¯æ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59d40548-2840-4c0d-ac11-59802a914f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_text = dataset.map(render_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e92f8e-2579-43f5-b0ad-bc7398609274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages_json', 'tools_json', 'text'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb8b4d-4f86-4991-91c1-05adba63720b",
   "metadata": {},
   "source": [
    "å…¶ä¸­'text'å­—æ®µä¸­å°±ä¿å­˜ç€å¯ä»¥ç”¨äºå¾®è°ƒçš„å®Œæ•´æ–‡æœ¬ç»“æ„ï¼Œå…¶ä¸­assistantä¹‹å‰æ˜¯è¾“å…¥ï¼Œä¹‹åæ˜¯è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa315bc-edcf-41fb-909d-d4570c2fee50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-25\n",
      "\n",
      "Reasoning: low\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering.\n",
      "\n",
      "# Tools\n",
      "\n",
      "## functions\n",
      "\n",
      "namespace functions {\n",
      "\n",
      "// Checks the availability of a hotel for a given date range.\n",
      "type is_hotel_available = (_: {\n",
      "// The name of the hotel.\n",
      "hotel: string,\n",
      "// The city where the hotel is located.\n",
      "city: string,\n",
      "// The check-in date in the format \"YYYY-MM-DD\".\n",
      "checkin: string,\n",
      "// The check-out date in the format \"YYYY-MM-DD\".\n",
      "checkout: string\n",
      "}) => any;\n",
      "\n",
      "} // namespace functions<|end|><|start|>user<|message|>I want to book a hotel in Berlin from 3/15/2022 to 3/20/2022. Is the Queens Hotel available?<|end|><|start|>assistant to=functions.is_hotel_available<|channel|>commentary json<|message|>{\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\"}<|call|><|start|>functions.is_hotel_available to=assistant<|channel|>commentary<|message|>{\"name\": \"is_hotel_available\", \"content\": {\"hotel\": \"Queens Hotel\", \"city\": \"Berlin, Germany\", \"checkin\": \"2022-03-15\", \"checkout\": \"2022-03-20\", \"availability\": \"Available\", \"rooms_left\": 5, \"price_per_night\": 189, \"total_price\": 945, \"cancellation_policy\": \"Flexible until 5 days before arrival\", \"special_offers\": []}}<|end|>\n"
     ]
    }
   ],
   "source": [
    "print(ds_text[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc30b0a-32ce-4276-95ac-a9d81a209fb1",
   "metadata": {},
   "source": [
    "- **Step 3.LoRAå‚æ•°çŒæ³¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbf10b63-5c3f-4b30-96c7-c2ffb141c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94dd724-0664-45ca-bb58-6451d73f1517",
   "metadata": {},
   "source": [
    "åŸºæœ¬å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4bb46-2713-4677-bc41-d915e8856eec",
   "metadata": {},
   "source": [
    "| å‚æ•°                               | å«ä¹‰                   | æ¨èå€¼/å½“å‰è®¾ç½®                        | ä½œç”¨ä¸è¯´æ˜                                                     |\n",
    "| -------------------------------- | -------------------- | ------------------------------- | --------------------------------------------------------- |\n",
    "| **r**                            | LoRA ç§©ï¼ˆrankï¼‰         | 8ï¼ˆå¯é€‰ 8/16/32/64/128ï¼‰            | æ§åˆ¶æ–°å¢å‚æ•°è§„æ¨¡ï¼›å€¼è¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿéšä¹‹ä¸Šå‡ã€‚ä¸­æ–‡æ¨ç†é“¾æ¨èä» 8/16 èµ·æ­¥ã€‚           |\n",
    "| **target\\_modules**              | æ³¨å…¥çš„çº¿æ€§å±‚               | q/k/v/o æŠ•å½± + MLP (gate/up/down) | æŒ‡å®šåœ¨å“ªäº›å±‚æŒ‚ LoRAã€‚è¦†ç›–æ³¨æ„åŠ› + MLP â†’ æ›´å¥½é€‚é…ä¸­æ–‡å¥æ³•ä¸é€»è¾‘ï¼›è‹¥æ˜¾å­˜ä¸è¶³å¯åªä¿ç•™ q/k/v/oã€‚ |\n",
    "| **lora\\_alpha**                  | LoRA ç¼©æ”¾ç³»æ•°            | 16                              | æ”¾å¤§ LoRA åˆ†æ”¯è¾“å‡ºï¼Œç­‰æ•ˆè°ƒæ•´å­¦ä¹ ç‡ã€‚å¸¸ç”¨èŒƒå›´ 16â€“32ï¼Œr å°æ—¶å¯é€‚å½“è°ƒé«˜ã€‚                |\n",
    "| **lora\\_dropout**                | LoRA dropout æ¦‚ç‡      | 0                               | æ§åˆ¶ LoRA åˆ†æ”¯çš„éšæœºä¸¢å¼ƒç‡ã€‚0 åœ¨ Unsloth ä¸­ä¼˜åŒ–è¿‡ï¼Œæ›´å¿«æ›´çœï¼›è‹¥è¿‡æ‹Ÿåˆå¯å°è¯• 0.05â€“0.1ã€‚  |\n",
    "| **bias**                         | Bias å‚æ•°è®­ç»ƒç­–ç•¥          | \"none\"                          | æ˜¯å¦å¯¹æ³¨å…¥å±‚ bias å¯è®­ç»ƒã€‚\"none\" å¼€é”€æœ€å°ä¸”ä¼˜åŒ–æœ€å¥½ã€‚                         |\n",
    "| **use\\_gradient\\_checkpointing** | æ¢¯åº¦æ£€æŸ¥ç‚¹                | \"unsloth\"                       | èŠ‚çœ \\~30% æ˜¾å­˜ï¼Œæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡å’Œæ›´å¤§ batchï¼›æ¨èå¼€å¯ã€‚                        |\n",
    "| **random\\_state**                | éšæœºç§å­                 | 3407                            | ä¿è¯å®éªŒå¯å¤ç°ï¼›ä»»æ„å›ºå®šå€¼å‡å¯ã€‚                                          |\n",
    "| **use\\_rslora**                  | Rank-Stabilized LoRA | False                           | åœ¨å¤§ rï¼ˆâ‰¥64ï¼‰æ—¶å¯æé«˜ç¨³å®šæ€§ï¼›ä¸€èˆ¬åœºæ™¯ä¸å¿…å¯ç”¨ã€‚                                |\n",
    "| **loftq\\_config**                | LoftQ é‡åŒ–é…ç½®           | None                            | ç»“åˆé‡åŒ–è®­ç»ƒ LoRAï¼Œåœ¨ 4/8bit æ¨ç†æ—¶æ›´ç¨³ã€‚é»˜è®¤å…³é—­ï¼›è‹¥ç›®æ ‡æ˜¯ä½æ¯”ç‰¹éƒ¨ç½²ï¼Œå¯é…ç½®å¯ç”¨ã€‚          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824da34f-0a1a-4ac7-bb5f-edeac69a76d3",
   "metadata": {},
   "source": [
    "- **Step 4.è®¾ç½®å¾®è°ƒå‚æ•°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90d812dd-7de5-4c87-a842-f03697abbd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages_json', 'tools_json', 'text'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5343d94-8e02-4b5e-a11f-88ff41f9b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 128.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds_text,\n",
    "    args = SFTConfig(\n",
    "        max_seq_length = 4096,\n",
    "        dataset_text_field = \"text\",   # ä½ ä¸Šä¸€æ­¥ map ç”Ÿæˆäº† text åˆ—\n",
    "        packing = False,               # ä¸€èˆ¬å…ˆå…³æ‰ packingï¼Œé¿å…æ‹¼æ¥æ ·æœ¬\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to=\"tensorboard\",     # ğŸ“Œ å¯ç”¨ TensorBoard\n",
    "        logging_dir=\"./logs\",        # æ—¥å¿—ç›®å½•\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1,            # è®°å½•é¢‘ç‡\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db3c22-d6b7-407f-ba11-b3e5b0363dcd",
   "metadata": {},
   "source": [
    "- **Step 5.å¼€å§‹å¾®è°ƒ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c17b5c15-963e-4de9-bb03-a7fb908329e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 200 | Num Epochs = 1 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 3,981,312 of 20,918,738,496 (0.02% trained)\n",
      "/root/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_inductor/runtime/triton_helpers.py:150: UserWarning: Logical operators 'and' and 'or' are deprecated for non-scalar tensors; please use '&' or '|' instead\n",
      "  mask |= a_isnan and not b_isnan\n",
      "UserWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/root/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_inductor/runtime/triton_helpers.py:150: UserWarning: Logical operators 'and' and 'or' are deprecated for non-scalar tensors; please use '&' or '|' instead\n",
      "  mask |= a_isnan and not b_isnan\n",
      "UserWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/root/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_inductor/runtime/triton_helpers.py:152: UserWarning: Logical operators 'and' and 'or' are deprecated for non-scalar tensors; please use '&' or '|' instead\n",
      "  equal |= a_isnan and b_isnan\n",
      "UserWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/root/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_inductor/runtime/triton_helpers.py:152: UserWarning: Logical operators 'and' and 'or' are deprecated for non-scalar tensors; please use '&' or '|' instead\n",
      "  equal |= a_isnan and b_isnan\n",
      "UserWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.769000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61e22e-ee4a-4a07-aee1-59e6c6552f1a",
   "metadata": {},
   "source": [
    "- **Step 7.å¾®è°ƒåæ¨¡å‹å¯¼å‡º**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384f25b-46a4-4b16-9de2-81c15fe22153",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å°†æ¨¡å‹åˆå¹¶å¯¼å‡ºï¼Œä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6884067d-aef8-4459-a81e-6b9ee006befd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GptOssForCausalLM(\n",
       "      (model): GptOssModel(\n",
       "        (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x GptOssDecoderLayer(\n",
       "            (self_attn): GptOssAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2880, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2880, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2880, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2880, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2880, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2880, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): GptOssMLP(\n",
       "              (router): GptOssTopKRouter()\n",
       "              (experts): GptOssExperts()\n",
       "            )\n",
       "            (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "            (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (rotary_emb): GptOssRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b48aaee-0100-4f83-b7c4-33ea3c24c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected local model directory: ./gpt-oss-20b-bf16\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Copying safetensors from local directory: ./gpt-oss-20b-bf16\n",
      "Copied safetensors index file from local model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00008-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  11%|â–ˆ         | 1/9 [00:10<01:20, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00004-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  22%|â–ˆâ–ˆâ–       | 2/9 [00:19<01:06,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00005-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [00:28<00:56,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00003-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:37<00:46,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00001-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:46<00:37,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00007-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:56<00:28,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00002-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:06<00:18,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00006-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:15<00:09,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00009-of-00009.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:19<00:00,  8.88s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(save_directory = \"GPT-OSS-finetuned\", \n",
    "                             tokenizer = tokenizer, \n",
    "                             save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb626a-4e8e-4e7f-a0fe-38112ab7412a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250825204654903.png\" alt=\"image-20250825204654903\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98a4c8-7048-487f-928b-a9ec64e44e34",
   "metadata": {},
   "source": [
    "- **Step 8.å¾®è°ƒæ•ˆæœå¯¹æ¯”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18ccfa-2c1b-4c72-8905-931d02e225b0",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥å³å¯ä½¿ç”¨vllmå¯¹å…¶è¿›è¡Œè°ƒç”¨ï¼Œå¹¶å€ŸåŠ©EvalScopeè¿›è¡Œæµ‹è¯•ã€‚éœ€è¦å€ŸåŠ©vLLMè°ƒç”¨å¯¼å‡ºåçš„æ¨¡å‹ï¼Œç„¶åå€ŸåŠ©evalscopeè¿›è¡Œæµ‹è¯•ã€‚\n",
    "\n",
    "```python\n",
    "from evalscope import TaskConfig, run_task\n",
    "task_cfg = TaskConfig(\n",
    "    model='./GPT-OSS-finetuned',\n",
    "    api_url='http://127.0.0.1:8000/v1/chat/completions',\n",
    "    eval_type='service',\n",
    "    datasets=[\n",
    "        'data_collection',\n",
    "    ],\n",
    "    dataset_args={\n",
    "        'data_collection': {\n",
    "            'dataset_id': 'modelscope/EvalScope-Qwen3-Test',\n",
    "            'filters': {'remove_until': '</think>'}  # è¿‡æ»¤æ‰æ€è€ƒçš„å†…å®¹\n",
    "        }\n",
    "    },\n",
    "    eval_batch_size=128,\n",
    "    generation_config={\n",
    "        'max_tokens': 30000,  # æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œå»ºè®®è®¾ç½®ä¸ºè¾ƒå¤§å€¼é¿å…è¾“å‡ºæˆªæ–­\n",
    "        'temperature': 0.6,  # é‡‡æ ·æ¸©åº¦ (qwen æŠ¥å‘Šæ¨èå€¼)\n",
    "        'top_p': 0.95,  # top-pé‡‡æ · (qwen æŠ¥å‘Šæ¨èå€¼)\n",
    "        'top_k': 20,  # top-ké‡‡æ · (qwen æŠ¥å‘Šæ¨èå€¼)\n",
    "        'n': 1,  # æ¯ä¸ªè¯·æ±‚äº§ç”Ÿçš„å›å¤æ•°é‡\n",
    "    },\n",
    "    timeout=60000,  # è¶…æ—¶æ—¶é—´\n",
    "    stream=True,  # æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º\n",
    "    limit=2000,  # è®¾ç½®ä¸º2000æ¡æ•°æ®è¿›è¡Œæµ‹è¯•\n",
    ")\n",
    "\n",
    "run_task(task_cfg=task_cfg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f161e-65d8-4451-8e64-13ef18c38124",
   "metadata": {},
   "source": [
    "æµ‹è¯•åå³å¯åœ¨å‰ç«¯ä¸­è§‚å¯Ÿæµ‹è¯„æŠ¥å‘Šï¼š\n",
    "```bash\n",
    "evalscope app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1db8c-5d5d-434e-a664-37bc4091450a",
   "metadata": {},
   "source": [
    "åŸå§‹æ¨¡å‹è¯„åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcd301-e01a-47be-b957-f978e0633fe7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/dc35abae0cad62215355b8640f2681e0.png\" alt=\"dc35abae0cad62215355b8640f2681e0\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d5ad4-295a-4fd3-b4db-50ec9156b9df",
   "metadata": {},
   "source": [
    "å¾®è°ƒåæ¨¡å‹è¯„åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6927-2ed5-48c1-8173-46bdbd2cdb71",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/403e787b08e9472d59915854d436f480.png\" alt=\"403e787b08e9472d59915854d436f480\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53eee6f-c4ae-4fd4-ad7d-412c89ef61a8",
   "metadata": {},
   "source": [
    "ä¸¤ä¸ªæ¨¡å‹å¯¹æ¯”ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ca705-3b46-474a-902b-cd93f11eed58",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/acfc8a3144f201ab63cad576a73ec7d0.png\" alt=\"acfc8a3144f201ab63cad576a73ec7d0\" style=\"zoom:50%;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
