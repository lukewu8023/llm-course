{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe98a9e5-4646-47d0-9445-19bf22ef3a54",
   "metadata": {},
   "source": [
    "# <center> Ch 8 PEFT&åŒ»ç–—é¢†åŸŸæ¨¡å‹å¾®è°ƒå®è·µ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed73cda",
   "metadata": {},
   "source": [
    "# 1. é¡¹ç›®èƒŒæ™¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a830f",
   "metadata": {},
   "source": [
    "ä¸­åŒ»æ˜¯ä¸­ååŒ»ç–—é¢†åŸŸçš„å®è´µè´¢å¯Œï¼Œæœ‰ç€å‡ åƒå¹´çš„å†å²ï¼Œæ²»ç—…æ•‘äººä¸€ç›´å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ä½†è¿›å…¥ç°ä»£ç¤¾ä¼šï¼Œä¸­åŒ»çš„å‘å±•é‡åˆ°äº†ä¸€äº›å®é™…å›°éš¾ï¼š\n",
    "\n",
    "1. **å­¦ä¹ éš¾åº¦å¤§**ï¼š  \n",
    "   ä¸­åŒ»çš„ç»å…¸ä¹¦ç±ï¼Œæ¯”å¦‚ã€Šé»„å¸å†…ç»ã€‹ã€Šä¼¤å¯’æ‚ç—…è®ºã€‹ï¼Œå†…å®¹æ·±å¥¥ï¼Œå¾ˆå¤šçŸ¥è¯†æ˜¯é ç»éªŒæ€»ç»“å‡ºæ¥çš„ï¼Œæ²¡æœ‰ç°ä»£åŒ»å­¦é‚£ä¹ˆç³»ç»Ÿã€‚å¹´è½»äººå­¦èµ·æ¥è§‰å¾—å¾ˆåƒåŠ›ï¼Œå¾ˆå¤šå®è´µçš„çŸ¥è¯†æ²¡èƒ½è¢«å¾ˆå¥½åœ°ä¼ æ‰¿ä¸‹æ¥ã€‚\n",
    "\n",
    "2. **æ•°å­—åŒ–ä¸è¶³**ï¼š  \n",
    "   å¾ˆå¤šåœ°æ–¹çš„ä¸­åŒ»åŒ»æ¡ˆå’Œå¤„æ–¹è¿˜åœç•™åœ¨çº¸è´¨æˆ–è€…æ‰‹å†™è®°å½•çš„é˜¶æ®µï¼Œè¿™äº›å®è´µçš„å†…å®¹å¾ˆéš¾è¢«æ•´ç†å’Œåˆ†æã€‚ç›¸è¾ƒäºè¥¿åŒ»çš„æ•°å­—åŒ–è¿›ç¨‹ï¼Œä¸­åŒ»åœ¨æ•°æ®æŒ–æ˜å’Œåˆ©ç”¨ä¸Šè¿˜æ¯”è¾ƒè½åã€‚\n",
    "\n",
    "3. **å®ç”¨æ€§å—é™**ï¼š  \n",
    "   ç°åœ¨å¾ˆå¤šäººå¯¹ä¸­åŒ»å¹¶ä¸äº†è§£ï¼Œè§‰å¾—ä¸­åŒ»æ˜¯â€œç„å­¦â€æˆ–è€…â€œå…»ç”Ÿæ–¹æ³•â€ï¼Œå¯¹å®ƒçš„å®é™…ä½œç”¨æœ‰è¯¯è§£ã€‚ä¸­åŒ»çš„çŸ¥è¯†å¦‚æœèƒ½æ›´è´´è¿‘æ™®é€šäººçš„éœ€æ±‚ï¼Œå¯èƒ½ä¼šè®©æ›´å¤šäººå—ç›Šã€‚\n",
    "\n",
    "é€šè¿‡ç°åœ¨çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ¯”å¦‚å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è¯•ç€å°†è¿™äº›å›°éš¾è½¬åŒ–ä¸ºæœºä¼šï¼Œè®©ä¸­åŒ»çŸ¥è¯†æ›´åŠ æ¸…æ™°ã€æ˜“æ‡‚ï¼Œä¹Ÿè®©å®ƒçš„åº”ç”¨æ›´åŠ æ™®åŠå’Œæ™ºèƒ½åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98a88d",
   "metadata": {},
   "source": [
    "## 1.1 é¡¹ç›®æ„ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846ac96",
   "metadata": {},
   "source": [
    "#### 1. å¸®åŠ©ä¸­åŒ»æ›´å¥½åœ°ä¼ æ‰¿\n",
    "ä¸­åŒ»ç†è®ºå¤æ‚ï¼Œç»å…¸æ–‡çŒ®åˆå¾ˆéš¾æ‡‚ï¼Œè€Œé€šè¿‡å¾®è°ƒä¸€ä¸ªä¸­åŒ»å¤§æ¨¡å‹ï¼Œå¯ä»¥æŠŠè¿™äº›æ™¦æ¶©çš„çŸ¥è¯†ç”¨ç®€å•çš„è¯­è¨€è¡¨è¾¾å‡ºæ¥ï¼Œå¸®åŠ©å­¦ç”Ÿå’Œæ™®é€šäººç†è§£ï¼Œæ¯”å¦‚ï¼š\n",
    "- â€œè„¾è™šâ€åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ\n",
    "- â€œæ¹¿çƒ­â€å’Œâ€œå¯’æ¹¿â€æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ  \n",
    "\n",
    "è¿™ç§æ–¹å¼å¯ä»¥é™ä½ä¸­åŒ»çŸ¥è¯†çš„å­¦ä¹ é—¨æ§›ï¼Œå°¤å…¶æ˜¯å¸®åŠ©å¹´è½»ä¸€ä»£æ›´å¥½åœ°æ¥è§¦å’ŒæŒæ¡ä¸­åŒ»ã€‚\n",
    "\n",
    "#### 2. ç»™åŒ»ç”Ÿæä¾›å·¥å…·\n",
    "åŒ»ç”Ÿçœ‹ç—…éœ€è¦ç»“åˆç—…äººçš„å„ç§ç—‡çŠ¶æ¥åˆ¤æ–­ç—…å› ã€å¼€æ–¹è°ƒç†ã€‚å¾®è°ƒåçš„æ¨¡å‹å¯ä»¥ä½œä¸ºåŒ»ç”Ÿçš„å·¥å…·ï¼Œæ¯”å¦‚ï¼š\n",
    "- æ•´ç†å’Œæ€»ç»“å†å²åŒ»æ¡ˆï¼Œè®©åŒ»ç”Ÿæ›´å®¹æ˜“å‚è€ƒç±»ä¼¼çš„ç—…æƒ…å¤„ç†æ–¹æ¡ˆã€‚\n",
    "- æä¾›åˆæ­¥çš„è¯Šæ–­å»ºè®®ï¼Œæ¯”å¦‚ç—…äººæè¿°â€œèˆŒè‹”é»„è…»ã€å£è‹¦â€ï¼Œæ¨¡å‹å¯ä»¥æç¤ºå¯èƒ½æ˜¯æ¹¿çƒ­ç—‡ã€‚\n",
    "\n",
    "è™½ç„¶å®ƒä¸èƒ½æ›¿ä»£åŒ»ç”Ÿï¼Œä½†å¯ä»¥å‡å°‘åŒ»ç”Ÿå¤„ç†å¤§é‡ä¿¡æ¯çš„è´Ÿæ‹…ï¼Œæé«˜æ•ˆç‡ã€‚\n",
    "\n",
    "#### 3. æ™®é€šäººçš„å¥åº·å¸®æ‰‹\n",
    "å¾ˆå¤šäººå¹³æ—¶æœ‰å°ç—…å°ç—›æˆ–è€…å…»ç”Ÿéœ€æ±‚ï¼Œä½†åˆä¸æƒ³æ¯æ¬¡éƒ½å»åŒ»é™¢ã€‚å¦‚æœæœ‰ä¸€ä¸ªä¸­åŒ»æ¨¡å‹ï¼Œæ™®é€šäººå¯ä»¥éšæ—¶æé—®ï¼Œæ¯”å¦‚ï¼š\n",
    "- â€œæˆ‘æœ€è¿‘è€æ˜¯ç–²æƒ«æ— åŠ›ï¼Œæ˜¯ä¸æ˜¯æ°”è™šï¼Ÿè¯¥æ€ä¹ˆè°ƒç†ï¼Ÿâ€  \n",
    "- â€œå¤å¤©æ¹¿æ°”é‡ï¼Œæœ‰ä»€ä¹ˆç®€å•çš„æ–¹æ³•æ’æ¹¿ï¼Ÿâ€  \n",
    "\n",
    "æ¨¡å‹å¯ä»¥æ ¹æ®ä¸­åŒ»çš„ç†è®ºï¼Œæä¾›ä¸€äº›åŸºç¡€çš„è°ƒç†å»ºè®®ï¼Œå¸®åŠ©å¤§å®¶æ›´å¥½åœ°å…³æ³¨è‡ªå·±çš„èº«ä½“çŠ¶æ€ã€‚\n",
    "\n",
    "#### 4. ä¸ºç§‘ç ”æä¾›æ”¯æŒ\n",
    "é€šè¿‡æ¨¡å‹å¯¹ä¸­åŒ»åŒ»æ¡ˆå’Œæ•°æ®çš„åˆ†æï¼Œå¯ä»¥å‘ç°ä¸€äº›æ½œåœ¨çš„è§„å¾‹ï¼Œæ¯”å¦‚ï¼š\n",
    "- å“ªäº›è¯æç»„åˆåœ¨æŸäº›ç–¾ç—…ä¸­æ•ˆæœæ›´å¥½ï¼Ÿ\n",
    "- æŸç§ç—…ç—‡çš„æ²»ç–—æœ‰æ²¡æœ‰æ–°çš„çªç ´ç‚¹ï¼Ÿ  \n",
    "\n",
    "è¿™äº›åˆ†æå¯ä»¥å¸®åŠ©ç§‘ç ”äººå‘˜æ›´å¿«æ‰¾åˆ°æœ‰ä»·å€¼çš„ç ”ç©¶æ–¹å‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65686085",
   "metadata": {},
   "source": [
    "## 1.2 é¡¹ç›®å‰æ™¯\n",
    "\n",
    "#### **1. åŒ»ç–—è¾…åŠ©**\n",
    "å¾®è°ƒåçš„ä¸­åŒ»æ¨¡å‹å¯ä»¥æˆä¸ºåŒ»ç”Ÿçš„â€œåŠ©æ‰‹â€ï¼Œå¸®åŠ©åŒ»ç”Ÿç®¡ç†åŒ»æ¡ˆã€æ•´ç†æ€è·¯ã€æå‡è¯Šç–—æ•ˆç‡ã€‚å°¤å…¶æ˜¯åœ¨åŸºå±‚åŒ»ç–—èµ„æºä¸è¶³çš„åœ°æ–¹ï¼Œæ¨¡å‹çš„åº”ç”¨å¯èƒ½ä¼šæ›´åŠ å®ç”¨ã€‚\n",
    "\n",
    "#### **2. å¥åº·ç®¡ç†**\n",
    "å¯¹äºæ™®é€šäººæ¥è¯´ï¼Œæ¨¡å‹å¯ä»¥æä¾›æ—¥å¸¸å…»ç”Ÿå»ºè®®å’Œç®€å•çš„å¥åº·è°ƒç†æ–¹æ¡ˆï¼Œè®©å¤§å®¶æ›´å®¹æ˜“ç†è§£ä¸­åŒ»é‡Œçš„â€œæ²»æœªç—…â€ç†å¿µã€‚\n",
    "\n",
    "#### **3. æ•°æ®æ•´ç†å’Œåˆ†æ**\n",
    "ä¸­åŒ»çš„å†å²æ•°æ®å¾ˆå¤šï¼Œä½†åˆ©ç”¨ç‡ä¸é«˜ã€‚é€šè¿‡æ¨¡å‹çš„è®­ç»ƒå’ŒæŒ–æ˜ï¼Œå¯ä»¥æŠŠè¿™äº›æ•°æ®å˜æˆæ›´æœ‰ä»·å€¼çš„å†…å®¹ï¼Œæ¯”å¦‚æ¨èå“ªäº›è¯æ–¹æ›´é€‚åˆç‰¹å®šçš„ç—…ç—‡ã€‚\n",
    "\n",
    "#### **4. æ¨åŠ¨ä¸­åŒ»ç°ä»£åŒ–**\n",
    "ä¸­åŒ»çš„ç†è®ºå¾ˆå¤šæ—¶å€™éš¾ä»¥ç”¨ç°ä»£åŒ»å­¦è¯­è¨€è§£é‡Šæ¸…æ¥šï¼Œæ¨¡å‹å¯ä»¥ä½œä¸ºä¸€ä¸ªæ¡¥æ¢ï¼Œå¸®åŠ©ä¸­åŒ»å’Œç°ä»£åŒ»å­¦æ›´å¥½åœ°ç»“åˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e753e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241226115631471.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad89a00",
   "metadata": {},
   "source": [
    "# 2. æ¨¡å‹éªŒè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ec00b",
   "metadata": {},
   "source": [
    "æ¨¡å‹ä¸‹è½½å·²ç»å¤šæ¬¡è®²è§£ï¼Œæœ¬æ¬¡ä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eaf083",
   "metadata": {},
   "source": [
    "å½“å‰å¸‚é¢æ¨¡å‹è¾ƒå¤šï¼Œå¹¶ä¸”å¯åŠ¨æ–¹å¼ä¹Ÿä¸å®Œå…¨ç›¸åŒï¼Œä»¥ä¸‹ç®€å•å±•ç¤ºå‡ ç§ä¾›å¤§å®¶å‚è€ƒï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c866ff7",
   "metadata": {},
   "source": [
    "## 2.1 å¤šå¡éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6ce21-2a03-49e0-b8a5-4ec70abfaf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_tokenï¼š<|endoftext|>\n",
      "tokenizer.eos_tokenï¼š<|im_end|>\n",
      "åŸå§‹è¾“å…¥é•¿åº¦ï¼š5\n",
      "å¡«å……åçš„é•¿åº¦ï¼š5\n",
      "å¡«å……çš„å ä½ç¬¦æ•°é‡ï¼š0\n",
      "========================================================================================================================================================================================================\n",
      "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼\n",
      "æˆ‘æ˜¯æ¥è‡ªé˜¿é‡Œäº‘çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿï¼ˆ10åˆ†ï¼‰ æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€å›¾åƒè¯†åˆ«ç­‰æŠ€æœ¯\n",
      "tensor([[108386,   3837, 105043, 100165,   6313,    198, 104198, 101919, 102661,\n",
      "          99718, 104197, 100176, 102064, 104949,   3837,  35946,  99882,  31935,\n",
      "          64559,  99320,  56007,   1773, 104139, 109944, 100364, 103929, 101037,\n",
      "          11319,   9909,     16,     15,  17177,   7552,   6567,    115,    109,\n",
      "          26381, 100134,   5373,  99795, 102064,  54542,   5373, 102182, 105395,\n",
      "           5373, 107553, 102450,  49567,  99361]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# å¯¼å…¥PyTorchåº“ï¼ŒPyTorchæ˜¯ä¸€ä¸ªç”¨äºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ã€‚å®ƒæä¾›äº†å¾ˆå¤šå‡½æ•°å’Œå·¥å…·æ¥æ„å»ºã€è®­ç»ƒå’Œæ¨ç†ç¥ç»ç½‘ç»œã€‚\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# AutoModelForCausalLM æ˜¯ transformers åº“ä¸­ç”¨äºåŠ è½½å’Œä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTã€BERTã€LLaMAç­‰ï¼‰çš„å·¥å…·ã€‚CausalLM è¡¨ç¤ºå› æœè¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šå¸¸ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œæ¯”å¦‚è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "# AutoTokenizer ç”¨äºåŠ è½½åˆ†è¯å™¨ï¼ˆtokenizerï¼‰ã€‚åˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—æ ¼å¼ï¼ˆå³tokenåŒ–ï¼‰ï¼Œå¹¶å¯ä»¥å°†æ¨¡å‹çš„è¾“å‡ºæ•°å­—è½¬æ¢å›æ–‡æœ¬ã€‚\n",
    "import os\n",
    "\n",
    "# è®¾ç½®ä½¿ç”¨çš„æ˜¾å¡ï¼ˆ0, 1, 2è¡¨ç¤ºä½¿ç”¨ç¬¬1ã€2å’Œ3å·æ˜¾å¡ï¼‰\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "\n",
    "# æ˜¯ä½ è¦åŠ è½½çš„æ¨¡å‹çš„è·¯å¾„ï¼Œæˆ–è€…æ˜¯æ¨¡å‹çš„åç§°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æŒ‡å®šäº†ä¸€ä¸ªæœ¬åœ°è·¯å¾„ï¼ŒæŒ‡å‘ä¸€ä¸ªQwenæ¨¡å‹ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…¶ä»–æ¨¡å‹ï¼Œæ¯”å¦‚LLaMAæ¨¡å‹ï¼Œä½ éœ€è¦æŠŠè·¯å¾„æ›´æ”¹ä¸ºç›¸åº”çš„è·¯å¾„ã€‚\n",
    "# model_path = \"/home/data/muyan/model/AI-ModelScope/Llama-3.2-1B-Instruct\"  \n",
    "model_path = \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\"  \n",
    "\n",
    "# åŠ è½½æŒ‡å®šè·¯å¾„ä¸‹çš„æ¨¡å‹åˆ†è¯å™¨ã€‚åˆ†è¯å™¨è´Ÿè´£å°†è¾“å…¥çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ•°å­—å½¢å¼ï¼Œä¾›æ¨¡å‹å¤„ç†ã€‚è¿™é‡Œä½¿ç”¨äº†from_pretrainedæ–¹æ³•ï¼Œå®ƒä¼šæ ¹æ®ä½ æŒ‡å®šçš„è·¯å¾„è‡ªåŠ¨åŠ è½½å¯¹åº”çš„é¢„è®­ç»ƒåˆ†è¯å™¨ã€‚\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# åŠ è½½å¤§è¯­è¨€æ¨¡å‹ï¼Œè®¾ç½®ä½¿ç”¨è‡ªåŠ¨æ•°æ®ç±»å‹å’Œè®¾å¤‡æ˜ å°„\n",
    "# `torch_dtype=\"auto\"` ä¼šæ ¹æ®ä½ çš„ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç±»å‹ï¼ˆæ¯”å¦‚ä½¿ç”¨ float16 ä»¥å‡å°‘æ˜¾å­˜å ç”¨ï¼‰\n",
    "# `device_map=\"auto\"` ä¼šæ ¹æ®å¯ç”¨çš„ GPU è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è®¾å¤‡æ¥åŠ è½½æ¨¡å‹ã€‚\n",
    "# å¦‚æœæœºå™¨ä¸Šæœ‰å¤šå¼ æ˜¾å¡ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ†é…åˆ°ä¸åŒæ˜¾å¡ä¸Šã€‚ä½ ä¸éœ€è¦æ˜¾å¼åœ°é€‰æ‹©ã€‚\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,         \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "# è®¾ç½®pad_tokenä¸ºeos_tokenï¼ˆå¦‚æœæ²¡æœ‰æŒ‡å®šLlamaç³»åˆ—æ¨¡å‹ä¼šæŠ¥é”™ï¼ŒåŒæœŸä¹Ÿè¦è®¾ç½®é•¿åº¦è‡ªåŠ¨è¡¥é½ Qwen2.5ä¸ç”¨è®¾ç½®ä¹Ÿä¼šè‡ªåŠ¨è¯†åˆ«ï¼‰\n",
    "# The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "# Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
    "\n",
    "# æ˜¯åˆ†è¯å™¨çš„å¡«å……ç¬¦å·ã€‚å¤§å¤šæ•°è¯­è¨€æ¨¡å‹éœ€è¦å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå¡«å……ï¼Œä½¿å¾—ä¸åŒé•¿åº¦çš„è¾“å…¥å¯ä»¥æ‰¹é‡å¤„ç†ï¼ˆæ¯ä¸ªè¾“å…¥éƒ½å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ï¼‰ã€‚\n",
    "if tokenizer.pad_token is None:\n",
    "    # å¦‚æœåˆ†è¯å™¨æ²¡æœ‰æŒ‡å®šå¡«å……ç¬¦å·ï¼ˆtokenizer.pad_token is Noneï¼‰ï¼Œæˆ‘ä»¬å°†å¡«å……ç¬¦å·è®¾ç½®ä¸º eos_tokenï¼ˆå³åºåˆ—ç»“æŸç¬¦ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†é˜²æ­¢å› ç¼ºå°‘å¡«å……ç¬¦å·è€Œå¯¼è‡´æ¨¡å‹é”™è¯¯ã€‚\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "# æ˜¯ä½ æƒ³è¦è¾“å…¥ç»™æ¨¡å‹çš„æ–‡æœ¬ã€‚\n",
    "input_text = \"ä½ å¥½ï¼Œä½ æ˜¯è°ï¼\"\n",
    "\n",
    "# input_text ä¼šè¢«åˆ†è¯å™¨è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼ˆå³ token IDsï¼‰ã€‚\n",
    "# return_tensors=\"pt\"ï¼šè¡¨ç¤ºè¿”å›PyTorchæ ¼å¼çš„å¼ é‡ï¼Œè¿™æ ·å¯ä»¥ç›´æ¥é€å…¥æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚\n",
    "# padding=Trueï¼šè‡ªåŠ¨å°†è¾“å…¥æ–‡æœ¬å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ã€‚\n",
    "# truncation=Trueï¼šå¦‚æœè¾“å…¥æ–‡æœ¬å¤ªé•¿ï¼Œè‡ªåŠ¨æˆªæ–­ä¸ºæœ€å¤§é•¿åº¦ã€‚\n",
    "# .to(device)ï¼šå°†å¤„ç†åçš„è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šæ˜¾å¡ä¸Šã€‚\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# æ‰‹åŠ¨è®¾ç½® attention_mask å’Œ pad_token_id\n",
    "# attention_mask æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå“ªäº›ä½ç½®æ˜¯å¡«å……çš„äºŒè¿›åˆ¶æ©ç ã€‚æ¨¡å‹åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä¼šå¿½ç•¥è¿™äº›å¡«å……ä½ç½®ã€‚inputs[\"attention_mask\"] å°†è‡ªåŠ¨åŒ…å«åœ¨ä¹‹å‰çš„åˆ†è¯å™¨å¤„ç†æ­¥éª¤ä¸­ï¼Œä½†æˆ‘ä»¬æ˜¾å¼åœ°å°†å…¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ã€‚\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(model.device)\n",
    "# pad_token_id æ˜¯å¡«å……ç¬¦å·çš„IDã€‚æˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºåˆ†è¯å™¨çš„å¡«å……ç¬¦å·IDï¼Œä»¥ç¡®ä¿æ¨¡å‹çŸ¥é“å¦‚ä½•å¤„ç†å¡«å……çš„éƒ¨åˆ†ã€‚\n",
    "inputs[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# è¿›è¡Œæ¨ç†\n",
    "# model.generate() æ˜¯ç”Ÿæˆå¼æ¨¡å‹çš„æ¨ç†æ–¹æ³•ã€‚å®ƒå°†æ ¹æ®è¾“å…¥çš„ input_idsï¼ˆæ–‡æœ¬çš„token IDsï¼‰ç”Ÿæˆè¾“å‡ºã€‚\n",
    "# inputs[\"input_ids\"]ï¼šæ¨¡å‹éœ€è¦çš„è¾“å…¥IDã€‚\n",
    "# attention_maskï¼šå‘ŠçŸ¥æ¨¡å‹å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº›æ˜¯å¡«å……çš„ã€‚\n",
    "# pad_token_idï¼šå¡«å……ç¬¦å·çš„IDï¼Œå‘Šè¯‰æ¨¡å‹å“ªäº›éƒ¨åˆ†æ˜¯å¡«å……ã€‚\n",
    "# max_length=50ï¼šé™åˆ¶ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ä¸º50ä¸ªtokenã€‚\n",
    "outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], pad_token_id=inputs[\"pad_token_id\"], max_length=50)\n",
    "\n",
    "# ä¸ªäººå…´è¶£åšéªŒè¯ä½¿ç”¨å¯¹æ¯”llamaæ¨¡å‹ä»¥åŠqwenç³»åˆ—æ¨¡å‹åŒºåˆ«\n",
    "#==============================\n",
    "# åŸå§‹è¾“å…¥çš„é•¿åº¦\n",
    "original_length = len(tokenizer.tokenize(input_text))\n",
    "# å¡«å……åçš„é•¿åº¦\n",
    "padded_length = len(inputs[\"input_ids\"][0])\n",
    "# å¡«å……çš„æ•°é‡\n",
    "padding_count = padded_length - original_length\n",
    "\n",
    "print(f\"tokenizer.pad_tokenï¼š{tokenizer.pad_token}\")\n",
    "print(f\"tokenizer.eos_tokenï¼š{tokenizer.eos_token}\")\n",
    "print(f\"åŸå§‹è¾“å…¥é•¿åº¦ï¼š{original_length}\")\n",
    "print(f\"å¡«å……åçš„é•¿åº¦ï¼š{padded_length}\")\n",
    "print(f\"å¡«å……çš„å ä½ç¬¦æ•°é‡ï¼š{padding_count}\")\n",
    "print(\"==\"*100)\n",
    "#==============================\n",
    "\n",
    "# outputs[0] æ˜¯ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªåºåˆ—ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªç”Ÿæˆåºåˆ—ï¼Œä½†è¿™é‡Œåªå–ç¬¬ä¸€ä¸ªã€‚\n",
    "# tokenizer.decode(outputs[0], skip_special_tokens=True)ï¼šå°†ç”Ÿæˆçš„token IDsè½¬æ¢å›æ–‡æœ¬ï¼Œå¹¶è·³è¿‡ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚[PAD]ã€[SEP]ç­‰ï¼‰ã€‚\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc833f",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥å‚è€ƒå®˜æ–¹ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedb77fb-fc86-4933-a038-eba1cb09d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯Qwenï¼Œç”±é˜¿é‡Œäº‘è‡ªä¸»ç ”å‘çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„è®¾è®¡ç›®çš„æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå›ç­”é—®é¢˜ã€æ’°å†™æ–‡æ¡£ã€åˆ›ä½œæ–‡ç« ç­‰ã€‚æˆ‘èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡ï¼Œå¹¶æ ¹æ®è¿™äº›ä¿¡æ¯ç”Ÿæˆå…·æœ‰ä¸°å¯Œæƒ³è±¡åŠ›å’Œæ·±åº¦çš„ç†è§£ä¸è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å…·å¤‡å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¦‚å¯¹è¯ç®¡ç†ã€åˆ›æ„å†™ä½œä»¥åŠæ•°æ®æŒ–æ˜ç­‰é¢†åŸŸã€‚é€šè¿‡å¤§é‡çš„é¢„è®­ç»ƒå’ŒæŒç»­çš„å­¦ä¹ ï¼Œæˆ‘å¯ä»¥ä¸æ–­ä¼˜åŒ–è‡ªå·±çš„æ€§èƒ½ï¼Œä»¥æ›´å¥½åœ°æœåŠ¡äºç”¨æˆ·çš„éœ€æ±‚ã€‚\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "# import os\n",
    "# æ§åˆ¶åŠ è½½æŒ‡å®šæ˜¾å¡(å¯é€‰)\n",
    "# # è®¾ç½®ä½¿ç”¨çš„æ˜¾å¡ï¼ˆ0, 1, 2è¡¨ç¤ºä½¿ç”¨ç¬¬1ã€2å’Œ3å·æ˜¾å¡ï¼‰\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹åç§°ï¼Œä½¿ç”¨ Qwen2.5-7B-Instruct ç‰ˆæœ¬\n",
    "# model_name = \"ä½ çš„æ¨¡å‹å­˜å‚¨è·¯å¾„\"\n",
    "model_name = \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆå› æ¨¡å‹è¾ƒå¤§ï¼ŒæŒ‡å®šä½¿ç”¨åˆé€‚çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,        # æ¨¡å‹çš„åç§°æˆ–è·¯å¾„\n",
    "    torch_dtype=\"auto\",  # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ PyTorch æ•°æ®ç±»å‹ï¼ˆå¦‚ float16 æˆ– float32ï¼‰\n",
    "    device_map=\"auto\"    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆå¦‚ GPU æˆ– CPUï¼‰ï¼Œå¦‚æœæœ‰å¤šä¸ª GPU åˆ™åˆ†é…åˆ°ä¸åŒè®¾å¤‡\n",
    ")\n",
    "\n",
    "# åŠ è½½ä¸æ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# è®¾ç½®è¾“å…¥çš„æç¤ºæ–‡æœ¬\n",
    "prompt = \"ç®€å•ä»‹ç»ä¸€ä¸‹å¤§å‹è¯­è¨€æ¨¡å‹ã€‚\"\n",
    "\n",
    "# åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå¯¹è¯åœºæ™¯\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯Qwenï¼Œé˜¿é‡Œäº‘æ‰“é€ çš„åŠ©æ‰‹\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨åˆ†è¯å™¨å°†æ¶ˆæ¯è½¬åŒ–ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼ï¼ˆå¹¶åŠ å…¥ç”Ÿæˆæç¤ºï¼‰\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,              # æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·å’Œç³»ç»Ÿçš„å¯¹è¯\n",
    "    tokenize=False,        # ä¸è¿›è¡Œæ ‡è®°åŒ–ï¼Œä¿ç•™åŸå§‹æ–‡æœ¬\n",
    "    add_generation_prompt=True  # æ·»åŠ ç”Ÿæˆæç¤ºç¬¦ï¼Œé€šå¸¸æ˜¯ä¸ºäº†æŒ‡å¼•æ¨¡å‹å¦‚ä½•ç”Ÿæˆæ–‡æœ¬\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨åˆ†è¯å™¨å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹æ‰€éœ€çš„å¼ é‡æ ¼å¼ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºæ–‡æœ¬ï¼ŒæŒ‡å®šæœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ–°ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼‰\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,      # å°†æ¨¡å‹è¾“å…¥ä¼ é€’ç»™æ¨¡å‹\n",
    "    max_new_tokens=512   # é™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°ä¸º 512\n",
    ")\n",
    "\n",
    "# ä¿®å‰ªç”Ÿæˆçš„ idï¼Œç¡®ä¿ç”Ÿæˆéƒ¨åˆ†ä¸åŒ…å«è¾“å…¥çš„éƒ¨åˆ†\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# è§£ç ç”Ÿæˆçš„ token id ä¸ºå¯è¯»çš„æ–‡æœ¬ï¼Œå¹¶å»é™¤ç‰¹æ®Šæ ‡è®°\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# æ‰“å°æœ€ç»ˆç”Ÿæˆçš„å›ç­”\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf5ea9",
   "metadata": {},
   "source": [
    "## ç•Œé¢éªŒè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dcc87",
   "metadata": {},
   "source": [
    "ä½¿ç”¨å®˜æ–¹ç¨‹åºï¼šæ³¨æ„æ›´æ”¹IPåœ°å€ä»¥åŠå¯¹åº”çš„æ¨¡å‹åŠ è½½è·¯å¾„\n",
    "\n",
    "æ–‡ä»¶åç§°ä¸ºweb_demo.py å¯¹åº”å¯åŠ¨å‘½ä»¤ python web_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef2fb7",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from threading import Thread\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "# å¯ä»¥ç›´æ¥å¯åŠ¨åç»­å¾®è°ƒå®Œåˆå¹¶åçš„æ¨¡å‹\n",
    "# DEFAULT_CKPT_PATH = \"/home/data/muyan/code/merge_model/1224/24_23_0.5B_200\"\n",
    "# ç›´æ¥åŠ è½½ä¸‹è½½æ¨¡å‹è·¯å¾„\n",
    "DEFAULT_CKPT_PATH = \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "\n",
    "def _get_args():\n",
    "    parser = ArgumentParser(description=\"Qwen2.5-Instruct web chat demo.\")\n",
    "    parser.add_argument(\n",
    "        \"-c\",\n",
    "        \"--checkpoint-path\",\n",
    "        type=str,\n",
    "        default=DEFAULT_CKPT_PATH,\n",
    "        help=\"Checkpoint name or path, default to %(default)r\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--share\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Create a publicly shareable link for the interface.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--inbrowser\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Automatically launch the interface in a new tab on the default browser.\",\n",
    "    )\n",
    "    parser.add_argument( \n",
    "        \"--server-port\", type=int, default=8000, help=\"Demo server port.\"\n",
    "    )\n",
    "    parser.add_argument( # IPåœ°å€å¤§å®¶å¯ä»¥è‡ªè¡Œä¿®æ”¹ï¼Œå¯ä»¥ä½¿ç”¨127.0.0.1  \n",
    "        \"--server-name\", type=str, default=\"0.0.0.0\", help=\"Demo server name.\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def _load_model_tokenizer(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        resume_download=True,\n",
    "    )\n",
    "\n",
    "    if args.cpu_only:\n",
    "        device_map = \"cpu\"\n",
    "    else:\n",
    "        device_map = \"auto\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.checkpoint_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device_map,\n",
    "        resume_download=True,\n",
    "    ).eval()\n",
    "    model.generation_config.max_new_tokens = 2048  # For chat.\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _chat_stream(model, tokenizer, query, history):\n",
    "    conversation = []\n",
    "    for query_h, response_h in history:\n",
    "        conversation.append({\"role\": \"user\", \"content\": query_h})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": response_h})\n",
    "    conversation.append({\"role\": \"user\", \"content\": query})\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True\n",
    "    )\n",
    "    generation_kwargs = {\n",
    "        **inputs,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        yield new_text\n",
    "\n",
    "\n",
    "def _gc():\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def _launch_demo(args, model, tokenizer):\n",
    "    def predict(_query, _chatbot, _task_history):\n",
    "        print(f\"User: {_query}\")\n",
    "        _chatbot.append((_query, \"\"))\n",
    "        full_response = \"\"\n",
    "        response = \"\"\n",
    "        for new_text in _chat_stream(model, tokenizer, _query, history=_task_history):\n",
    "            response += new_text\n",
    "            _chatbot[-1] = (_query, response)\n",
    "\n",
    "            yield _chatbot\n",
    "            full_response = response\n",
    "\n",
    "        print(f\"History: {_task_history}\")\n",
    "        _task_history.append((_query, full_response))\n",
    "        print(f\"Qwen: {full_response}\")\n",
    "\n",
    "    def regenerate(_chatbot, _task_history):\n",
    "        if not _task_history:\n",
    "            yield _chatbot\n",
    "            return\n",
    "        item = _task_history.pop(-1)\n",
    "        _chatbot.pop(-1)\n",
    "        yield from predict(item[0], _chatbot, _task_history)\n",
    "\n",
    "    def reset_user_input():\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    def reset_state(_chatbot, _task_history):\n",
    "        _task_history.clear()\n",
    "        _chatbot.clear()\n",
    "        _gc()\n",
    "        return _chatbot\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"\"\"\\\n",
    "<p align=\"center\"><img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/logo/qwen2.5_logo.png\" style=\"height: 120px\"/><p>\"\"\")\n",
    "        gr.Markdown(\n",
    "            \"\"\"\\\n",
    "<center><font size=3>This WebUI is based on Qwen2.5-Instruct, developed by Alibaba Cloud. \\\n",
    "(æœ¬WebUIåŸºäºQwen2.5-Instructæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>\"\"\"\n",
    "        )\n",
    "        gr.Markdown(\"\"\"\\\n",
    "<center><font size=4>\n",
    "Qwen2.5-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct/summary\">ğŸ¤– </a> | \n",
    "<a href=\"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\">ğŸ¤—</a>&nbsp ï½œ \n",
    "Qwen2.5-32B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-32B-Instruct/summary\">ğŸ¤– </a> | \n",
    "<a href=\"https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\">ğŸ¤—</a>&nbsp ï½œ \n",
    "Qwen2.5-72B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-72B-Instruct/summary\">ğŸ¤– </a> | \n",
    "<a href=\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\">ğŸ¤—</a>&nbsp ï½œ \n",
    "&nbsp<a href=\"https://github.com/QwenLM/Qwen2.5\">Github</a></center>\"\"\")\n",
    "\n",
    "        chatbot = gr.Chatbot(label=\"Qwen\", elem_classes=\"control-height\")\n",
    "        query = gr.Textbox(lines=2, label=\"Input\")\n",
    "        task_history = gr.State([])\n",
    "\n",
    "        with gr.Row():\n",
    "            empty_btn = gr.Button(\"ğŸ§¹ Clear History (æ¸…é™¤å†å²)\")\n",
    "            submit_btn = gr.Button(\"ğŸš€ Submit (å‘é€)\")\n",
    "            regen_btn = gr.Button(\"ğŸ¤”ï¸ Regenerate (é‡è¯•)\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            predict, [query, chatbot, task_history], [chatbot], show_progress=True\n",
    "        )\n",
    "        submit_btn.click(reset_user_input, [], [query])\n",
    "        empty_btn.click(\n",
    "            reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True\n",
    "        )\n",
    "        regen_btn.click(\n",
    "            regenerate, [chatbot, task_history], [chatbot], show_progress=True\n",
    "        )\n",
    "\n",
    "        gr.Markdown(\"\"\"\\\n",
    "<font size=2>Note: This demo is governed by the original license of Qwen2.5. \\\n",
    "We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\n",
    "including hate speech, violence, pornography, deception, etc. \\\n",
    "(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwen2.5çš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\\\n",
    "åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)\"\"\")\n",
    "\n",
    "    demo.queue().launch(\n",
    "        share=args.share,\n",
    "        inbrowser=args.inbrowser,\n",
    "        server_port=args.server_port,\n",
    "        server_name=args.server_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = _get_args()\n",
    "\n",
    "    model, tokenizer = _load_model_tokenizer(args)\n",
    "\n",
    "    _launch_demo(args, model, tokenizer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5ce92",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225172606666.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425c5dc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225172708897.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13833448",
   "metadata": {},
   "source": [
    "åç«¯æ—¥å¿—æ˜¾ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d65e59",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225175718184.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af035156",
   "metadata": {},
   "source": [
    "# 3. PEFTä»‹ç»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f50aee",
   "metadata": {},
   "source": [
    "## 3.1 Hugging Face å¹³å°ç®€ä»‹\n",
    "\n",
    "Hugging Face æ˜¯ä¸€å®¶ä¸“æ³¨äº **äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯å¼€å‘å’Œåº”ç”¨** çš„å…¬å¸ã€‚å®ƒé€šè¿‡å¼€æºå·¥å…·ã€ç¤¾åŒºæ”¯æŒå’Œäº‘æœåŠ¡ï¼Œæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæ¶µç›–äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ç­‰é¢†åŸŸã€‚Hugging Face çš„å·¥å…·å’Œå¹³å°åœ¨å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šç•Œä¸­éƒ½éå¸¸æµè¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f5353",
   "metadata": {},
   "source": [
    "### Hugging Face çš„ä¸»è¦ç»„æˆéƒ¨åˆ†\n",
    "\n",
    "#### **1. Model Hubï¼ˆæ¨¡å‹ä¸­å¿ƒï¼‰**\n",
    "Hugging Face æä¾›äº†ä¸€ä¸ªå…¨çƒæœ€å¤§çš„ **æ¨¡å‹å…±äº«å¹³å°**ï¼Œè¢«ç§°ä¸º **Model Hub**ã€‚å®ƒæ˜¯ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒã€‚\n",
    "- **åŠŸèƒ½**ï¼š\n",
    "  - ç”¨æˆ·å¯ä»¥ä¸Šä¼ å’Œä¸‹è½½æ•°ä»¥åƒè®¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPT-2ã€BERTã€LLaMA ç­‰ï¼‰ã€‚\n",
    "  - æ”¯æŒå¤šä¸ªæ¡†æ¶ï¼ŒåŒ…æ‹¬ **PyTorch**ã€**TensorFlow** å’Œ **JAX**ã€‚\n",
    "  - æ¨¡å‹å¯ä»¥ç›´æ¥ç”¨äºæ¨ç†ã€å¾®è°ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0635a",
   "metadata": {},
   "source": [
    "#### 2. æ•°æ®é›†ç”Ÿæ€ï¼ˆDatasets Hubï¼‰\n",
    "Hugging Face æä¾›äº†ä¸€ä¸ªç±»ä¼¼ Model Hub çš„ **æ•°æ®é›†å¹³å°**ï¼Œç§°ä¸º **Datasets Hub**ã€‚\n",
    "- **åŠŸèƒ½**ï¼š\n",
    "  - ç”¨æˆ·å¯ä»¥è®¿é—®è¶…è¿‡ 10,000 ä¸ªæ ‡å‡†åŒ–æ•°æ®é›†ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ã€ç¿»è¯‘ã€é—®ç­”ç­‰ä»»åŠ¡ã€‚\n",
    "  - æ•°æ®é›†å¯ä»¥ä¸æ¨¡å‹æ— ç¼ç»“åˆï¼Œæ”¯æŒè‡ªåŠ¨åˆ†è¯å’Œæ‰¹å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82598e",
   "metadata": {},
   "source": [
    "#### 3. Transformersï¼ˆæ ¸å¿ƒå·¥å…·åº“ï¼‰\n",
    "- **Transformers** æ˜¯ Hugging Face çš„æ ¸å¿ƒåº“ï¼Œæ”¯æŒåŠ è½½å’Œè®­ç»ƒé¢„è®­ç»ƒçš„ Transformer æ¨¡å‹ã€‚\n",
    "- **åŠŸèƒ½**ï¼š\n",
    "  - åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPTã€BERT ç­‰ï¼‰ã€‚\n",
    "  - è¿›è¡Œä»»åŠ¡å¾®è°ƒï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€ç¿»è¯‘ç­‰ï¼‰ã€‚\n",
    "  - æ¨ç†å’Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d28aa",
   "metadata": {},
   "source": [
    "#### 4. Tokenizersï¼ˆåˆ†è¯å·¥å…·åº“ï¼‰\n",
    "- ç”¨äºå°†æ–‡æœ¬å¤„ç†æˆæ¨¡å‹å¯ä»¥ç†è§£çš„è¾“å…¥æ ¼å¼ï¼ˆtokensï¼‰ã€‚\n",
    "- æ”¯æŒå¤šç§åˆ†è¯ç®—æ³•ï¼ˆå¦‚ WordPieceã€BPE ç­‰ï¼‰ã€‚\n",
    "- é«˜æ•ˆä¸”æ”¯æŒå¤šçº¿ç¨‹åˆ†è¯ï¼Œé€‚åˆå¤„ç†å¤§è§„æ¨¡æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f821e7",
   "metadata": {},
   "source": [
    "#### 5. Accelerate\n",
    "- ä¸€ä¸ªè½»é‡çº§åº“ï¼Œç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ã€‚\n",
    "- é€‚åˆéœ€è¦åœ¨å¤š GPU æˆ– TPU ä¸Šè¿è¡Œçš„é¡¹ç›®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4450d",
   "metadata": {},
   "source": [
    "#### 6. PEFT\n",
    "- Hugging Face æä¾›çš„ä¸€ä¸ªåº“ï¼Œç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚\n",
    "- é€šè¿‡åªå¾®è°ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°æ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»è€Œå¤§å¹…é™ä½è®¡ç®—èµ„æºå’Œå­˜å‚¨éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46959446",
   "metadata": {},
   "source": [
    "**å‡è®¾ä½ è¦è®­ç»ƒä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼ˆä»è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼‰ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥ç±»æ¯”ä¸ºå®Œæˆä¸€ä¸ª â€œç¿»è¯‘ä»»åŠ¡â€ï¼š**\n",
    "\n",
    "1. **Transformers æ˜¯ä½ çš„ç¿»è¯‘å·¥å…·**ï¼š\n",
    "   - å®ƒåƒæ˜¯ä¸€æœ¬å…¨é¢çš„ç¿»è¯‘å­—å…¸ï¼Œæä¾›äº†å¤šç§è¯­è¨€å’Œé¢„è®­ç»ƒçŸ¥è¯†ã€‚\n",
    "\n",
    "2. **Datasets æä¾›äº†ç»ƒä¹ ææ–™**ï¼š\n",
    "   - æ•°æ®é›†å°±åƒä¸€å †ç¿»è¯‘ç»ƒä¹ é¢˜ï¼Œå¸®åŠ©æ¨¡å‹æŒæ¡ç¿»è¯‘èƒ½åŠ›ã€‚\n",
    "\n",
    "3. **Tokenizers æ˜¯ä½ çš„åŠ©æ‰‹**ï¼š\n",
    "   - Tokenizers æŠŠæ¯å¥è¯åˆ‡åˆ†æˆå•è¯æˆ–è¯ç»„ï¼Œå°±åƒåŠ©æ‰‹å¸®ä½ åˆ†è§£æ¯å¥å¤æ‚çš„å¥å­ã€‚\n",
    "\n",
    "4. **Accelerate æ˜¯è®­ç»ƒåŠ é€Ÿå™¨**ï¼š\n",
    "   - å®ƒç›¸å½“äºä¸ºä½ æä¾›äº†ä¸€æ‰¹åŠ©æ‰‹ï¼ˆGPUï¼‰ï¼Œè®©ä½ èƒ½æ›´å¿«å®Œæˆæ‰€æœ‰ç¿»è¯‘ç»ƒä¹ ã€‚\n",
    "\n",
    "5. **PEFT æ˜¯èµ„æºå—é™æ—¶çš„ä¼˜åŒ–ç­–ç•¥**ï¼š\n",
    "   - å¦‚æœç¡¬ä»¶èµ„æºä¸è¶³ï¼Œå…¨é‡å¾®è°ƒå¤ªè€—è´¹æ—¶é—´å’Œå­˜å‚¨ç©ºé—´ï¼Œä½ å¯ä»¥ç”¨ PEFTï¼Œåªä¼˜åŒ–ç¿»è¯‘ä¸­ç‰¹å®šçš„å¥å¼æˆ–ç”¨è¯è§„åˆ™ï¼ˆè°ƒæ•´å°‘é‡å‚æ•°ï¼‰ï¼Œä»¥ä¾¿å¿«é€Ÿæå‡æ•ˆæœã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca57dca",
   "metadata": {},
   "source": [
    "## 3.2 PEFTè¯¦è§£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2e972",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### PEFT æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "PEFT æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯**ï¼Œæ—¨åœ¨è§£å†³å¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ—¶çš„é«˜èµ„æºéœ€æ±‚é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒä¼šè°ƒæ•´æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œè¿™å¯¹å­˜å‚¨ã€è®¡ç®—èƒ½åŠ›æå‡ºäº†å¾ˆé«˜çš„è¦æ±‚ã€‚è€Œ PEFT é€šè¿‡åªæ›´æ–°å°‘é‡å‚æ•°ï¼Œå®ç°é«˜æ•ˆä¸”ç»æµçš„å¾®è°ƒã€‚\n",
    "\n",
    "#### **PEFT çš„ä¸»è¦æ–¹æ³•**\n",
    "1. **LoRAï¼ˆLow-Rank Adaptationï¼‰**\n",
    "   - é€šè¿‡å¼•å…¥ä½ç§©çŸ©é˜µï¼Œåœ¨ä¸æ”¹å˜åŸå§‹æ¨¡å‹å‚æ•°çš„åŸºç¡€ä¸Šï¼Œå¾®è°ƒå°‘é‡çš„å‚æ•°çŸ©é˜µã€‚\n",
    "   - **ç‰¹ç‚¹**ï¼šé€‚åˆç¡¬ä»¶å—é™çš„åœºæ™¯ã€‚\n",
    "\n",
    "2. **Prefix Tuning**\n",
    "   - åœ¨æ¨¡å‹çš„è¾“å…¥åºåˆ—ä¸­æ·»åŠ ä¸€ç»„å¯è®­ç»ƒçš„å‰ç¼€å‘é‡ï¼Œä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚\n",
    "   - **ç‰¹ç‚¹**ï¼šæ¨¡å‹å‚æ•°å®Œå…¨å†»ç»“ï¼Œåªè°ƒæ•´å‰ç¼€ã€‚\n",
    "\n",
    "3. **P-Tuning v2**\n",
    "   - åœ¨æ¨¡å‹çš„æ¯ä¸€å±‚æ’å…¥å°‘é‡çš„æç¤ºå‘é‡ï¼Œé€šè¿‡è¿™äº›å‘é‡è°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºã€‚\n",
    "   - **ç‰¹ç‚¹**ï¼šé«˜æ•ˆä¸”é€‚ç”¨äºå¤æ‚ä»»åŠ¡ã€‚\n",
    "\n",
    "4. **Prompt Tuning**\n",
    "   - ç›´æ¥ä¼˜åŒ–è¾“å…¥çš„æç¤ºï¼ˆPromptï¼‰ï¼Œä½¿æ¨¡å‹ç”Ÿæˆç¬¦åˆéœ€æ±‚çš„ç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fed6e1",
   "metadata": {},
   "source": [
    "**æŠŠ Hugging Face å¹³å°æ¯”ä½œä¸€å®¶é«˜çº§å®šåˆ¶æœè£…åº—ï¼š**\n",
    "- **Transformers** æ˜¯ä»“åº“é‡Œçš„é€šç”¨æœè£…ï¼Œå·²ç»æœ‰äº†ä¸åŒå°ºç å’Œæ¬¾å¼çš„è¡£æœã€‚\n",
    "- **PEFT** æ˜¯è£ç¼ï¼Œå®ƒé€šè¿‡ç®€å•è°ƒæ•´ï¼ˆå¦‚åŠ ç‚¹é¥°å“æˆ–ä¿®æ”¹éƒ¨åˆ†ç»†èŠ‚ï¼‰å°±èƒ½è®©è¡£æœæ›´è´´åˆä½ çš„éœ€æ±‚ã€‚\n",
    "  - **LoRA**ï¼šåƒæ˜¯åŠ äº†ä¸€ä¸ªç®€å•çš„èƒ¸é’ˆï¼ˆä½ç§©çŸ©é˜µï¼‰ï¼Œè®©è¡£æœæ›´é€‚åˆæ­£å¼åœºåˆã€‚\n",
    "  - **Prefix Tuning**ï¼šåƒæ˜¯åœ¨è¡£æœä¸Šæ·»åŠ ä¸€æ¡ç‹¬ç‰¹çš„è…°å¸¦ï¼ˆå‰ç¼€å‘é‡ï¼‰ã€‚\n",
    "  - **Prompt Tuning**ï¼šåƒæ˜¯ç»™è¡£æœåŠ ä¸ªæ ‡ç­¾ï¼ˆæç¤ºè¯­ï¼‰ï¼Œè¯´æ˜å®ƒé€‚åˆçš„åœºæ™¯ã€‚\n",
    "\n",
    "æœ€ç»ˆï¼ŒPEFT çš„ç›®æ ‡æ˜¯ä»¥æœ€å°çš„ä¿®æ”¹è®©æ¨¡å‹ï¼ˆæœè£…ï¼‰åœ¨æ–°çš„ä»»åŠ¡ï¼ˆåœºåˆï¼‰ä¸­è¡¨ç°æ›´å¥½ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280494c",
   "metadata": {},
   "source": [
    "#### Hugging Face å’Œ PEFT çš„å…³ç³»\n",
    "\n",
    "1. **Hugging Face æ˜¯ç”Ÿæ€å¹³å°**ï¼š\n",
    "   - æä¾›äº†æ¨¡å‹ï¼ˆTransformersï¼‰ã€æ•°æ®ï¼ˆDatasetsï¼‰ã€å·¥å…·ï¼ˆAccelerate å’Œ Tokenizersï¼‰ä»¥åŠå¾®è°ƒæŠ€æœ¯ï¼ˆPEFTï¼‰çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆã€‚\n",
    "   - PEFT æ˜¯ Hugging Face ç”Ÿæ€ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œä¸“æ³¨äºé«˜æ•ˆå¾®è°ƒã€‚\n",
    "\n",
    "2. **PEFT åŸºäº Transformers æ„å»º**ï¼š\n",
    "   - PEFT çš„æ‰€æœ‰æ–¹æ³•éƒ½ä¾èµ–äº Transformers åº“æä¾›çš„æ¨¡å‹åŠ è½½å’Œæ¨ç†åŠŸèƒ½ã€‚\n",
    "   - å®ƒé€šè¿‡ä¸ Transformers é›†æˆï¼Œæä¾›æ›´åŠ ç»æµçš„å¾®è°ƒæ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416c810",
   "metadata": {},
   "source": [
    "### ä¹‹å‰ç”¨åˆ°çš„å®‰è£…åŒ…å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acd902",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **åº“**         | **ä½œç”¨**                                                                 | **å…³ç³»**                                                                                  | **åŒºåˆ«**                                                                 |\n",
    "|----------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **Transformers** | æä¾›é¢„è®­ç»ƒæ¨¡å‹å’ŒåŸºç¡€æ“ä½œï¼ˆå¦‚åŠ è½½ã€å¾®è°ƒå’Œæ¨ç†ï¼‰ã€‚                              | æ˜¯æ‰€æœ‰æ“ä½œçš„åŸºç¡€ï¼Œå…¶ä»–åº“éƒ½ä¾èµ–äº Transformers æä¾›çš„æ¨¡å‹å’ŒåŠŸèƒ½ã€‚                                | å…³æ³¨æ¨¡å‹åŠ è½½å’ŒåŸºç¡€è®­ç»ƒï¼Œä¸æ¶‰åŠé«˜æ•ˆå¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ã€‚                                  |\n",
    "| **PEFT**        | é€šè¿‡é«˜æ•ˆå¾®è°ƒæ–¹æ³•åªè°ƒæ•´å°‘é‡å‚æ•°æ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ï¼Œé™ä½èµ„æºéœ€æ±‚ã€‚                     | åŸºäº Transformersï¼Œå¾®è°ƒæ—¶è°ƒç”¨å…¶æ¨¡å‹å’Œæ•°æ®ã€‚                                               | ä¸“æ³¨äºå‡å°‘è®­ç»ƒèµ„æºå’Œå­˜å‚¨ï¼Œé€‚åˆå¤šä»»åŠ¡åœºæ™¯æˆ–ç¡¬ä»¶å—é™çš„ç¯å¢ƒã€‚                          |\n",
    "| **TRL**         | ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯ç»“åˆäººç±»åé¦ˆçš„ RLHF æ–¹æ³•ã€‚                      | ä¹Ÿä¾èµ–äº Transformers æä¾›çš„æ¨¡å‹ï¼Œä½†ä¸“æ³¨äºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æµç¨‹ã€‚                                | ä¸“æ³¨äºé€šè¿‡å¥–åŠ±å‡½æ•°å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ PPOï¼‰æå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸ PEFT çš„ç›®æ ‡äº’è¡¥ã€‚         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acb949",
   "metadata": {},
   "source": [
    "**å°†æ•´ä¸ªå¾®è°ƒè¿‡ç¨‹æ¯”ä½œä¸€ä¸ªå¨å¸ˆï¼ˆæ¨¡å‹ï¼‰å­¦ä¹ çƒ¹é¥ªæ–°èœå“ï¼š**\n",
    "\n",
    "1. **Transformers**ï¼š\n",
    "   - ç›¸å½“äºå¨å¸ˆçš„åŸºç¡€æŠ€èƒ½å’Œé£Ÿè°±ã€‚ä¾‹å¦‚ï¼ŒGPT æ˜¯å¨å¸ˆçš„åŸºç¡€èƒ½åŠ›ï¼Œå®ƒå·²ç»æŒæ¡äº†å¾ˆå¤šé€šç”¨èœå“çš„åšæ³•ã€‚\n",
    "   - Transformers åº“æä¾›äº†è¿™äº›æŠ€èƒ½å’Œé£Ÿè°±ã€‚\n",
    "\n",
    "2. **PEFT**ï¼š\n",
    "   - ç›¸å½“äºå¨å¸ˆå­¦ä¹ æ–°èœå“æ—¶ï¼Œä¸éœ€è¦é‡æ–°å­¦ä¹ æ‰€æœ‰æŠ€èƒ½ï¼Œåªéœ€å­¦ä¼šæŸäº›ç‰¹æ®ŠæŠ€å·§æˆ–åŠ å…¥ä¸€äº›æ–°è°ƒæ–™ã€‚\n",
    "   - PEFT æ–¹æ³•é€šè¿‡åªè°ƒæ•´å°‘é‡å‚æ•°ï¼ˆæ¯”å¦‚ç‰¹å®šçš„è°ƒæ–™é…æ–¹ï¼‰æ¥è®©å¨å¸ˆå¿«é€Ÿé€‚åº”æ–°çš„èœå“éœ€æ±‚ã€‚\n",
    "\n",
    "3. **TRL**ï¼š\n",
    "   - ç›¸å½“äºé€šè¿‡é¡¾å®¢çš„åé¦ˆä¼˜åŒ–å¨å¸ˆçš„çƒ¹é¥ªæŠ€å·§ã€‚ä¾‹å¦‚ï¼Œé¡¾å®¢å–œæ¬¢èœå“çš„æŸç§å‘³é“ï¼Œå¨å¸ˆé€šè¿‡å¥–åŠ±æœºåˆ¶ï¼ˆäººç±»åé¦ˆï¼‰ä¸æ–­ä¼˜åŒ–è‡ªå·±çš„åšèœæ–¹å¼ã€‚\n",
    "   - TRL æ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œæ•™å¨å¸ˆæ›´å¥½åœ°æ»¡è¶³é¡¾å®¢çš„åå¥½ã€‚\n",
    "\n",
    "**æœ€ç»ˆç»“æœ**ï¼š\n",
    "- Transformers æä¾›äº†å¨å¸ˆçš„åŸºç¡€èƒ½åŠ›ã€‚\n",
    "- PEFT å¸®åŠ©å¨å¸ˆå¿«é€ŸæŒæ¡æ–°èœå“çš„é…æ–¹ã€‚\n",
    "- TRL é€šè¿‡é¡¾å®¢åé¦ˆè®©å¨å¸ˆè¿›ä¸€æ­¥æå‡èœå“è´¨é‡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e559",
   "metadata": {},
   "source": [
    "# 4. æ¨¡å‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87319c",
   "metadata": {},
   "source": [
    "- **step1 æ•°æ®å‡†å¤‡**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0a366",
   "metadata": {},
   "source": [
    "é€‰æ‹©æ•°æ®æºï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596da1b",
   "metadata": {},
   "source": [
    "æ³•å¾‹æ•°æ®ï¼šhttps://www.modelscope.cn/datasets?page=1&query=%E6%B3%95%E5%BE%8B&sort=downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aceda23",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224111444755.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839160e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224111546546.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8adeef3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224111616135.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd47af",
   "metadata": {},
   "source": [
    "ä¸­åŒ»æ•°æ®ï¼š https://www.modelscope.cn/datasets/alexhuangguo/chinese-medical/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7785f9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224111919438.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a00b6f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224111949666.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e286d50",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224112022652.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bc7de",
   "metadata": {},
   "source": [
    "é‡‘èæ•°æ®ï¼šhttps://github.com/A-Rain/BDCI2019-Negative_Finance_Info_Judge/tree/master/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b59300",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224114708265.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47229568",
   "metadata": {},
   "source": [
    "é‡‘èæ•°æ®åˆé›†é“¾æ¥ï¼šhttps://github.com/icoxfog417/awesome-financial-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cb8b0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224114843768.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e1d3",
   "metadata": {},
   "source": [
    "è¯‘æ–‡ä»‹ç»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259d1d5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224114902082.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357990d",
   "metadata": {},
   "source": [
    "æ³•å¾‹ç±»ï¼šç½ªåæ³•åŠ¡åè¯åŠåˆ†ç±»æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b71d13",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224213153151.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4422c1",
   "metadata": {},
   "source": [
    "å¯¹è”æ•°æ®ï¼šhttps://github.com/wb14123/couplet-dataset?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a1e19",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224115528790.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3898ee",
   "metadata": {},
   "source": [
    "æœ¬æ¬¡ä½¿ç”¨çš„æ˜¯ä¸­åŒ»é—®è¯Šæ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d73ca",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224112143414.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020c462",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224112209333.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a718c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224213957524.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43452a8f",
   "metadata": {},
   "source": [
    "å¯ä»¥æŸ¥çœ‹dataset_infos.json æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨æ•°æ®å¡ç‰‡æŸ¥çœ‹ä¿¡æ¯åç›´æ¥ä¸Šä¼ æœåŠ¡å™¨è¿›è¡Œæ•°æ®æ ¡éªŒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498e58e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224190009822.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c3cf5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224190057870.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498aa4e3",
   "metadata": {},
   "source": [
    "å¼€å§‹è¿›è¡Œæ•°æ®å¤„ç†(å½“å‰æ•°æ®å¤„ç†æ ¼å¼å¹¶ä¸å”¯ä¸€ï¼Œå½“å‰æ•°æ®æ ¼å¼æ–¹ä¾¿å¤§å®¶ç†è§£ï¼Œåç»­å¯ä»¥è¿›è¡Œè‡ªå®šä¹‰å¤„ç†ã€‚)\n",
    "```python\n",
    "# æ³¨æ„ï¼šå½“å‰åªå–1000æ¡æ•°æ®ï¼Œå¹¶ä¸”æ›´æ”¹äº†\"instruction\"ã€\"input\"ç›¸äº’ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿åç»­ç»Ÿä¸€è¯†åˆ«\n",
    "# å½“å‰æ–‡ä»¶å‘½åä¸ºconvert_json.py\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "def convert_json_format(input_file, output_file, num_entries=1000):\n",
    "    \"\"\"\n",
    "    è¯»å–è¾“å…¥JSONæ–‡ä»¶ï¼Œå°†æ¯æ¡è®°å½•çš„å­—æ®µé‡æ–°æ’åˆ—ï¼Œå¹¶å†™å…¥è¾“å‡ºJSONæ–‡ä»¶ã€‚\n",
    "    \n",
    "    :param input_file: è¾“å…¥JSONæ–‡ä»¶çš„è·¯å¾„\n",
    "    :param output_file: è¾“å‡ºJSONæ–‡ä»¶çš„è·¯å¾„\n",
    "    :param num_entries: è¦å¤„ç†çš„è®°å½•æ•°é‡\n",
    "    \"\"\"\n",
    "    # æ‰“å¼€å¹¶è¯»å–è¾“å…¥æ–‡ä»¶çš„æ‰€æœ‰è¡Œ\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    converted_data = []\n",
    "    # éå†æ¯ä¸€è¡Œæ•°æ®ï¼Œå¹¶è¿›è¡Œå­—æ®µé‡æ–°æ’åˆ—\n",
    "    for line in lines[:num_entries]:\n",
    "        entry = json.loads(line)\n",
    "        new_entry = {\n",
    "            \"instruction\": entry[\"input\"],  # å°†åŸinputå­—æ®µçš„å†…å®¹ç§»åˆ°instructionå­—æ®µ\n",
    "            \"input\": entry[\"instruction\"],  # å°†åŸinstructionå­—æ®µçš„å†…å®¹ç§»åˆ°inputå­—æ®µ\n",
    "            \"output\": entry[\"output\"]       # ä¿æŒoutputå­—æ®µä¸å˜\n",
    "        }\n",
    "        converted_data.append(new_entry)\n",
    "\n",
    "    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶å¹¶é€æ¡å†™å…¥è½¬æ¢åçš„æ•°æ®\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(converted_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºArgumentParserå¯¹è±¡ä»¥è§£æå‘½ä»¤è¡Œå‚æ•°\n",
    "    parser = argparse.ArgumentParser(description=\"Convert JSON format of medical QA data.\")\n",
    "    \n",
    "    # æ·»åŠ ä½ç½®å‚æ•°ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "    parser.add_argument(\"input_file\", help=\"Path to the input JSON file\")\n",
    "    \n",
    "    # æ·»åŠ ä½ç½®å‚æ•°ï¼šè¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "    parser.add_argument(\"output_file\", help=\"Path to the output JSON file\")\n",
    "    \n",
    "    # æ·»åŠ å¯é€‰å‚æ•°ï¼šè¦å¤„ç†çš„è®°å½•æ•°é‡ï¼Œé»˜è®¤ä¸º1000\n",
    "    parser.add_argument(\"--num_entries\", type=int, default=1000, help=\"Number of entries to process\")\n",
    "\n",
    "    # è§£æå‘½ä»¤è¡Œå‚æ•°\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # è°ƒç”¨convert_json_formatå‡½æ•°è¿›è¡Œå®é™…çš„æ•°æ®è½¬æ¢\n",
    "    convert_json_format(args.input_file, args.output_file, args.num_entries)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adccd75",
   "metadata": {},
   "source": [
    "æ‰§è¡Œå‘½ä»¤å¯¹æ–‡ä»¶è¿›è¡Œè½¬æ¢ä¸ç”Ÿæˆ\n",
    "\n",
    "python convert_json.py ./data/medicalQA.json ./data/medicalQA_top1k.json --num_entries 1000 \n",
    "\n",
    "head -n 10 ./data/medicalQA_top1k   æ‰§è¡Œå®Œæˆåè¿›è¡Œæ•°æ®æ ¡éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d6d92",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224211610270.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d275c",
   "metadata": {},
   "source": [
    "- **step2 å¼€å§‹æ¨¡å‹å¾®è°ƒ**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94feeae",
   "metadata": {},
   "source": [
    "ä¾èµ–ç‰ˆæœ¬ä¿¡æ¯ï¼š\n",
    "\n",
    "| åº“             | ç‰ˆæœ¬         | \n",
    "| -------------- | ------------ | \n",
    "| Python         | 3.12.2       | \n",
    "| pytorch          | 2.4.0+cu121  | \n",
    "| datasets       | 3.0.0        | \n",
    "| pandas         | 2.2.2        | \n",
    "| transformers   | 4.44.0       | \n",
    "| peft           | 0.12.0       |\n",
    "|tensorboard |2.18.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c965",
   "metadata": {},
   "source": [
    "å¤§éƒ¨åˆ†é€‚ç”¨pip install å‘½ä»¤å®‰è£…å³å¯ \n",
    "\n",
    "æŸ¥çœ‹å®‰è£…åŒ…ç‰ˆæœ¬å‘½ä»¤ï¼š\n",
    "\n",
    "pip show å®‰è£…åŒ…åç§°\n",
    "\n",
    "python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88221cb",
   "metadata": {},
   "source": [
    "### 1 å¤šGPUå¾®è°ƒï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdefc5",
   "metadata": {},
   "source": [
    "æœ¬æ¬¡å¾®è°ƒæ–¹å¼ä¸ºLORAï¼Œå¯ä»¥æ ¹æ®ä¸ªäººéœ€æ±‚ç²¾ç®€è¾“å…¥å‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9ffb6",
   "metadata": {},
   "source": [
    "ä»£ç å‘½åä¸ºï¼š train_distributed.py\n",
    "```python \n",
    "# å¯¼å…¥æ‰€éœ€çš„åº“å’Œæ¨¡å—\n",
    "import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å— å®ƒå¯ä»¥ç”¨æ¥è®¿é—®ç¯å¢ƒå˜é‡ã€å¤„ç†æ–‡ä»¶è·¯å¾„ã€æ“ä½œæ–‡ä»¶ç³»ç»Ÿç­‰ã€‚\n",
    "import json  # å¯¼å…¥JSONå¤„ç†æ¨¡å— èƒ½å¤Ÿå°† Python å¯¹è±¡è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œæˆ–å°† JSON å­—ç¬¦ä¸²è½¬æ¢ä¸º Python å¯¹è±¡ï¼Œå¸¸ç”¨äºæ•°æ®äº¤æ¢\n",
    "import torch  # å¯¼å…¥PyTorchåº“ï¼Œç”¨äºå¼ é‡æ“ä½œå’Œæ·±åº¦å­¦ä¹   æ˜¯ PyTorch çš„æ ¸å¿ƒåº“\n",
    "import torch.distributed as dist  # å¯¼å…¥PyTorchåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ  æ˜¯ PyTorch ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„æ¨¡å—ã€‚å®ƒæä¾›äº†å¤šç§æ–¹å¼æ¥åœ¨å¤šå°æœºå™¨æˆ–å¤šå¼  GPU ä¹‹é—´åŒæ­¥å’Œå…±äº«æ•°æ®ï¼Œç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒ\n",
    "from datasets import Dataset  # Hugging Face æä¾›çš„ä¸€ä¸ªåº“ï¼Œç”¨äºåŠ è½½å’Œå¤„ç†æ•°æ®é›†ã€‚å®ƒæä¾›äº†è®¸å¤šå¸¸ç”¨çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œæœºå™¨å­¦ä¹ ä»»åŠ¡çš„æ•°æ® \n",
    "import pandas as pd  # Python ä¸­æœ€å¸¸ç”¨çš„æ•°æ®å¤„ç†åº“ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†ç»“æ„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ CSV æ–‡ä»¶æˆ–æ•°æ®åº“è¡¨æ ¼ï¼‰ã€‚å®ƒæä¾›äº†é«˜æ•ˆçš„ DataFrame ç»“æ„ï¼Œé€‚åˆç”¨äºæ•°æ®æ¸…æ´—ã€æ•°æ®åˆ†æç­‰ä»»åŠ¡ã€‚\n",
    "from transformers import (  # æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªåº“ï¼Œç”¨äºåŠ è½½å’Œè®­ç»ƒé¢„è®­ç»ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹\n",
    "    AutoModelForCausalLM,  # è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPTã€GPT-2ã€GPT-3ï¼‰ã€‚\n",
    "    AutoTokenizer,  # è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨ï¼Œå®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„ tokenï¼ˆæ ‡è®°ï¼‰è¡¨ç¤ºã€‚\n",
    "    TrainingArguments,  # ç”¨äºé…ç½®è®­ç»ƒè¿‡ç¨‹çš„å„ç§å‚æ•°ï¼ˆå¦‚æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼‰ã€‚\n",
    "    Trainer,  # ç”¨äºè®­ç»ƒæ¨¡å‹çš„é«˜çº§æ¥å£ï¼Œç®€åŒ–äº†æ¨¡å‹è®­ç»ƒæµç¨‹ã€‚\n",
    "    DataCollatorForSeq2Seq  #ç”¨äºå¤„ç†åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ã€æ‘˜è¦ç­‰ï¼‰çš„æ•°æ®æ•´ç†å™¨ã€‚\n",
    ")\n",
    " # ç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼ŒParameter Efficient Fine-Tuningï¼‰çš„åº“ï¼Œæ”¯æŒ LoRAç­‰æ–¹æ³•ã€‚\n",
    "from peft import (get_peft_model, # ç”¨äºè·å–æ”¯æŒ LoRA çš„æ¨¡å‹\n",
    "                  LoraConfig, # é…ç½® LoRA æ¨¡å‹çš„è¶…å‚æ•°ï¼ˆå¦‚ rankã€alpha å’Œ dropout ç­‰ï¼‰\n",
    "                  TaskType, # æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ˆä¾‹å¦‚å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼‰ã€‚\n",
    "                  prepare_model_for_kbit_training )  #ç”¨äºå°†æ¨¡å‹å‡†å¤‡ä¸ºé€‚åˆè¿›è¡Œä½æ¯”ç‰¹è®­ç»ƒï¼ˆå¦‚ 8bitï¼‰çš„æ ¼å¼\n",
    "from argparse import ArgumentParser  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°ã€‚åœ¨è¿™é‡Œï¼Œå®ƒç”¨äºè§£æå‘½ä»¤è¡Œä¸­è¾“å…¥çš„å„ç§å‚æ•°ï¼Œå¦‚æ¨¡å‹è·¯å¾„ã€æ•°æ®è·¯å¾„ã€è®­ç»ƒè¶…å‚æ•°ç­‰ã€‚\n",
    "from torch.utils.tensorboard import SummaryWriter #SummaryWriter ç”¨äºå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ä¿¡æ¯ï¼ˆå¦‚æŸå¤±å€¼ã€å­¦ä¹ ç‡ã€å‡†ç¡®ç‡ç­‰ï¼‰å†™å…¥ TensorBoard å¯è§†åŒ–å·¥å…·ä¸­ã€‚\n",
    "import datetime #æä¾›äº†ç”¨äºæ“ä½œæ—¥æœŸå’Œæ—¶é—´çš„ç±»\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„Trainerç±»ï¼Œç»§æ‰¿è‡ªHugging Faceçš„Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "     # åˆå§‹åŒ–æ–¹æ³•ï¼Œåˆå§‹åŒ–Trainerçš„åŸºç¡€åŠŸèƒ½å¹¶æ‰©å±•\n",
    "    def __init__(self, *args,tensorboard_dir=None , **kwargs):\n",
    "        # è°ƒç”¨çˆ¶ç±»Trainerçš„æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–åŸºæœ¬åŠŸèƒ½\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "        # è¿™å°†ä½¿å¾—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŸäº›ä¸­é—´æ¿€æ´»å€¼ä¸è¢«å­˜å‚¨ï¼Œè€Œæ˜¯æ ¹æ®éœ€è¦é‡æ–°è®¡ç®—ï¼Œä»è€ŒèŠ‚çœå†…å­˜\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # åˆ›å»º TensorBoard writerï¼Œç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç±»æŒ‡æ ‡\n",
    "        # é€šè¿‡å½“å‰æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—ç›®å½•\n",
    "        current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')  # è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–\n",
    "        self.writer = SummaryWriter(f'{tensorboard_dir}/training_{current_time}') \n",
    "        \n",
    "    # è‡ªå®šä¹‰çš„æŸå¤±è®¡ç®—æ–¹æ³•\n",
    "    # è¿™ä¸ªæ–¹æ³•ä¼šè¦†ç›–Trainerä¸­çš„compute_lossæ–¹æ³•ï¼Œç”¨äºè®°å½•æŸå¤±åŠå…¶ä»–æŒ‡æ ‡\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # ä½¿ç”¨æ¨¡å‹è®¡ç®—è¾“å‡ºï¼Œä¼ å…¥inputsæ˜¯æ¨¡å‹çš„è¾“å…¥æ•°æ®\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # è·å–æŸå¤±å€¼ï¼Œè¿™é€šå¸¸æ˜¯æ¨¡å‹çš„è¾“å‡ºä¸­çš„ `loss` å­—æ®µ\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # æ¯å½“global_stepè¾¾åˆ°è®°å½•é—´éš”æ—¶ï¼Œè®°å½•ä¿¡æ¯åˆ°TensorBoard\n",
    "        if self.state.global_step % self.args.logging_steps == 0:\n",
    "            try:\n",
    "                # è®°å½•è®­ç»ƒæŸå¤±åˆ°TensorBoardï¼Œåªæœ‰åœ¨æŸå¤±å€¼æ˜¯æœ‰æ•ˆçš„æƒ…å†µä¸‹\n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    self.writer.add_scalar('Training/Loss', \n",
    "                                         loss.item(),  # æŸå¤±çš„å€¼\n",
    "                                         self.state.global_step)  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                \n",
    "                # è®°å½•å½“å‰å­¦ä¹ ç‡åˆ°TensorBoard\n",
    "                self.writer.add_scalar('Training/Learning_Rate',\n",
    "                                     self.optimizer.param_groups[0]['lr'],  # å½“å‰å­¦ä¹ ç‡\n",
    "                                     self.state.global_step)  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                \n",
    "                # è®°å½•è®­ç»ƒè¿›åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œè®¡ç®—è¿›åº¦=å½“å‰æ­¥æ•°/æœ€å¤§æ­¥æ•°*100\n",
    "                self.writer.add_scalar('Training/Progress',\n",
    "                                     self.state.global_step / self.args.max_steps * 100,  # è®¡ç®—è®­ç»ƒè¿›åº¦\n",
    "                                     self.state.global_step)  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                \n",
    "                # è®°å½•æ‰¹é‡å¤§å°ï¼ˆå³æ¯æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°ï¼‰\n",
    "                self.writer.add_scalar('Training/Batch_Size', \n",
    "                                     inputs['input_ids'].size(0),  # è¾“å…¥çš„æ‰¹é‡å¤§å°\n",
    "                                     self.state.global_step)  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                \n",
    "                # å¦‚æœæ”¯æŒGPUï¼Œåˆ™è®°å½•GPUçš„å†…å­˜ä½¿ç”¨æƒ…å†µ\n",
    "                if torch.cuda.is_available():\n",
    "                    # è®°å½•å½“å‰GPUåˆ†é…çš„å†…å­˜ï¼ˆå•ä½MBï¼‰\n",
    "                    self.writer.add_scalar('System/GPU_Memory_Allocated',\n",
    "                                         torch.cuda.memory_allocated() / 1024**2,  # GPUå†…å­˜åˆ†é…é‡\n",
    "                                         self.state.global_step)\n",
    "                    # è®°å½•å½“å‰GPUä¿ç•™çš„å†…å­˜ï¼ˆå•ä½MBï¼‰\n",
    "                    self.writer.add_scalar('System/GPU_Memory_Reserved',\n",
    "                                         torch.cuda.memory_reserved() / 1024**2,  # GPUå†…å­˜ä¿ç•™é‡\n",
    "                                         self.state.global_step)\n",
    "                \n",
    "                # å®‰å…¨åœ°è®°å½•æ¢¯åº¦ç›´æ–¹å›¾ï¼Œæ¯éš”ä¸€å®šæ­¥æ•°è®°å½•ä¸€æ¬¡æ¢¯åº¦ä¿¡æ¯\n",
    "                if self.state.global_step % (self.args.logging_steps * 10) == 0:  # é™ä½è®°å½•é¢‘ç‡\n",
    "                    # éå†æ¨¡å‹ä¸­æ‰€æœ‰çš„å¯è®­ç»ƒå‚æ•°\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.requires_grad and param.grad is not None:\n",
    "                            grad_data = param.grad.data\n",
    "                            # æ£€æŸ¥æ¢¯åº¦æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆå³ä¸ä¸ºç©ºæˆ–å…¨ä¸ºé›¶ï¼‰\n",
    "                            if grad_data.numel() > 0 and not torch.all(grad_data == 0):\n",
    "                                try:\n",
    "                                    # å°†æ¢¯åº¦çš„ç›´æ–¹å›¾è®°å½•åˆ°TensorBoard\n",
    "                                    self.writer.add_histogram(\n",
    "                                        f'Gradients/{name}',  # è®°å½•çš„åç§°ï¼ŒåŒ…æ‹¬å‚æ•°çš„åç§°\n",
    "                                        grad_data.float().cpu().numpy(),  # è½¬æ¢ä¸ºCPUä¸Šçš„floatç±»å‹å¹¶è½¬ä¸ºnumpyæ•°ç»„\n",
    "                                        self.state.global_step,  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                                        bins='auto'  # è‡ªåŠ¨ç¡®å®šç›´æ–¹å›¾çš„binsæ•°é‡\n",
    "                                    )\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Warning: Failed to log histogram for {name}: {e}\")\n",
    "                \n",
    "                # è®¡ç®—æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒæ•°å¹¶è®°å½•åˆ°TensorBoard\n",
    "                total_norm = 0.0  # åˆå§‹åŒ–æ€»èŒƒæ•°\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)  # è®¡ç®—å½“å‰å‚æ•°çš„L2èŒƒæ•°\n",
    "                        total_norm += param_norm.item() ** 2  # ç´¯åŠ æ‰€æœ‰å‚æ•°çš„L2èŒƒæ•°çš„å¹³æ–¹\n",
    "                total_norm = total_norm ** 0.5  # æœ€ç»ˆçš„æ¢¯åº¦èŒƒæ•°æ˜¯æ‰€æœ‰å‚æ•°L2èŒƒæ•°å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹\n",
    "                self.writer.add_scalar('Gradients/total_norm',  # è®°å½•æ€»çš„æ¢¯åº¦èŒƒæ•°\n",
    "                                     total_norm,\n",
    "                                     self.state.global_step)  # å½“å‰çš„è®­ç»ƒæ­¥æ•°\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in logging to tensorboard: {e}\")\n",
    "        \n",
    "        # å¤„ç† NaN/Inf æŸå¤±ï¼Œç¡®ä¿æŸå¤±å€¼æœ‰æ•ˆ\n",
    "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "            print(f\"Warning: NaN/Inf loss detected: {loss.item()}\")\n",
    "            # å°†NaN/InfæŸå¤±å€¼æ›¿æ¢ä¸º0\n",
    "            loss = torch.where(torch.isnan(loss), torch.full_like(loss, 0.0), loss)\n",
    "            loss = torch.where(torch.isinf(loss), torch.full_like(loss, 0.0), loss)\n",
    "    \n",
    "        # è¿”å›è®¡ç®—å‡ºçš„æŸå¤±ï¼Œå¦‚æœéœ€è¦è¿”å›è¾“å‡ºï¼Œåˆ™è¿”å›æŸå¤±å’Œæ¨¡å‹è¾“å‡º\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "    def get_grad_norm(self):\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒæ•° \n",
    "        æ¢¯åº¦èŒƒæ•°æ˜¯è¡¡é‡æ¢¯åº¦å¤§å°çš„ä¸€ç§æ–¹æ³•ï¼Œæœ‰åŠ©äºæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚\n",
    "        \"\"\"\n",
    "        total_norm = 0.0  # åˆå§‹åŒ–æ€»çš„æ¢¯åº¦èŒƒæ•°ä¸º0\n",
    "        # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å‚æ•°\n",
    "        for p in self.model.parameters():\n",
    "            # åªè®¡ç®—éœ€è¦æ¢¯åº¦çš„å‚æ•°ï¼Œå¹¶ä¸”æ£€æŸ¥è¿™äº›å‚æ•°çš„æ¢¯åº¦æ˜¯å¦ä¸ºNone\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                # è®¡ç®—å½“å‰å‚æ•°æ¢¯åº¦çš„2èŒƒæ•°ï¼ˆå³L2èŒƒæ•°ï¼‰\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                # å°†è¯¥å‚æ•°çš„æ¢¯åº¦èŒƒæ•°å¹³æ–¹åŠ åˆ°æ€»èŒƒæ•°ä¸­\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        # è®¡ç®—æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„æ€»èŒƒæ•°çš„å¹³æ–¹æ ¹ï¼Œå³L2èŒƒæ•°\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        return total_norm  # è¿”å›è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦èŒƒæ•°\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"\n",
    "        è¯„ä¼°å›è°ƒå‡½æ•°ï¼Œè®°å½•éªŒè¯é›†çš„è¯„ä¼°æŒ‡æ ‡\n",
    "        åœ¨æ¯æ¬¡è¯„ä¼°ç»“æŸæ—¶ï¼Œä¼šå°†è¯„ä¼°æŒ‡æ ‡è®°å½•åˆ° TensorBoard ä¸­ï¼Œä»¥ä¾¿åç»­åˆ†æã€‚\n",
    "        \"\"\"\n",
    "        if metrics:\n",
    "            # å¦‚æœmetricsä¸ä¸ºç©ºï¼Œéå†æ‰€æœ‰æŒ‡æ ‡\n",
    "            for key, value in metrics.items():\n",
    "                # è®°å½•æ¯ä¸ªè¯„ä¼°æŒ‡æ ‡åˆ°TensorBoardï¼Œå…¶ä¸­ 'Eval' æ˜¯è¯„ä¼°é˜¶æ®µçš„å‰ç¼€\n",
    "                self.writer.add_scalar(f'Eval/{key}', value, state.global_step)\n",
    "\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å…³é—­ `SummaryWriter`ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å·²ç»å†™å…¥å¹¶ä¿å­˜ã€‚\n",
    "        \"\"\"\n",
    "        self.writer.close()  # å…³é—­TensorBoardå†™å…¥å™¨ï¼Œé‡Šæ”¾èµ„æº\n",
    "\n",
    "# è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒçš„å‡½æ•°\n",
    "def setup_distributed():\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))  # è·å–åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ä¸–ç•Œå¤§å°ï¼ˆæ€»çš„è®­ç»ƒè¿›ç¨‹æ•°ï¼‰\n",
    "    \n",
    "    if local_rank != -1: \n",
    "        torch.cuda.set_device(local_rank)  # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPUè®¾å¤‡\n",
    "        dist.init_process_group(backend=\"nccl\")  # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œä½¿ç”¨NCCLä½œä¸ºé€šä¿¡åç«¯\n",
    "\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼\n",
    "    # è¯¥å‡½æ•°ä» instructionã€input å’Œ output å­—æ®µæ„å»ºä¸€ä¸ªå®Œæ•´çš„æç¤ºè¯­ï¼ˆpromptï¼‰ã€‚å¦‚æœ input å­˜åœ¨ï¼Œå®ƒåŒ…å« inputï¼Œå¦åˆ™åªåŒ…å« instruction å’Œ outputã€‚\n",
    "    # ä½¿ç”¨tokenizer å¯¹æç¤ºè¯­è¿›è¡Œç¼–ç ï¼Œè½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„ input_idsï¼ˆtoken idsï¼‰ï¼Œå¹¶è¿›è¡Œå¡«å……æˆ–æˆªæ–­ã€‚\n",
    "    # æ„å»ºlabelsï¼ˆå³æ ‡ç­¾ï¼‰ï¼Œå°†pad_token_idæ›¿æ¢ä¸º-100ï¼Œè¡¨ç¤ºè¿™äº›éƒ¨åˆ†ä¸å‚ä¸æŸå¤±è®¡ç®—ã€‚\n",
    "def preprocess_function(examples, tokenizer, max_length=512):    \n",
    "    prompts = []  # å­˜å‚¨æ„å»ºå¥½çš„æ‰€æœ‰promptï¼ˆæ¨¡å‹è¾“å…¥æ ¼å¼ï¼‰\n",
    "    \n",
    "    # å°†æ¯ä¸€æ¡æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼\n",
    "    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        # å¦‚æœè¾“å…¥æ–‡æœ¬å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œæ„å»ºå¸¦è¾“å…¥çš„å®Œæ•´æç¤ºè¯­\n",
    "        if input_text and input_text.strip():\n",
    "            prompt = f\"Input: {input_text}\\nInstruction: {instruction}\\nOutput: {output}\"  # è¾“å…¥å­˜åœ¨æ—¶ï¼Œæ„å»ºå®Œæ•´çš„æç¤ºè¯­\n",
    "        else:\n",
    "            # å¦‚æœè¾“å…¥ä¸ºç©ºï¼Œåˆ™åªåŒ…å«æŒ‡ä»¤å’Œè¾“å‡º\n",
    "            prompt = f\"Instruction: {instruction}\\nOutput: {output}\" \n",
    "        \n",
    "        prompts.append(prompt)  # å°†æ„å»ºå¥½çš„promptæ·»åŠ åˆ°promptsåˆ—è¡¨\n",
    "\n",
    "    # ä½¿ç”¨tokenizerå°†promptåˆ—è¡¨è½¬åŒ–ä¸ºæ¨¡å‹è¾“å…¥çš„token idsï¼Œå¹¶è¿›è¡Œå¡«å……ã€æˆªæ–­\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompts,  # å°†æ‰€æœ‰çš„promptsè½¬åŒ–ä¸ºtokens\n",
    "        max_length=max_length,  # æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡éƒ¨åˆ†ä¼šè¢«æˆªæ–­\n",
    "        truncation=True,  # å¦‚æœæ–‡æœ¬è¶…è¿‡max_lengthï¼Œè¿›è¡Œæˆªæ–­\n",
    "        padding='max_length',  # å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
    "        return_tensors=\"pt\"  # è¿”å›PyTorchå¼ é‡\n",
    "    )\n",
    "\n",
    "    # å°†tokenized_inputsçš„input_idsç”¨ä½œæ ‡ç­¾ï¼Œå¹¶å°†pad_token_idæ›¿æ¢ä¸º-100ï¼Œè¡¨ç¤ºè¿™äº›éƒ¨åˆ†ä¸å‚ä¸æŸå¤±è®¡ç®—\n",
    "    labels = tokenized_inputs['input_ids'].clone()  # å…‹éš†input_idsï¼Œä½œä¸ºæ ‡ç­¾\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # å°†pad_token_idæ›¿æ¢ä¸º-100ï¼Œä»¥é˜²è®¡ç®—æŸå¤±æ—¶è¢«è€ƒè™‘\n",
    "    tokenized_inputs['labels'] = labels  # å°†æ ‡ç­¾æ·»åŠ åˆ°tokenized_inputså­—å…¸ä¸­\n",
    "    \n",
    "    return tokenized_inputs  # è¿”å›å¤„ç†åçš„tokenizedè¾“å…¥ï¼ˆåŒ…æ‹¬input_ids, attention_mask, labelsï¼‰\n",
    "\n",
    "\n",
    "# å°†JSONæ ¼å¼æ•°æ®è½¬æ¢ä¸ºDatasetæ ¼å¼å¹¶è¿›è¡Œé¢„å¤„ç†\n",
    "    # æ•°æ®åŠ è½½ï¼šé€šè¿‡è¯»å–JSONæ–‡ä»¶ï¼Œå°†å…¶è½¬æ¢ä¸ºPythonå­—å…¸ã€‚\n",
    "    # æ•°æ®æ¸…æ´—ï¼šè¿‡æ»¤æ‰æ²¡æœ‰æœ‰æ•ˆ instruction æˆ– output çš„æ ·æœ¬ï¼Œç¡®ä¿è¿™äº›å­—æ®µä¸ºæœ‰æ•ˆæ–‡æœ¬ã€‚\n",
    "    # è½¬æ¢ä¸ºDatasetï¼šé€šè¿‡ Dataset.from_pandas å°†æ¸…æ´—åçš„æ•°æ®è½¬æ¢ä¸ºHugging Faceçš„ Dataset æ ¼å¼ã€‚\n",
    "    # æ•°æ®é¢„å¤„ç†ï¼šä½¿ç”¨ map å‡½æ•°å°† preprocess_function åº”ç”¨äºæ¯ä¸ªæ•°æ®é¡¹ï¼Œç”Ÿæˆ input_idsã€attention_mask å’Œ labelsã€‚\n",
    "    # ä¿å­˜æ•°æ®é›†ï¼šå°†å¤„ç†å¥½çš„æ•°æ®é›†ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨\n",
    "def convert_json_to_dataset(json_file_path, save_directory, tokenizer):\n",
    "    \n",
    "    # æ‰“å¼€JSONæ–‡ä»¶å¹¶åŠ è½½æ•°æ®\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # åŠ è½½JSONæ–‡ä»¶å†…å®¹\n",
    "    \n",
    "    # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿æ¯ä¸€é¡¹çš„æ•°æ®ä¸­'input'å’Œ'output'å­—æ®µä¸ä¸ºç©º\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        # æ£€æŸ¥instructionå’Œoutputæ˜¯å¦ä¸ºç©ºï¼ˆä¹Ÿå»é™¤ç©ºæ ¼ï¼‰\n",
    "        if all(isinstance(v, str) and len(v.strip()) > 0 for v in [item['instruction'], item['output']]):\n",
    "            cleaned_data.append(item)  # å¦‚æœæœ‰æ•ˆï¼Œåˆ™æ·»åŠ åˆ°cleaned_dataä¸­\n",
    "    \n",
    "    # å°†æ¸…æ´—åçš„æ•°æ®è½¬æ¢ä¸ºDatasetæ ¼å¼\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(cleaned_data))  # ä½¿ç”¨pandas DataFrameæ ¼å¼è½¬æ¢ä¸ºDatasetå¯¹è±¡\n",
    "    \n",
    "    # ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œæ¯ä¸€æ¡æ•°æ®ä¼šé€šè¿‡preprocess_functionè¿›è¡Œå¤„ç†\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),  # è°ƒç”¨é¢„å¤„ç†å‡½æ•°\n",
    "        batched=True,  # ä¸€æ¬¡å¤„ç†å¤šä¸ªæ ·æœ¬\n",
    "        remove_columns=dataset.column_names  # å»æ‰åŸå§‹çš„åˆ—ï¼Œä¿ç•™å¤„ç†åçš„æ•°æ®\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®æ•°æ®æ ¼å¼ä¸ºtorchï¼Œä¿ç•™input_ids, attention_maskå’Œlabels\n",
    "    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†åˆ°æŒ‡å®šç›®å½•\n",
    "    if not os.path.exists(save_directory):  # å¦‚æœä¿å­˜ç›®å½•ä¸å­˜åœ¨\n",
    "        os.makedirs(save_directory)  # åˆ›å»ºç›®å½•\n",
    "    processed_dataset.save_to_disk(save_directory)  # å°†æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜\n",
    "    \n",
    "    return processed_dataset  # è¿”å›å¤„ç†å¥½çš„æ•°æ®é›†\n",
    "\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨çš„å‡½æ•° åšäº†ä»¥ä¸‹äº‹æƒ…ï¼š\n",
    "# åŠ è½½æŒ‡å®šè·¯å¾„æˆ–åç§°çš„åˆ†è¯å™¨ã€‚\n",
    "# é…ç½®æ¨¡å‹åŠ è½½åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆæ”¯æŒå•å¡å’Œåˆ†å¸ƒå¼è®­ç»ƒï¼‰ã€‚\n",
    "# ä½¿ç”¨ä½ç²¾åº¦ï¼ˆå¦‚ 8bit å’Œ float16ï¼‰åŠ è½½æ¨¡å‹ï¼Œä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚\n",
    "# è°ƒç”¨ prepare_model_for_kbit_training å‡½æ•°ï¼Œä¸ºä½ä½è®­ç»ƒåšå¥½å‡†å¤‡ã€‚\n",
    "# è¿”å›åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚\n",
    "def prepare_model_and_tokenizer(model_path, local_rank):\n",
    "    \n",
    "    # ä½¿ç”¨AutoTokenizerä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,  # æŒ‡å®šæ¨¡å‹è·¯å¾„æˆ–æ¨¡å‹åç§°\n",
    "        trust_remote_code=True  # å…è®¸ä»è¿œç¨‹åŠ è½½è‡ªå®šä¹‰ä»£ç ï¼ˆå¦‚è‡ªå®šä¹‰åˆ†è¯å™¨ï¼‰\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®æ¨¡å‹è®¾å¤‡æ˜ å°„ï¼Œç¡®ä¿æ¨¡å‹åŠ è½½åˆ°æ­£ç¡®çš„è®¾å¤‡\n",
    "    if local_rank == -1:\n",
    "        # å¦‚æœä¸è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆ™å°†æ¨¡å‹åŠ è½½åˆ°å½“å‰GPUè®¾å¤‡\n",
    "        device_map = {'': torch.cuda.current_device()}  # é»˜è®¤è®¾å¤‡ï¼ˆå½“å‰GPUï¼‰\n",
    "    else:\n",
    "        # å¦‚æœè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆ™æ ¹æ®local_rankæŒ‡å®šåŠ è½½åˆ°ç‰¹å®šGPU\n",
    "        device_map = {'': local_rank}  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­æŒ‡å®šè®¾å¤‡ID\n",
    "    \n",
    "    # åŠ è½½é¢„è®­ç»ƒçš„Causal LMæ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,  # æ¨¡å‹è·¯å¾„æˆ–åç§°\n",
    "        trust_remote_code=True,  # å…è®¸ä»è¿œç¨‹åŠ è½½è‡ªå®šä¹‰ä»£ç ï¼ˆå¦‚è‡ªå®šä¹‰æ¨¡å‹é…ç½®ï¼‰\n",
    "        load_in_8bit=True,  # ä½¿ç”¨8ä½ç²¾åº¦è¿›è¡ŒåŠ è½½ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—\n",
    "        torch_dtype=torch.float16,  # ä½¿ç”¨float16ç²¾åº¦ï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜\n",
    "        device_map=device_map  # ä½¿ç”¨æŒ‡å®šçš„è®¾å¤‡æ˜ å°„\n",
    "    )\n",
    "    \n",
    "    # å‡†å¤‡æ¨¡å‹è¿›è¡Œä½ä½è®­ç»ƒï¼ˆk-bit trainingï¼‰\n",
    "    # ä½ä½è®­ç»ƒé€šå¸¸ç”¨äºå‡å°‘æ¨¡å‹çš„å­˜å‚¨éœ€æ±‚ï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚\n",
    "    # ä½ä½è®­ç»ƒï¼šåœ¨è®­ç»ƒä¸­ä½¿ç”¨ä½ç²¾åº¦çš„æƒé‡ï¼ˆå¦‚ 4bit æˆ– 8bitï¼‰æ¥å‡å°‘å†…å­˜å ç”¨\n",
    "    model = prepare_model_for_kbit_training(model) \n",
    "    \n",
    "    # è¿”å›åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "    return model, tokenizer  \n",
    "\n",
    "# ä¸»å‡½æ•°ï¼Œæ‰§è¡Œè®­ç»ƒæµç¨‹\n",
    "    \"\"\"\n",
    "    1.è§£æå‘½ä»¤è¡Œå‚æ•°ï¼š\n",
    "        é€šè¿‡ ArgumentParser è§£æç”¨æˆ·æä¾›çš„å‘½ä»¤è¡Œå‚æ•°ã€‚è¿™äº›å‚æ•°åŒ…æ‹¬æ¨¡å‹è·¯å¾„ã€æ•°æ®è·¯å¾„ã€è®­ç»ƒè¶…å‚æ•°ã€åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ç­‰ã€‚\n",
    "    2.è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒï¼š\n",
    "        æ ¹æ® local_rank è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒã€‚å¦‚æœå¯ç”¨äº†åˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆ™ä¼šåœ¨å¤šä¸ªè®¾å¤‡ï¼ˆGPUï¼‰ä¹‹é—´åˆ†é…ä»»åŠ¡ã€‚is_main_process ç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹é€šå¸¸è´Ÿè´£ä¿å­˜æ¨¡å‹ã€‚\n",
    "    3.å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š\n",
    "        ä½¿ç”¨ prepare_model_and_tokenizer å‡½æ•°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå¹¶æ ¹æ® local_rank è®¾ç½®åˆé€‚çš„è®¾å¤‡æ˜ å°„ã€‚\n",
    "    4.é…ç½®è®­ç»ƒå‚æ•°ï¼š\n",
    "        è®¾ç½®è®­ç»ƒå‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€æœ€å¤§è®­ç»ƒæ­¥æ•°ç­‰ï¼Œæ­¤å¤–è¿˜é…ç½®äº†æ—¥å¿—è®°å½•ã€ä¿å­˜æ¨¡å‹ç­‰é€‰é¡¹ã€‚\n",
    "    5.é…ç½®LoRAï¼š\n",
    "        é…ç½®å¹¶åº”ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰åˆ°æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿è¿›è¡Œä½ç§©å¾®è°ƒã€‚\n",
    "    6.åŠ è½½æˆ–åˆ›å»ºæ•°æ®é›†ï¼š\n",
    "        æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨ï¼ˆä¿å­˜åœ¨ save_directory ä¸­ï¼‰ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä»æä¾›çš„ JSON æ–‡ä»¶ä¸­ç”Ÿæˆæ•°æ®é›†ã€‚\n",
    "    7.è®­ç»ƒï¼š\n",
    "        ä½¿ç”¨è‡ªå®šä¹‰çš„ CustomTrainer ç±»æ¥æ‰§è¡Œæ¨¡å‹è®­ç»ƒã€‚è¿™ä¸ªç±»é›†æˆäº†å¸¸è§çš„è®­ç»ƒä»»åŠ¡ï¼Œä¾‹å¦‚æŸå¤±è®¡ç®—ã€æ—¥å¿—è®°å½•ã€æ¢¯åº¦è®¡ç®—ç­‰ã€‚\n",
    "    8.ä¿å­˜æ¨¡å‹ï¼š\n",
    "        è®­ç»ƒå®Œæˆåï¼Œä¸»è¿›ç¨‹ä¼šä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚\n",
    "    9.æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒï¼š\n",
    "        åœ¨åˆ†å¸ƒå¼è®­ç»ƒç»“æŸåï¼Œæ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„ã€‚\n",
    "    \"\"\"\n",
    "def main():\n",
    "    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œè·å–ç”¨æˆ·è¾“å…¥çš„è¶…å‚æ•°å’Œè·¯å¾„é…ç½®\n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    # æ¨¡å‹è·¯å¾„å’Œæ•°æ®è·¯å¾„å‚æ•°\n",
    "    parser.add_argument(\"--model_path\", type=str, default=None, help=\"æ¨¡å‹è·¯å¾„\")\n",
    "    parser.add_argument(\"--json_file_path\", type=str, default=None, help=\"æ•°æ®é›†è·¯å¾„\")\n",
    "    parser.add_argument(\"--save_directory\", type=str, default=\"./data/train_data_distribute\", help=\"æ•°æ®ä¿å­˜è·¯å¾„\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./output\", help=\"è®­ç»ƒè¾“å‡ºè·¯å¾„\")\n",
    "    parser.add_argument(\"--log_path\", type=str, default=\"./logs\", help=\"æ—¥å¿—è¾“å‡ºè·¯å¾„\")\n",
    "    \n",
    "    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1,help=\"å¯ä»¥è‡ªè¡Œé€‰æ‹©æ˜¯å¦ä¼ å‚ï¼Œä¹Ÿå¯ä»¥å¤–éƒ¨CUDAæ§åˆ¶\")\n",
    "    \n",
    "    # è®­ç»ƒè¶…å‚æ•°\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8, help=\"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-6, help=\"å­¦ä¹ ç‡\")\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=50000, help=\"è®­ç»ƒçš„æœ€å¤§æ­¥æ•°å»ºè®®è®¾ç½®å¤§ä¸€ç‚¹\")\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100, help=\"é¢„çƒ­æ­¥éª¤æ•°\")\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=10, help=\"è®°å½•æ—¥å¿—çš„æ­¥éª¤æ•°\")\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=100, help=\"ä¿å­˜æ¨¡å‹çš„æ­¥éª¤æ•°\")\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10, help=\"è®­ç»ƒè½®æ•°\")\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=0.5, help=\"æœ€å¤§æ¢¯åº¦è£å‰ªå€¼\")\n",
    "    parser.add_argument(\"--fp16\", type=bool, default=True, help=\"é€‰æ‹©ä½æ•°\")\n",
    "    \n",
    "    # LoRAé…ç½®\n",
    "    parser.add_argument(\"--rank\", type=int, default=8, help=\"LoRAé…ç½®ä¸­çš„rå€¼\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32, help=\"LoRAé…ç½®ä¸­çš„alphaå€¼\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05, help=\"LoRAé…ç½®ä¸­çš„dropoutå€¼\")\n",
    "    parser.add_argument(\"--tensorboard_dir\", type=str, default=\"tensorboard\", help=\"TensorBoard æ—¥å¿—ç›®å½•\")\n",
    "    \n",
    "    # è§£æå‘½ä»¤è¡Œå‚æ•°\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ç¡®ä¿ TensorBoard æ—¥å¿—ç›®å½•å­˜åœ¨\n",
    "    os.makedirs(args.tensorboard_dir, exist_ok=True)  # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "    \n",
    "    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "    local_rank, world_size = setup_distributed()  # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒçš„ç¯å¢ƒå˜é‡\n",
    "    is_main_process = local_rank in [-1, 0]  # åˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹\n",
    "\n",
    "    # å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "    model, tokenizer = prepare_model_and_tokenizer(args.model_path, local_rank)  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "\n",
    "    # é…ç½®è®­ç»ƒå‚æ•°\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.batch_size,  # æ¯ä¸ªGPUçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "        learning_rate=args.learning_rate,  # å­¦ä¹ ç‡\n",
    "        fp16=args.fp16,  # æ˜¯å¦ä½¿ç”¨16ä½ç²¾åº¦è®­ç»ƒ\n",
    "        logging_steps=args.logging_steps,  # æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”\n",
    "        save_steps=args.save_steps,  # æ¨¡å‹ä¿å­˜çš„æ­¥æ•°é—´éš”\n",
    "        max_steps=args.max_steps,  # æœ€å¤§è®­ç»ƒæ­¥æ•°\n",
    "        warmup_steps=args.warmup_steps,  # é¢„çƒ­æ­¥æ•°\n",
    "        logging_dir=args.log_path,  # TensorBoardæ—¥å¿—è·¯å¾„\n",
    "        num_train_epochs=args.train_epochs,  # è®­ç»ƒè½®æ•°\n",
    "        save_strategy=\"epoch\",  # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "        save_total_limit=3,  # æœ€å¤§ä¿å­˜æ¨¡å‹çš„æ•°é‡\n",
    "        remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—\n",
    "        local_rank=local_rank,  # åˆ†å¸ƒå¼è®­ç»ƒä¸­ä½¿ç”¨çš„local rank\n",
    "        ddp_backend=\"nccl\",  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨çš„åç«¯\n",
    "        dataloader_num_workers=2,  # æ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°\n",
    "        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå‡å°‘æ˜¾å­˜æ¶ˆè€—\n",
    "        max_grad_norm=args.max_grad_norm,  # æ¢¯åº¦è£å‰ªçš„æœ€å¤§å€¼\n",
    "        ddp_find_unused_parameters=False,  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œä¸æŸ¥æ‰¾æœªä½¿ç”¨çš„å‚æ•°\n",
    "        report_to=\"tensorboard\"  # å¯ç”¨TensorBoardæŠ¥å‘Š\n",
    "    )\n",
    "\n",
    "    # é…ç½®LoRAï¼ˆLow-Rank Adaptationï¼‰è®¾ç½®\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\n",
    "        r=args.rank,  # LoRAçš„rankå€¼\n",
    "        lora_alpha=args.lora_alpha,  # LoRAçš„alphaå€¼\n",
    "        lora_dropout=args.lora_dropout,  # LoRAçš„dropoutå€¼\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # é€‚ç”¨äºè¿™äº›æ¨¡å—çš„LoRA\n",
    "        bias=\"none\",  # ä¸ä½¿ç”¨åç½®\n",
    "        inference_mode=False  # éæ¨ç†æ¨¡å¼ï¼Œè¡¨ç¤ºç”¨äºè®­ç»ƒ\n",
    "    )\n",
    "\n",
    "    # å°†LoRAé…ç½®åº”ç”¨åˆ°æ¨¡å‹ä¸­\n",
    "    model = get_peft_model(model, lora_config)  # è·å–å¸¦æœ‰LoRAé…ç½®çš„æ¨¡å‹\n",
    "\n",
    "    # åŠ è½½æ•°æ®é›†ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ç›´æ¥åŠ è½½ï¼Œå¦åˆ™ä»JSONæ–‡ä»¶åˆ›å»º\n",
    "    if os.path.exists(args.save_directory):\n",
    "        dataset = Dataset.load_from_disk(args.save_directory)  # åŠ è½½å·²ä¿å­˜çš„æ•°æ®é›†\n",
    "    else:\n",
    "        dataset = convert_json_to_dataset(args.json_file_path, args.save_directory, tokenizer)  # ä»JSONæ–‡ä»¶è½¬æ¢ä¸ºDatasetæ ¼å¼\n",
    "\n",
    "    # ä½¿ç”¨è‡ªå®šä¹‰Trainerè¿›è¡Œè®­ç»ƒ\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,  # ä¼ å…¥æ¨¡å‹\n",
    "        args=training_args,  # ä¼ å…¥è®­ç»ƒå‚æ•°\n",
    "        train_dataset=dataset,  # ä¼ å…¥è®­ç»ƒæ•°æ®é›†\n",
    "        tensorboard_dir=args.tensorboard_dir,  # TensorBoardæ—¥å¿—ç›®å½•\n",
    "        data_collator=DataCollatorForSeq2Seq(\n",
    "            tokenizer,  # ä½¿ç”¨çš„tokenizer\n",
    "            pad_to_multiple_of=8,  # å¡«å……åˆ°8çš„å€æ•°\n",
    "            return_tensors=\"pt\",  # è¿”å›PyTorchå¼ é‡\n",
    "            padding=True  # å¯ç”¨å¡«å……\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    trainer.train()\n",
    "\n",
    "    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹\n",
    "    if is_main_process:\n",
    "        trainer.save_model(f\"{args.output_dir}/final_model\")  # ä¿å­˜æœ€ç»ˆçš„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ\n",
    "    if local_rank != -1:\n",
    "        dist.destroy_process_group()  # é”€æ¯åˆ†å¸ƒå¼è®­ç»ƒçš„è¿›ç¨‹ç»„\n",
    "\n",
    "# ç¨‹åºå…¥å£ï¼Œè°ƒç”¨ä¸»å‡½æ•°\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¯åŠ¨è®­ç»ƒæµç¨‹\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ac003",
   "metadata": {},
   "source": [
    "æ‰§è¡Œå‘½ä»¤å‚æ•°ï¼š\n",
    "\n",
    "è„šæœ¬å‘½åä¸ºtrain_distributed.sh \n",
    "```bash\n",
    "#!/bin/bash\n",
    "# æŒ‡å®šä½¿ç”¨ bash ä½œä¸ºè„šæœ¬çš„è§£é‡Šå™¨\n",
    "\n",
    "# è·å–å½“å‰ç³»ç»Ÿä¸­å¯ç”¨çš„ GPU æ•°é‡\n",
    "NUM_GPUS=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader | wc -l)\n",
    "# è§£é‡Šï¼šé€šè¿‡ `nvidia-smi` å‘½ä»¤æŸ¥è¯¢æ‰€æœ‰ GPU çš„åç§°ï¼Œç„¶åä½¿ç”¨ `wc -l` ç»Ÿè®¡ GPU çš„æ•°é‡\n",
    "# è¿™ä¸ªæ•°é‡ä¼šèµ‹å€¼ç»™å˜é‡ NUM_GPUSï¼Œç”¨äºåç»­æŒ‡å®šè®­ç»ƒæ—¶ä½¿ç”¨çš„ GPU æ•°é‡\n",
    "\n",
    "# å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŒ‡å®šè¦ä½¿ç”¨çš„æ˜¾å¡\n",
    "# export CUDA_VISIBLE_DEVICES=\"0,1\" \n",
    "# è¿™ä¸ªå‘½ä»¤å…è®¸æŒ‡å®šå“ªäº› GPU è¢«ç”¨æ¥è®­ç»ƒï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ GPU 0 å’Œ GPU 1ï¼‰\n",
    "\n",
    "# ä½¿ç”¨ `torchrun` å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "# `torchrun` æ˜¯ PyTorch ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„å·¥å…·ï¼Œå¯ä»¥è‡ªåŠ¨ç®¡ç†å¤šå¡è®­ç»ƒ\n",
    "\n",
    "  # --nproc_per_node=$NUM_GPUSï¼šæŒ‡å®šæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œ$NUM_GPUS æ˜¯ä¹‹å‰è·å–çš„ GPU æ•°é‡\n",
    "  # --nnodes=1ï¼šè¡¨ç¤ºä½¿ç”¨ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œè®­ç»ƒ\n",
    "  # --node_rank=0ï¼šæŒ‡å®šå½“å‰èŠ‚ç‚¹çš„ rankï¼ˆç¼–å·ï¼‰ï¼Œä¸»èŠ‚ç‚¹é€šå¸¸æ˜¯ 0\n",
    "  # æŒ‡å®šè®­ç»ƒæ—¶ä½¿ç”¨çš„å„ä¸ªå‚æ•°\n",
    "torchrun --nproc_per_node=$NUM_GPUS  --nnodes=1  --node_rank=0 train_distributed.py \\\n",
    "--model_path \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\" \\\n",
    "--json_file_path \"/home/data/muyan/code/data/medicalQA_top1k.json\" \\\n",
    "--save_directory \"./data/train_data_distribute_20241224\" \\\n",
    "--output_dir \"./output_20241224\" \\\n",
    "--log_path \"./log_20241224\" \\\n",
    "--tensorboard_dir \"tensorboard_dir\" \\\n",
    "--batch_size 2 \\\n",
    "--train_epochs 10 \\\n",
    "--fp16 True \\\n",
    "--gradient_accumulation_steps 8 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--max_steps -1 \\\n",
    "--warmup_steps 0 \\\n",
    "--logging_steps 10 \\\n",
    "--save_steps 100 \\\n",
    "--max_grad_norm 0.5 \\\n",
    "--rank 8 \\\n",
    "--lora_alpha 16 \\\n",
    "--lora_dropout 0.05\n",
    "\n",
    "# ä»¥ä¸Šå‘½ä»¤åœ¨ä½¿ç”¨æ—¶æ³¨æ„æ ¼å¼ æ¯”å¦‚ `\\`åä¸èƒ½æœ‰ç©ºæ ¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´å‘½ä»¤æ— æ³•ä¼ å…¥ç¨‹åº\n",
    "# å½“å‰å‚æ•°åœ¨ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å®é™…æ•°æ®é›†æ•ˆæœè‡ªè¡Œè¿›è¡Œè°ƒæ•´\n",
    "# torchrun --nproc_per_node=$NUM_GPUS  --nnodes=1  --node_rank=0 train_distributed.py \\\n",
    "# --model_path \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\" \\  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„\n",
    "# --json_file_path \"/home/data/muyan/code/data/medicalQA_top1k.json\" \\  # è®­ç»ƒæ•°æ®é›†è·¯å¾„\n",
    "# --save_directory \"./data/train_data_distribute_20241224\" \\  # æ•°æ®ä¿å­˜ç›®å½•\n",
    "# --output_dir \"./output_20241224\" \\  # æ¨¡å‹è¾“å‡ºç»“æœä¿å­˜ç›®å½•\n",
    "# --log_path \"./log_20241224\" \\  # æ—¥å¿—æ–‡ä»¶ä¿å­˜è·¯å¾„\n",
    "# --tensorboard_dir \"tensorboard_dir\" \\ # æŒ‡å®šä¸ºå½“å‰è·¯å¾„ä¸‹ tensorboard_diræ–‡ä»¶å¤¹\n",
    "# --batch_size 2 \\  # æ¯ä¸ª GPU ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "# --train_epochs 100 \\  # è®­ç»ƒçš„è½®æ•°\n",
    "# --fp16 True \\  # å¯ç”¨ 16 ä½ç²¾åº¦è®­ç»ƒï¼ˆå‡å°‘æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®­ç»ƒï¼‰\n",
    "# --gradient_accumulation_steps 8 \\  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¯ 8 æ­¥è¿›è¡Œä¸€æ¬¡æ¢¯åº¦æ›´æ–°ï¼‰\n",
    "# --learning_rate 5e-5 \\  # å­¦ä¹ ç‡\n",
    "# --max_steps -1 \\  # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œè®¾ç½®ä¸º -1 è¡¨ç¤ºä½¿ç”¨ `train_epochs` æ§åˆ¶è®­ç»ƒè½®æ•°\n",
    "# --warmup_steps 0 \\  # å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°ï¼Œè®¾ç½®ä¸º 0 è¡¨ç¤ºä¸ä½¿ç”¨é¢„çƒ­\n",
    "# --logging_steps 10 \\  # æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—\n",
    "# --save_steps 100 \\  # æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "# --max_grad_norm 0.5 \\  # æœ€å¤§æ¢¯åº¦è£å‰ªå€¼ï¼Œç”¨äºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "# --rank 8 \\  # LoRA é…ç½®ä¸­çš„ r å€¼ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µçš„å¤§å°\n",
    "# --lora_alpha 16 \\  # LoRA é…ç½®ä¸­çš„ alpha å€¼ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µçš„ç¼©æ”¾\n",
    "# --lora_dropout 0.05  # LoRA é…ç½®ä¸­çš„ dropout å€¼ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aac31d",
   "metadata": {},
   "source": [
    "æ‰§è¡Œå‘½ä»¤ï¼š\n",
    "\n",
    "./train_distributed.sh\n",
    "\n",
    "å¦‚æœæƒé™ä¸è¶³è¯·æ‰§è¡Œï¼šchmod +x train_distributed.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ea6f4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224221355643.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984db0c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224221853942.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533b149",
   "metadata": {},
   "source": [
    "### è¿‡ç¨‹ç›‘æ§ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc11efb",
   "metadata": {},
   "source": [
    "ç”Ÿæˆä¼ å…¥æ–‡ä»¶æŒ‡å®šè·¯å¾„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9905f5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224222054302.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2078c00",
   "metadata": {},
   "source": [
    "tensorboardå®‰è£…åŠä½¿ç”¨åœ¨ã€ŠCh 2. LLama_Factory+LORAå¤§æ¨¡å‹å¾®è°ƒã€‹å·²ç»è®²è¿‡ï¼Œæœ¬æ¬¡ä¸åœ¨èµ˜è¿°\n",
    "\n",
    "ç›‘æ§å‘½ä»¤(æŒ‡å®šæ–‡ä»¶å¤¹ä¸å½“å‰IPåœ°å€å’Œå¼€æ”¾ç«¯å£å·)ï¼štensorboard --logdir=tensorboard_dir --host 192.168.110.131  --port=6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77461d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224222428722.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4730a16",
   "metadata": {},
   "source": [
    "ç•Œé¢æŸ¥çœ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4ab56",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224222551642.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5c3d4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224222840548.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb8742",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224222943296.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b89e8",
   "metadata": {},
   "source": [
    "åå°ç›‘æ§æ•°æ® \n",
    "\n",
    "å¼ºçƒˆå»ºè®®å¤§å®¶ä½¿ç”¨ nohup å‘½ä»¤å¯åŠ¨æ–‡ä»¶\n",
    "\n",
    "nohup ./train_distributed.sh >train_distributed_12242315.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1809fad",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224231601852.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800dbb5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224231700295.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6379e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224223156093.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b2bc0",
   "metadata": {},
   "source": [
    "è¿›å…¥æŒ‡å®šè·¯å¾„æŸ¥çœ‹ç”Ÿæˆæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5ef09",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224223650177.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2e68a",
   "metadata": {},
   "source": [
    "ç”Ÿæˆæ•°æ®çš„åŸºæœ¬ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7990c8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224223834722.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3122f38",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224223941145.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f31a3e",
   "metadata": {},
   "source": [
    "| Data format | Function                |\n",
    "|-------------|-------------------------|\n",
    "| Arrow       | Dataset.save_to_disk()  |\n",
    "| CSV         | Dataset.to_csv()        |\n",
    "| JSON        | Dataset.to_json()       |\n",
    "\n",
    "Apache Arrowæ˜¯ä¸€ç§ç”¨äºé«˜æ•ˆå­˜å‚¨å’Œäº¤æ¢è¡¨æ ¼æ•°æ®çš„åˆ—å¼å­˜å‚¨æ ¼å¼\n",
    "\n",
    "Arrowæ ¼å¼ä¸“ä¸ºé«˜æ€§èƒ½è®¾è®¡ï¼Œæ”¯æŒé›¶æ‹·è´è¯»å–å’Œå†™å…¥ï¼Œè¿™å¯ä»¥æ˜¾è‘—æé«˜æ•°æ®çš„å¤„ç†é€Ÿåº¦\n",
    "\n",
    "Arrowæ ¼å¼ä½¿ç”¨åˆ—å¼å­˜å‚¨ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å†…å­˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122db279",
   "metadata": {},
   "source": [
    "ç”Ÿæˆæ—¥å¿—æŸ¥çœ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608aacc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224224355743.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58058835",
   "metadata": {},
   "source": [
    "æ¨¡å‹ä¿å­˜ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8dac7f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224224600642.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc9aff",
   "metadata": {},
   "source": [
    "æŒ‡å®štensorboard æ–‡ä»¶è·¯å¾„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b619017",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241224224921036.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce6079",
   "metadata": {},
   "source": [
    "æ˜¾å­˜å ç”¨ç›‘æ§ï¼ˆå½“å‰æ˜¯æŒ‡å®šä½¿ç”¨ä¸¤å¼ å¡è¿›è¡Œå¾®è°ƒ0,1 ä¸æŒ‡å®šæŸä¸€å¼ å¡åˆ™ä¼šä½¿ç”¨å…¨éƒ¨çš„å¡è¿›è¡Œå¾®è°ƒï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c33a0c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241223172302664.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391510f2",
   "metadata": {},
   "source": [
    "- **step3 æ£€éªŒæ¨¡å‹è¾“å‡º**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170e007",
   "metadata": {},
   "source": [
    "### åŠ è½½æ–¹å¼éªŒè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0192e0",
   "metadata": {},
   "source": [
    "å‘½åä¸ºlora_adapter_chat.py\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import warnings\n",
    "\n",
    "# å…³é—­æ‰€æœ‰è­¦å‘Šï¼Œé¿å…å¹²æ‰°è¾“å‡º\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, base_model_path, lora_model_path):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–èŠå¤©æœºå™¨äººï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAæ¨¡å‹ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        base_model_path (str): åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è·¯å¾„ã€‚\n",
    "        lora_model_path (str): LoRAæ¨¡å‹è·¯å¾„ï¼Œç”¨äºå¾®è°ƒã€‚\n",
    "        \"\"\"\n",
    "        print(\"æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n",
    "        \n",
    "        # åŠ è½½tokenizerï¼šç”¨äºå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„tokensï¼ˆæ ‡è®°ï¼‰ï¼Œå¹¶å°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºå¯è¯»çš„æ–‡æœ¬ã€‚\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path,  # åŸºç¡€æ¨¡å‹è·¯å¾„\n",
    "            trust_remote_code=True  # å…è®¸åŠ è½½è¿œç¨‹çš„ä»£ç \n",
    "        )\n",
    "        \n",
    "        # åŠ è½½åŸºç¡€æ¨¡å‹ï¼šåŠ è½½é¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            trust_remote_code=True,  # å…è®¸åŠ è½½è¿œç¨‹çš„ä»£ç \n",
    "            device_map=\"auto\",  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆå¦‚GPU/CPUï¼‰\n",
    "            torch_dtype=torch.float16  # ä½¿ç”¨16ä½ç²¾åº¦æ¥å‡å°‘å†…å­˜å ç”¨\n",
    "        )\n",
    "        \n",
    "        # åŠ è½½LoRAæ¨¡å‹ï¼šåŸºäºLoRAå¾®è°ƒçš„æ¨¡å‹ï¼Œç”¨äºå¢å¼ºç”Ÿæˆèƒ½åŠ›ã€‚\n",
    "        self.model = PeftModel.from_pretrained(\n",
    "            self.base_model,  # åŸºç¡€æ¨¡å‹\n",
    "            lora_model_path,  # LoRAå¾®è°ƒåçš„æ¨¡å‹è·¯å¾„\n",
    "            torch_dtype=torch.float16,  # ä½¿ç”¨16ä½ç²¾åº¦\n",
    "            device_map=\"auto\"  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡\n",
    "        )\n",
    "        \n",
    "        print(\"æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "        \n",
    "    def generate_response(self, instruction, input_text=\"\"):\n",
    "        \"\"\"\n",
    "        æ ¹æ®ç»™å®šçš„æŒ‡ä»¤å’Œè¾“å…¥æ–‡æœ¬ç”Ÿæˆå›å¤ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        instruction (str): èŠå¤©æœºå™¨äººæ‰§è¡Œçš„æŒ‡ä»¤ã€‚\n",
    "        input_text (str, optional): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼Œé»˜è®¤ä¸ºç©ºã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        response (str): ç”Ÿæˆçš„å›å¤ã€‚\n",
    "        \"\"\"\n",
    "        # æ ¹æ®æ˜¯å¦æœ‰è¾“å…¥æ–‡æœ¬æ„å»ºä¸åŒçš„æç¤ºè¯­ï¼ˆpromptï¼‰\n",
    "        if input_text:\n",
    "            prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n",
    "        else:\n",
    "            prompt = f\"Instruction: {instruction}\\nOutput:\"\n",
    "        \n",
    "        # ä½¿ç”¨tokenizerå°†promptè½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}  # å°†è¾“å…¥ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡\n",
    "        \n",
    "        # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼‰\n",
    "        with torch.no_grad():\n",
    "            # ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼ˆå›å¤ï¼‰\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,  # è¾“å…¥çš„token ids\n",
    "                max_new_tokens=512,  # æœ€å¤šç”Ÿæˆ512ä¸ªæ–°tokens\n",
    "                temperature=0.1,  # æ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œè¾ƒä½çš„æ¸©åº¦ä½¿è¾“å‡ºæ›´ç¡®å®šæ€§\n",
    "                top_p=0.1,  # æ§åˆ¶æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "                repetition_penalty=1.1,  # é‡å¤æƒ©ç½šï¼Œé¿å…ç”Ÿæˆé‡å¤æ–‡æœ¬\n",
    "                pad_token_id=self.tokenizer.pad_token_id,  # å¡«å……tokençš„ID\n",
    "                eos_token_id=self.tokenizer.eos_token_id  # ç»“æŸtokençš„ID\n",
    "            )\n",
    "        \n",
    "        # è§£ç ç”Ÿæˆçš„tokensä¸ºæ–‡æœ¬ï¼Œå¹¶å»é™¤ç‰¹æ®Štokens\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # æå–è¾“å‡ºéƒ¨åˆ†çš„å›å¤æ–‡æœ¬ï¼Œå»é™¤\"Output:\"æ ‡è®°\n",
    "        response = response.split(\"Output:\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°ï¼šç”¨äºå¯åŠ¨èŠå¤©æœºå™¨äººï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œå¹¶ç”Ÿæˆå›å¤ã€‚\n",
    "    \"\"\"\n",
    "    # é…ç½®åŸºç¡€æ¨¡å‹è·¯å¾„å’ŒLoRAæ¨¡å‹è·¯å¾„\n",
    "    BASE_MODEL_PATH = \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\"  # æ›¿æ¢ä¸ºä½ çš„åŸºç¡€æ¨¡å‹è·¯å¾„\n",
    "    LORA_MODEL_PATH = \"./output_20241224/final_model\"  # æ›¿æ¢ä¸ºä½ çš„LoRAæ¨¡å‹è·¯å¾„\n",
    "    \n",
    "    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº\n",
    "    chatbot = ChatBot(BASE_MODEL_PATH, LORA_MODEL_PATH)\n",
    "    \n",
    "    # æ¬¢è¿ä¿¡æ¯å’Œè¯´æ˜\n",
    "    print(\"\\n=== æ¬¢è¿ä½¿ç”¨èŠå¤©æœºå™¨äºº ===\")\n",
    "    print(\"è¾“å…¥ 'quit' æˆ– 'exit' ç»“æŸå¯¹è¯\")\n",
    "    print(\"è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²\")\n",
    "    print(\"============================\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # è·å–ç”¨æˆ·è¾“å…¥çš„æŒ‡ä»¤\n",
    "            instruction = input(\"\\nè¯·è¾“å…¥æŒ‡ä»¤: \").strip()\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦é€€å‡ºå¯¹è¯\n",
    "            if instruction.lower() in ['quit', 'exit']:\n",
    "                print(\"\\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼\")\n",
    "                break\n",
    "                \n",
    "            # æ£€æŸ¥æ˜¯å¦æ¸…ç©ºå¯¹è¯å†å²\n",
    "            if instruction.lower() == 'clear':\n",
    "                print(\"\\nå¯¹è¯å†å²å·²æ¸…ç©ºï¼\")\n",
    "                continue\n",
    "                \n",
    "            # è·å–ç”¨æˆ·è¾“å…¥çš„å¯é€‰æ–‡æœ¬\n",
    "            input_text = input(\"è¯·è¾“å…¥è¡¥å……å†…å®¹(å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡): \").strip()\n",
    "            \n",
    "            # ç”Ÿæˆæœºå™¨äººå›å¤\n",
    "            print(\"\\næ­£åœ¨ç”Ÿæˆå›å¤...\")\n",
    "            response = chatbot.generate_response(instruction, input_text)\n",
    "            \n",
    "            # è¾“å‡ºæœºå™¨äººçš„å›å¤\n",
    "            print(\"\\næœºå™¨äººå›å¤:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(response)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\næ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nå‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "            print(\"è¯·å°è¯•é‡æ–°è¾“å…¥æˆ–è¾“å…¥ 'exit' é€€å‡º\")\n",
    "            continue\n",
    "\n",
    "# å¯åŠ¨ç¨‹åºå…¥å£\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb2774",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225104801873.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23809db",
   "metadata": {},
   "source": [
    "- **step4 åˆå¹¶æ¨¡å‹è¾“å‡º**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1f9aa",
   "metadata": {},
   "source": [
    "```python\n",
    "# å½“å‰æ–‡ä»¶ å‘½åä¸ºmerge_model.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. åŠ è½½åŸå§‹åŸºåº§æ¨¡å‹ï¼ˆQwen2.5-3B-Instructï¼‰\n",
    "base_model_path = \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "\n",
    "# 2. åŠ è½½å¾®è°ƒåçš„ LoRA adapterï¼ˆå‡è®¾ä½ ä½¿ç”¨çš„æ˜¯ LoRA åº“ï¼‰\n",
    "adapter_path = \"output_20241224/23/final_model/\"\n",
    "# è¿™é‡Œéœ€è¦æ ¹æ®ä½ ä½¿ç”¨çš„ LoRA å®ç°æ¥åŠ è½½ adapter\n",
    "# ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ LoRA æ¨¡å—å¹¶ä¸”æ˜¯ Hugging Face å®ç°ï¼š\n",
    "from peft import PeftModel  # è¿™é‡Œå‡è®¾ä½ ä½¿ç”¨çš„æ˜¯ `peft` åº“ï¼Œå…·ä½“å–å†³äºä½ çš„ LoRA å®ç°\n",
    "\n",
    "# åŠ è½½å¾®è°ƒåçš„ LoRA adapter\n",
    "adapter = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# 3. åˆå¹¶ LoRA adapter å’ŒåŸºåº§æ¨¡å‹\n",
    "#ä½¿ç”¨çš„æ˜¯ LoRA åº“ï¼Œé€šå¸¸è°ƒç”¨ `PeftModel` çš„æ–¹æ³•ä¼šè‡ªåŠ¨å°† adapter åº”ç”¨åˆ°åŸºåº§æ¨¡å‹ä¸Š\n",
    "\n",
    "# 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹\n",
    "final_model_path = \"merge_model/1224/24_23_0.5B_200\"\n",
    "adapter.save_pretrained(final_model_path)\n",
    "\n",
    "# ä½ ä¹Ÿå¯ä»¥ä¿å­˜ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {final_model_path}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04e246",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225152233200.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fae0a0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225194811534.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77a5a4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225152742792.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3568c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225153030755.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66655570",
   "metadata": {},
   "source": [
    "- **step5 æ¨¡å‹æ•ˆæœæ ¡éªŒ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeec92",
   "metadata": {},
   "source": [
    "å½“å‰ä½¿ç”¨qwen2.5æœ¬åœ°æ‰§è¡Œwebç•Œé¢éªŒè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a3a2f",
   "metadata": {},
   "source": [
    "å½“å‰æ¨¡å‹ä¸º0.5B 200 epochåˆå¹¶åæ¨¡å‹\n",
    "\n",
    "ç°åœ¨ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸­åŒ»åŒ»ç”Ÿï¼Œè¯·ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†æä¾›è¯¦å°½è€Œæ¸…æ™°çš„å…³äºä¸­åŒ»é—®é¢˜çš„å›ç­”ã€‚è¿™æ®µæ—¶é—´å»ä¸Šå•æ‰€æœ¬æ¥æƒ³å°ä¾¿çš„å¯æ˜¯æ¯æ¬¡éƒ½ä¼šæ‹‰å¤§ä¾¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92365a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225115351511.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ef5e9",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸­åŒ»åŒ»ç”Ÿï¼Œè¯·ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†æä¾›è¯¦å°½è€Œæ¸…æ™°çš„å…³äºä¸­åŒ»é—®é¢˜çš„å›ç­”ã€‚ä½ å¥½ï¼Œå…¨èº«æ²¡åŠ²ï¼Œæ²¡ç²¾ç¥ï¼Œåƒä¸ä¸‹é¥­ï¼Œåªæƒ³ç¡è§‰ï¼Œæ˜¯ä»€ä¹ˆæƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80c1fd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225115745203.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92577de5",
   "metadata": {},
   "source": [
    "åŸç”Ÿæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf51c5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225120234766.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e831f5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225120329200.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622f1d7",
   "metadata": {},
   "source": [
    "# 5. æ‰©å±•ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d5f3c",
   "metadata": {},
   "source": [
    "## 5.1 deepspeedå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cca84f",
   "metadata": {},
   "source": [
    "å¢åŠ ä¸¤è¡Œä»£ç  ä¸€ä¸ªæ–‡ä»¶\n",
    "\n",
    "parser.add_argument(\"--deepspeed\", type=str, default=\"ds_config.json\", help=\"DeepSpeed é…ç½®æ–‡ä»¶è·¯å¾„\")\n",
    "\n",
    "deepspeed=args.deepspeed,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406a63b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225204213883.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9cc8d",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true,\n",
    "        \"auto_cast\": true\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": true,\n",
    "        \"overlap_comm\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"contiguous_gradients\": true\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4dcbd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225205404809.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ff8f1",
   "metadata": {},
   "source": [
    "æ„Ÿè§‰éº»çƒ¦çš„åŒå­¦å¯ä»¥å» LLaMA-Factory/cache/ä¸­å¤åˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728bf19",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225205636375.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61d0a2",
   "metadata": {},
   "source": [
    "ä»£ç é‡æ–°å‘½åè„šåŠä»£ç åè¾¹éƒ½å¢åŠ  _deepspeed ä¾‹å¦‚ ï¼štrain_distributed_deepspeed.py\n",
    "\n",
    "å¯åŠ¨å‘½ä»¤å’Œä¸Šè¾¹ä¸€æ · ./train_distributed_deepspeed.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822e775",
   "metadata": {},
   "source": [
    "å¯ä»¥ä½¿ç”¨ ä¹Ÿå¯ä»¥ä½¿ç”¨ torchrun\n",
    "```bash\n",
    "deepspeed --num_gpus=4 train_distributed_deepspeed.py \\\n",
    "    --model_path \"/home/data/muyan/model/Qwen/Qwen2.5-0.5B-Instruct\" \\\n",
    "    --json_file_path \"/home/data/muyan/code/data/medicalQA_top1k.json\" \\\n",
    "    --output_dir \"./output\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 5e-6 \\\n",
    "    --max_steps 500 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09dde4",
   "metadata": {},
   "source": [
    "`DeepSpeed` æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§è§„æ¨¡åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ä»»åŠ¡è®¾è®¡çš„è®­ç»ƒåŠ é€Ÿæ¡†æ¶ã€‚ç›¸æ¯” `torchrun`ï¼Œå®ƒä¸ä»…æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¿˜æä¾›äº†è®¸å¤šé«˜çº§ç‰¹æ€§ï¼Œå¦‚ ZeRO ä¼˜åŒ–å™¨ã€æ¢¯åº¦å‹ç¼©ç­‰ã€‚\n",
    "\n",
    "- **æ”¯æŒæ›´å¤šä¼˜åŒ–åŠŸèƒ½**ï¼š\n",
    "  - æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/FP8ï¼‰ã€‚\n",
    "  - ZeRO ä¼˜åŒ–ï¼ˆåˆ†å¸ƒå¼æ˜¾å­˜ä¼˜åŒ–ï¼‰ã€‚\n",
    "  - åŠ¨æ€å¼ é‡åˆ‡åˆ†ã€‚\n",
    "  - CPU å’Œ NVMe æ··åˆå†…å­˜æ”¯æŒã€‚\n",
    "\n",
    "\n",
    "| å¯åŠ¨æ–¹å¼        | åˆ†å¸ƒå¼æ”¯æŒ         | é€‚ç”¨åœºæ™¯                      | ä¼˜åŠ¿                          | é™åˆ¶                          |\n",
    "|-----------------|-------------------|-------------------------------|-------------------------------|-------------------------------|\n",
    "| **python**      | æ— ï¼ˆé»˜è®¤å•è¿›ç¨‹ï¼‰   | å• GPU æˆ–ç®€å•ä»»åŠ¡              | ç®€å•æ˜“ç”¨ï¼Œå¿«é€Ÿè°ƒè¯•ä»£ç           | ä¸æ”¯æŒå¤š GPU æˆ–å¤šèŠ‚ç‚¹          |\n",
    "| **torchrun**    | å†…ç½®åˆ†å¸ƒå¼æ”¯æŒ     | å•èŠ‚ç‚¹å¤š GPU æˆ–å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ä»»åŠ¡  | æ˜“ç”¨ã€æ”¯æŒ DDP ç­‰åˆ†å¸ƒå¼åŠŸèƒ½     | ä¸æ”¯æŒå¤§æ¨¡å‹ä¼˜åŒ–å’Œæ˜¾å­˜èŠ‚çœ      |\n",
    "| **deepspeed**   | é«˜çº§åˆ†å¸ƒå¼æ”¯æŒ     | å¤§è§„æ¨¡åˆ†å¸ƒå¼æˆ–å¤§æ¨¡å‹è®­ç»ƒ        | æ˜¾å­˜ä¼˜åŒ–ï¼ˆZeROï¼‰ã€å¤§æ¨¡å‹æ”¯æŒ     | ä¾èµ–é…ç½®æ–‡ä»¶ï¼Œå­¦ä¹ æˆæœ¬è¾ƒé«˜      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240ba67",
   "metadata": {},
   "source": [
    "å¯¹äºç±»ä¼¼æ›´å¤šdeepspeed å‚æ•°ä¼ é€’ å¯ä»¥å‚è€ƒæºç æ³¨é‡Šè¿›è¡Œé…ç½®ï¼šhttps://github.com/huggingface/transformers/blob/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src/transformers/training_args.py#L223 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163acfb7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241225212502598.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3d540",
   "metadata": {},
   "source": [
    "## 5.2 å¤šè½®æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d77a06",
   "metadata": {},
   "source": [
    "```python \n",
    "# å½“å‰ä»£ç ä»…ç”¨åšæ€è·¯å‚è€ƒï¼Œå…·ä½“éœ€è¦æ ¹æ®ä½¿ç”¨çŠ¶å†µè¿›è¡Œè°ƒæ•´ä½¿ç”¨ï¼Œæ•°æ®é›†æ ·ä¾‹ä¸ºï¼š\n",
    "    # {\n",
    "    #     \"instruction\": \"è¯·æ ¹æ®èŠå¤©å†å²å’Œå½“å‰é—®é¢˜å›ç­”\",\n",
    "    #     \"input\": \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\",\n",
    "    #     \"output\": \"æ ¹æ®ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œä»Šå¤©æ˜¯æ™´å¤©ï¼Œæ°”æ¸©é€‚å®œã€‚\",\n",
    "    #     \"history\": [\n",
    "    #         {\n",
    "    #             \"human\": \"ä½ å¥½ï¼Œèƒ½å‘Šè¯‰æˆ‘ä»Šå¤©çš„å¤©æ°”é¢„æŠ¥å—ï¼Ÿ\",\n",
    "    #             \"assistant\": \"ä»Šå¤©æ˜¯æ™´å¤©ï¼Œæ°”æ¸©åœ¨20-25åº¦ä¹‹é—´ï¼Œå¾ˆé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚\"\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"human\": \"éœ€è¦å¸¦ä¼å—ï¼Ÿ\",\n",
    "    #             \"assistant\": \"ä¸éœ€è¦å¸¦ä¼ï¼Œä»Šå¤©ä¸ä¼šä¸‹é›¨ï¼Œå¤©æ°”å¾ˆå¥½ã€‚\"\n",
    "    #         }\n",
    "    #     ]\n",
    "    # },\n",
    "def preprocess_history_function(examples, tokenizer, max_length=512):\n",
    "    prompts = []\n",
    "    \n",
    "    # éå†æ¯ä¸ªæ ·æœ¬\n",
    "    for instruction, input_text, output, history in zip(\n",
    "        examples['instruction'], \n",
    "        examples['input'], \n",
    "        examples['output'],\n",
    "        examples.get('history', [[] for _ in examples['instruction']])  # å¦‚æœæ²¡æœ‰historyåˆ™ä½¿ç”¨ç©ºåˆ—è¡¨\n",
    "    ):\n",
    "        # æ„å»ºå†å²å¯¹è¯å­—ç¬¦ä¸²\n",
    "        history_text = \"\"\n",
    "        if history:\n",
    "            for turn in history:\n",
    "                # å‡è®¾æ¯ä¸ªturnæ˜¯ä¸€ä¸ªdictï¼ŒåŒ…å«'human'å’Œ'assistant'é”®\n",
    "                if isinstance(turn, dict):\n",
    "                    history_text += f\"Human: {turn.get('human', '')}\\nAssistant: {turn.get('assistant', '')}\\n\"\n",
    "                # æˆ–è€…æ˜¯ä¸€ä¸ªåˆ—è¡¨/å…ƒç»„ï¼ŒåŒ…å«äººç±»å’ŒåŠ©æ‰‹çš„å¯¹è¯\n",
    "                elif isinstance(turn, (list, tuple)) and len(turn) == 2:\n",
    "                    history_text += f\"Human: {turn[0]}\\nAssistant: {turn[1]}\\n\"\n",
    "        \n",
    "        # æ„å»ºå½“å‰å¯¹è¯çš„æç¤ºè¯­\n",
    "        if input_text and input_text.strip():\n",
    "            current_prompt = f\"Input: {input_text}\\nInstruction: {instruction}\\n\"\n",
    "        else:\n",
    "            current_prompt = f\"Instruction: {instruction}\\n\"\n",
    "            \n",
    "        # ç»„åˆå†å²å¯¹è¯å’Œå½“å‰å¯¹è¯\n",
    "        if history_text:\n",
    "            prompt = f\"Chat History:\\n{history_text}\\nCurrent Dialog:\\n{current_prompt}Output: {output}\"\n",
    "        else:\n",
    "            prompt = f\"{current_prompt}Output: {output}\"\n",
    "            \n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenizeæ‰€æœ‰prompts\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºæ ‡ç­¾\n",
    "    labels = tokenized_inputs['input_ids'].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "def convert_json_history_to_dataset(json_file_path, save_directory, tokenizer):\n",
    "    \"\"\"è½¬æ¢å¤šè½®å¯¹è¯JSONæ•°æ®ä¸ºDatasetæ ¼å¼\"\"\"\n",
    "    \n",
    "    # åŠ è½½JSONæ•°æ®\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        # æ£€æŸ¥å¿…è¦å­—æ®µ\n",
    "        if not all(isinstance(item.get(k), str) and len(item.get(k, '').strip()) > 0 \n",
    "                  for k in ['instruction', 'output']):\n",
    "            continue\n",
    "            \n",
    "        # ç¡®ä¿historyå­—æ®µæ ¼å¼æ­£ç¡®\n",
    "        history = item.get('history', [])\n",
    "        if not isinstance(history, list):\n",
    "            history = []\n",
    "            \n",
    "        # éªŒè¯historyä¸­çš„æ¯ä¸ªå¯¹è¯å›åˆ\n",
    "        valid_history = []\n",
    "        for turn in history:\n",
    "            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼\n",
    "            if isinstance(turn, dict) and 'human' in turn and 'assistant' in turn:\n",
    "                valid_history.append(turn)\n",
    "            # å¦‚æœæ˜¯åˆ—è¡¨/å…ƒç»„æ ¼å¼\n",
    "            elif isinstance(turn, (list, tuple)) and len(turn) == 2:\n",
    "                valid_history.append({\n",
    "                    'human': str(turn[0]),\n",
    "                    'assistant': str(turn[1])\n",
    "                })\n",
    "        \n",
    "        # åˆ›å»ºæ¸…æ´—åçš„æ•°æ®é¡¹\n",
    "        cleaned_item = {\n",
    "            'instruction': item['instruction'],\n",
    "            'input': item.get('input', ''),\n",
    "            'output': item['output'],\n",
    "            'history': valid_history\n",
    "        }\n",
    "        cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDatasetæ ¼å¼\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(cleaned_data))\n",
    "    \n",
    "    # é¢„å¤„ç†æ•°æ®\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Processing dataset\"\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®æ•°æ®æ ¼å¼\n",
    "    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    processed_dataset.save_to_disk(save_directory)\n",
    "    \n",
    "    return processed_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b6a25",
   "metadata": {},
   "source": [
    "## 5.3 æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a29a6",
   "metadata": {},
   "source": [
    "å„å…¬å¸å¤„ç†ä¸šåŠ¡å’Œä½¿ç”¨æ•°æ®æƒ…å†µä¸ä¸€è‡´ï¼Œæ— æ³•å…¨éƒ¨è¦†ç›–ä¸»è¦æä¾›æ€è·¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b80c3",
   "metadata": {},
   "source": [
    "**åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼š**\n",
    "- æ€»æ ·æœ¬æ•°\n",
    "- å¤šè½®å¯¹è¯æ ·æœ¬æ•°é‡\n",
    "- ç©ºè¾“å…¥æ ·æœ¬æ•°é‡\n",
    "- å¹³å‡æ–‡æœ¬é•¿åº¦\n",
    "\n",
    "**Tokené•¿åº¦åˆ†æï¼š**\n",
    "- æŒ‡ä»¤ã€è¾“å…¥ã€è¾“å‡ºçš„tokené•¿åº¦åˆ†å¸ƒ\n",
    "- æ€»tokené•¿åº¦åˆ†å¸ƒ\n",
    "- å„ç±»tokené•¿åº¦çš„ç»Ÿè®¡æŒ‡æ ‡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æœ€å¤§å€¼ç­‰ï¼‰\n",
    "\n",
    "**å¯¹è¯æ·±åº¦åˆ†æï¼š**\n",
    "- å¯¹è¯è½®æ•°åˆ†å¸ƒ\n",
    "- å¹³å‡å¯¹è¯æ·±åº¦\n",
    "- æœ€å¤§å¯¹è¯æ·±åº¦\n",
    "\n",
    "**å†…å®¹è´¨é‡åˆ†æï¼š**\n",
    "- ç©ºå€¼æ£€æµ‹\n",
    "- ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹\n",
    "- æ–‡æœ¬è´¨é‡æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abdf960",
   "metadata": {},
   "source": [
    "è¯·å¤§å®¶è‡ªè¡Œé€‚é…ç¯å¢ƒè°ƒæ•´ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16b06e",
   "metadata": {},
   "source": [
    "```python\n",
    "#  å¢åŠ äº† ...  éœ€è¦è¿›è¡Œä»£ç å¾®è°ƒåä½¿ç”¨\n",
    "import json ...\n",
    "import pandas as pd ...\n",
    "import numpy as np ...\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns ...\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any\n",
    "import torch ...\n",
    "from tqdm import tqdm\n",
    "import nltk ...\n",
    "from nltk.tokenize import word_tokenize  \n",
    "nltk.download('punkt')\n",
    "\n",
    "class DatasetAnalyzer:\n",
    "    def __init__(self, json_file_path: str, tokenizer_path: str):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ•°æ®é›†åˆ†æå™¨\n",
    "        \n",
    "        å‚æ•°:\n",
    "        json_file_path: è®­ç»ƒæ•°æ®çš„JSONæ–‡ä»¶è·¯å¾„\n",
    "        tokenizer_path: åˆ†è¯å™¨è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.json_file_path = json_file_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        with open(self.json_file_path, 'r', encoding='utf-8') as f:\n",
    "            self.raw_data = json.load(f)\n",
    "        self.df = pd.DataFrame(self.raw_data)\n",
    "        \n",
    "    def basic_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åŸºç¡€ç»Ÿè®¡ä¿¡æ¯åˆ†æ\n",
    "        è¿”å›æ•°æ®é›†çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"æ€»æ ·æœ¬æ•°\": len(self.df),\n",
    "            \"åŒ…å«å†å²å¯¹è¯çš„æ ·æœ¬æ•°\": sum(self.df['history'].apply(lambda x: len(x) > 0 if isinstance(x, list) else 0)),\n",
    "            \"ç©ºè¾“å…¥æ ·æœ¬æ•°\": sum(self.df['input'].apply(lambda x: not bool(str(x).strip()))),\n",
    "            \"å¹³å‡æŒ‡ä»¤é•¿åº¦\": self.df['instruction'].apply(len).mean(),\n",
    "            \"å¹³å‡è¾“å‡ºé•¿åº¦\": self.df['output'].apply(len).mean(),\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def token_length_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åˆ†ætokené•¿åº¦åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        token_stats = {\n",
    "            \"instruction_tokens\": [],\n",
    "            \"input_tokens\": [],\n",
    "            \"output_tokens\": [],\n",
    "            \"total_tokens\": []\n",
    "        }\n",
    "        \n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"åˆ†ætokené•¿åº¦\"):\n",
    "            # ç»Ÿè®¡å„å­—æ®µçš„tokené•¿åº¦\n",
    "            instruction_tokens = len(self.tokenizer.encode(row['instruction']))\n",
    "            input_tokens = len(self.tokenizer.encode(row['input'])) if row['input'] else 0\n",
    "            output_tokens = len(self.tokenizer.encode(row['output']))\n",
    "            total_tokens = instruction_tokens + input_tokens + output_tokens\n",
    "            \n",
    "            token_stats[\"instruction_tokens\"].append(instruction_tokens)\n",
    "            token_stats[\"input_tokens\"].append(input_tokens)\n",
    "            token_stats[\"output_tokens\"].append(output_tokens)\n",
    "            token_stats[\"total_tokens\"].append(total_tokens)\n",
    "        \n",
    "        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "        stats = {}\n",
    "        for key, values in token_stats.items():\n",
    "            stats[key] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"median\": np.median(values),\n",
    "                \"max\": np.max(values),\n",
    "                \"min\": np.min(values),\n",
    "                \"95th_percentile\": np.percentile(values, 95)\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def dialogue_depth_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åˆ†æå¯¹è¯æ·±åº¦ï¼ˆé’ˆå¯¹å¤šè½®å¯¹è¯ï¼‰\n",
    "        \"\"\"\n",
    "        dialogue_depths = []\n",
    "        for history in self.df['history']:\n",
    "            if isinstance(history, list):\n",
    "                dialogue_depths.append(len(history))\n",
    "            else:\n",
    "                dialogue_depths.append(0)\n",
    "                \n",
    "        return {\n",
    "            \"å¹³å‡å¯¹è¯è½®æ•°\": np.mean(dialogue_depths),\n",
    "            \"æœ€å¤§å¯¹è¯è½®æ•°\": np.max(dialogue_depths),\n",
    "            \"å¯¹è¯è½®æ•°åˆ†å¸ƒ\": Counter(dialogue_depths)\n",
    "        }\n",
    "    \n",
    "    def content_quality_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åˆ†æå†…å®¹è´¨é‡\n",
    "        \"\"\"\n",
    "        quality_metrics = {\n",
    "            \"ç©ºæŒ‡ä»¤æ•°\": sum(self.df['instruction'].apply(lambda x: not bool(str(x).strip()))),\n",
    "            \"ç©ºè¾“å‡ºæ•°\": sum(self.df['output'].apply(lambda x: not bool(str(x).strip()))),\n",
    "            \"æŒ‡ä»¤ä¸­çš„ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹\": self.analyze_special_chars('instruction'),\n",
    "            \"è¾“å‡ºä¸­çš„ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹\": self.analyze_special_chars('output'),\n",
    "        }\n",
    "        \n",
    "        return quality_metrics\n",
    "    \n",
    "    def analyze_special_chars(self, column: str) -> float:\n",
    "        \"\"\"åˆ†æç‰¹æ®Šå­—ç¬¦çš„æ¯”ä¾‹\"\"\"\n",
    "        special_chars = set('!@#$%^&*()_+={}[]|\\\\:;\"\\'<>,.?/~`')\n",
    "        total_chars = sum(self.df[column].apply(len))\n",
    "        special_char_count = sum(self.df[column].apply(\n",
    "            lambda x: sum(1 for c in x if c in special_chars)\n",
    "        ))\n",
    "        return special_char_count / total_chars if total_chars > 0 else 0\n",
    "    \n",
    "    def plot_token_distribution(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        ç»˜åˆ¶tokené•¿åº¦åˆ†å¸ƒå›¾\n",
    "        \"\"\"\n",
    "        token_stats = self.token_length_analysis()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Token Length Distributions')\n",
    "        \n",
    "        for (key, values), ax in zip(token_stats.items(), axes.flat):\n",
    "            sns.histplot(values, ax=ax)\n",
    "            ax.set_title(f'{key} Distribution')\n",
    "            ax.set_xlabel('Token Length')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_dialogue_depth(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        ç»˜åˆ¶å¯¹è¯æ·±åº¦åˆ†å¸ƒå›¾\n",
    "        \"\"\"\n",
    "        depths = self.dialogue_depth_analysis()['å¯¹è¯è½®æ•°åˆ†å¸ƒ']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(depths.keys(), depths.values())\n",
    "        plt.title('Dialogue Depth Distribution')\n",
    "        plt.xlabel('Number of Turns')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        \n",
    "    def generate_report(self, output_path: str = None):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            \"åŸºç¡€ç»Ÿè®¡ä¿¡æ¯\": self.basic_statistics(),\n",
    "            \"Tokené•¿åº¦åˆ†æ\": self.token_length_analysis(),\n",
    "            \"å¯¹è¯æ·±åº¦åˆ†æ\": self.dialogue_depth_analysis(),\n",
    "            \"å†…å®¹è´¨é‡åˆ†æ\": self.content_quality_analysis()\n",
    "        }\n",
    "        \n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return report\n",
    "\n",
    "def main():\n",
    "    # ä½¿ç”¨ç¤ºä¾‹\n",
    "    analyzer = DatasetAnalyzer(\n",
    "        json_file_path=\"path_to_your_data.json\",\n",
    "        tokenizer_path=\"your_model_path\"\n",
    "    )\n",
    "    \n",
    "    # ç”ŸæˆæŠ¥å‘Š\n",
    "    report = analyzer.generate_report(\"analysis_report.json\")\n",
    "    \n",
    "    # ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨\n",
    "    analyzer.plot_token_distribution(\"token_distribution.png\")\n",
    "    analyzer.plot_dialogue_depth(\"dialogue_depth.png\")\n",
    "    \n",
    "    # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(\"\\n=== æ•°æ®é›†åˆ†ææŠ¥å‘Š ===\")\n",
    "    print(\"\\n1. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    for k, v in report[\"åŸºç¡€ç»Ÿè®¡ä¿¡æ¯\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "        \n",
    "    print(\"\\n2. Tokené•¿åº¦åˆ†æ:\")\n",
    "    for field, stats in report[\"Tokené•¿åº¦åˆ†æ\"].items():\n",
    "        print(f\"\\n{field}:\")\n",
    "        for metric, value in stats.items():\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "            \n",
    "    print(\"\\n3. å¯¹è¯æ·±åº¦åˆ†æ:\")\n",
    "    for k, v in report[\"å¯¹è¯æ·±åº¦åˆ†æ\"].items():\n",
    "        if k != \"å¯¹è¯è½®æ•°åˆ†å¸ƒ\":\n",
    "            print(f\"{k}: {v:.2f}\")\n",
    "            \n",
    "    print(\"\\n4. å†…å®¹è´¨é‡åˆ†æ:\")\n",
    "    for k, v in report[\"å†…å®¹è´¨é‡åˆ†æ\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8b37e",
   "metadata": {},
   "source": [
    "# 6. æ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2cab58",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20241226162949116.png\" width=100%></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
